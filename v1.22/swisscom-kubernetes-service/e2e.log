I1202 09:03:41.720756      23 e2e.go:129] Starting e2e run "6ce530b9-2879-4c46-af98-d08e31ab28b1" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1669971821 - Will randomize all specs
Will run 346 of 6444 specs

Dec  2 09:03:44.780: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 09:03:44.786: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  2 09:03:44.819: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  2 09:03:44.870: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  2 09:03:44.870: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Dec  2 09:03:44.870: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  2 09:03:44.888: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'istio-cni-node' (0 seconds elapsed)
Dec  2 09:03:44.888: INFO: e2e test version: v1.22.15
Dec  2 09:03:44.889: INFO: kube-apiserver version: v1.22.15+vmware.1
Dec  2 09:03:44.889: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 09:03:44.899: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:03:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 09:03:45.544627      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-9069", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 09:03:45.626805      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Dec  2 09:03:45.627: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec  2 09:03:45.646: INFO: PSP annotation exists on dry run pod: "e2e-test-privileged-psp"; assuming PodSecurityPolicy is enabled
W1202 09:03:45.653238      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Dec  2 09:03:45.687: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  2 09:03:45.829: INFO: Waiting up to 5m0s for pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35" in namespace "emptydir-9069" to be "Succeeded or Failed"
Dec  2 09:03:45.838: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.399599ms
Dec  2 09:03:47.848: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018660021s
Dec  2 09:03:49.862: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032046939s
Dec  2 09:03:51.869: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039762543s
Dec  2 09:03:53.886: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056936675s
Dec  2 09:03:55.894: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064901002s
Dec  2 09:03:57.908: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 12.078054248s
Dec  2 09:03:59.922: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 14.092587641s
Dec  2 09:04:01.935: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.105636985s
STEP: Saw pod success
Dec  2 09:04:01.935: INFO: Pod "pod-7b7679be-1049-48f5-b623-1eb52b10ab35" satisfied condition "Succeeded or Failed"
Dec  2 09:04:01.943: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-7b7679be-1049-48f5-b623-1eb52b10ab35 container test-container: <nil>
STEP: delete the pod
Dec  2 09:04:02.240: INFO: Waiting for pod pod-7b7679be-1049-48f5-b623-1eb52b10ab35 to disappear
Dec  2 09:04:02.248: INFO: Pod pod-7b7679be-1049-48f5-b623-1eb52b10ab35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:04:02.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9069" for this suite.

• [SLOW TEST:17.835 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":95,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:04:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-runtime
W1202 09:04:03.181503      23 warnings.go:70] No static IP address has been configured for the namespace "container-runtime-9060", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  2 09:04:12.501: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:04:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9060" for this suite.

• [SLOW TEST:10.310 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":111,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:04:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 09:04:13.510857      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-991", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:04:29.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-991" for this suite.

• [SLOW TEST:17.377 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":3,"skipped":111,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:04:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 09:04:30.949161      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-702", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-702
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-702
STEP: Waiting until pod test-pod will start running in namespace statefulset-702
STEP: Creating statefulset with conflicting port in namespace statefulset-702
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-702
Dec  2 09:04:51.709: INFO: Observed stateful pod in namespace: statefulset-702, name: ss-0, uid: 89ea144e-ebec-4eef-92e4-b55c745e0c8a, status phase: Pending. Waiting for statefulset controller to delete.
Dec  2 09:04:52.437: INFO: Observed stateful pod in namespace: statefulset-702, name: ss-0, uid: 89ea144e-ebec-4eef-92e4-b55c745e0c8a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  2 09:04:52.455: INFO: Observed stateful pod in namespace: statefulset-702, name: ss-0, uid: 89ea144e-ebec-4eef-92e4-b55c745e0c8a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  2 09:04:52.467: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-702
STEP: Removing pod with conflicting port in namespace statefulset-702
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-702 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 09:05:02.597: INFO: Deleting all statefulset in ns statefulset-702
Dec  2 09:05:02.604: INFO: Scaling statefulset ss to 0
Dec  2 09:05:12.659: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 09:05:12.666: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:12.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-702" for this suite.

• [SLOW TEST:42.822 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":4,"skipped":118,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:13.246: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 09:05:13.762720      23 warnings.go:70] No static IP address has been configured for the namespace "gc-7016", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1202 09:05:15.042013      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  2 09:05:15.042: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:15.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7016" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":5,"skipped":132,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:15.554: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 09:05:16.113900      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-3464", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:16.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3464" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":6,"skipped":138,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:17.176: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 09:05:17.756599      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-9928", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:31.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9928" for this suite.

• [SLOW TEST:14.375 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":7,"skipped":141,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:05:32.052334      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-4275", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  2 09:05:32.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4275 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  2 09:05:33.433: INFO: stderr: ""
Dec  2 09:05:33.433: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec  2 09:05:43.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4275 get pod e2e-test-httpd-pod -o json'
Dec  2 09:05:43.576: INFO: stderr: ""
Dec  2 09:05:43.577: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2022-12-02T09:05:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4275\",\n        \"resourceVersion\": \"52150829\",\n        \"uid\": \"752b61b9-65b9-47c4-aee8-175cc7ce0d3f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fqq2z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"kube-plus-pull-secret\"\n            }\n        ],\n        \"nodeName\": \"5e47d0ea-e90b-466b-b6de-2748d512ebf3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fqq2z\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-02T09:05:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-02T09:05:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-02T09:05:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-02T09:05:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2e4aac22a710babf878641889717161013b438917adeb3a3fd3add7e6d91860c\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-02T09:05:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"11.0.95.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"11.34.25.2\",\n        \"podIPs\": [\n            {\n                \"ip\": \"11.34.25.2\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-02T09:05:33Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  2 09:05:43.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4275 replace -f -'
Dec  2 09:05:46.406: INFO: stderr: ""
Dec  2 09:05:46.406: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec  2 09:05:46.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4275 delete pods e2e-test-httpd-pod'
Dec  2 09:05:49.477: INFO: stderr: ""
Dec  2 09:05:49.477: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:49.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4275" for this suite.

• [SLOW TEST:18.593 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":8,"skipped":144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context-test
W1202 09:05:50.697327      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-test-8735", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:05:50.903: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779" in namespace "security-context-test-8735" to be "Succeeded or Failed"
Dec  2 09:05:50.910: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779": Phase="Pending", Reason="", readiness=false. Elapsed: 6.834618ms
Dec  2 09:05:52.922: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019120928s
Dec  2 09:05:54.935: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031809044s
Dec  2 09:05:56.945: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041886117s
Dec  2 09:05:58.959: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055888988s
Dec  2 09:05:58.959: INFO: Pod "busybox-readonly-false-cdfe1d71-3100-487c-bb85-a4e0381b1779" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:05:58.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8735" for this suite.

• [SLOW TEST:9.361 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:171
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":190,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:05:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 09:05:59.983525      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-2823", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-fd6832f0-9540-4beb-8363-5652282cca1c in namespace container-probe-2823
Dec  2 09:06:08.319: INFO: Started pod busybox-fd6832f0-9540-4beb-8363-5652282cca1c in namespace container-probe-2823
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 09:06:08.328: INFO: Initial restart count of pod busybox-fd6832f0-9540-4beb-8363-5652282cca1c is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:10:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2823" for this suite.

• [SLOW TEST:250.947 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:10:10.458: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 09:10:11.008103      23 warnings.go:70] No static IP address has been configured for the namespace "pods-900", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  2 09:10:11.205: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:13.218: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:15.219: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:17.222: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:19.221: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:21.216: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:23.221: INFO: The status of Pod pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  2 09:10:23.765: INFO: Successfully updated pod "pod-update-018546f2-a3e2-4b92-98f7-24ad3d8794fc"
STEP: verifying the updated pod is in kubernetes
Dec  2 09:10:23.777: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:10:23.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-900" for this suite.

• [SLOW TEST:13.814 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":222,"failed":0}
SSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:10:24.272: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename lease-test
W1202 09:10:24.748088      23 warnings.go:70] No static IP address has been configured for the namespace "lease-test-5312", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-5312
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:10:25.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5312" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":12,"skipped":225,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:10:25.488: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-lifecycle-hook
W1202 09:10:25.959261      23 warnings.go:70] No static IP address has been configured for the namespace "container-lifecycle-hook-4233", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  2 09:10:26.199: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:28.207: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:30.210: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:32.215: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:34.211: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:36.218: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  2 09:10:36.255: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:38.270: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:40.267: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:10:42.269: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  2 09:10:42.327: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  2 09:10:42.335: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  2 09:10:44.336: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  2 09:10:44.348: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:10:44.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4233" for this suite.

• [SLOW TEST:19.393 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":230,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:10:44.881: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 09:10:45.357891      23 warnings.go:70] No static IP address has been configured for the namespace "dns-8090", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8090.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8090.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8090.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8090.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 09:11:02.191: INFO: DNS probes using dns-8090/dns-test-02cae61c-9cf5-4979-b9d9-36122abf45cf succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:11:02.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8090" for this suite.

• [SLOW TEST:18.326 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":14,"skipped":231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:11:03.210: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 09:11:03.658208      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-6525", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 09:11:04.303109      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-6525", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  2 09:11:04.374: INFO: Number of nodes with available pods: 0
Dec  2 09:11:04.374: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:11:05.412: INFO: Number of nodes with available pods: 0
Dec  2 09:11:05.412: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:11:06.400: INFO: Number of nodes with available pods: 0
Dec  2 09:11:06.400: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:11:07.396: INFO: Number of nodes with available pods: 0
Dec  2 09:11:07.396: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:11:08.394: INFO: Number of nodes with available pods: 0
Dec  2 09:11:08.394: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:11:09.396: INFO: Number of nodes with available pods: 1
Dec  2 09:11:09.396: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:10.395: INFO: Number of nodes with available pods: 1
Dec  2 09:11:10.395: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:11.396: INFO: Number of nodes with available pods: 1
Dec  2 09:11:11.396: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:12.398: INFO: Number of nodes with available pods: 1
Dec  2 09:11:12.399: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:13.396: INFO: Number of nodes with available pods: 1
Dec  2 09:11:13.396: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:14.396: INFO: Number of nodes with available pods: 1
Dec  2 09:11:14.396: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:15.396: INFO: Number of nodes with available pods: 1
Dec  2 09:11:15.396: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:16.397: INFO: Number of nodes with available pods: 1
Dec  2 09:11:16.397: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:17.399: INFO: Number of nodes with available pods: 1
Dec  2 09:11:17.400: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:18.395: INFO: Number of nodes with available pods: 1
Dec  2 09:11:18.395: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:19.395: INFO: Number of nodes with available pods: 1
Dec  2 09:11:19.395: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:20.399: INFO: Number of nodes with available pods: 1
Dec  2 09:11:20.399: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:11:21.397: INFO: Number of nodes with available pods: 3
Dec  2 09:11:21.397: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Dec  2 09:11:21.415: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Dec  2 09:11:21.439: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Dec  2 09:11:21.443: INFO: Observed &DaemonSet event: ADDED
Dec  2 09:11:21.443: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.444: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.444: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.445: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.445: INFO: Found daemon set daemon-set in namespace daemonsets-6525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  2 09:11:21.445: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Dec  2 09:11:21.458: INFO: Observed &DaemonSet event: ADDED
Dec  2 09:11:21.458: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.459: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.460: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.460: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.460: INFO: Observed daemon set daemon-set in namespace daemonsets-6525 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  2 09:11:21.460: INFO: Observed &DaemonSet event: MODIFIED
Dec  2 09:11:21.460: INFO: Found daemon set daemon-set in namespace daemonsets-6525 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec  2 09:11:21.460: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6525, will wait for the garbage collector to delete the pods
Dec  2 09:11:21.538: INFO: Deleting DaemonSet.extensions daemon-set took: 12.473332ms
Dec  2 09:11:21.639: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.710454ms
Dec  2 09:11:33.657: INFO: Number of nodes with available pods: 0
Dec  2 09:11:33.657: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 09:11:33.666: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52153144"},"items":null}

Dec  2 09:11:33.672: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52153144"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:11:33.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6525" for this suite.

• [SLOW TEST:30.969 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":15,"skipped":257,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:11:34.183: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 09:11:34.607778      23 warnings.go:70] No static IP address has been configured for the namespace "services-6730", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6730
STEP: creating service affinity-nodeport-transition in namespace services-6730
STEP: creating replication controller affinity-nodeport-transition in namespace services-6730
I1202 09:11:35.308204      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-6730, replica count: 3
I1202 09:11:38.358815      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:11:41.359617      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:11:44.360201      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 09:11:44.398: INFO: Creating new exec pod
Dec  2 09:11:49.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec  2 09:11:49.710: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec  2 09:11:49.710: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 09:11:49.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.232.239 80'
Dec  2 09:11:49.931: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.232.239 80\nConnection to 10.100.232.239 80 port [tcp/http] succeeded!\n"
Dec  2 09:11:49.931: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 09:11:49.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 30185'
Dec  2 09:11:50.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 30185\nConnection to 11.0.95.6 30185 port [tcp/*] succeeded!\n"
Dec  2 09:11:50.151: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 09:11:50.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 30185'
Dec  2 09:11:50.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 30185\nConnection to 11.0.95.7 30185 port [tcp/*] succeeded!\n"
Dec  2 09:11:50.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 09:11:50.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.95.7:30185/ ; done'
Dec  2 09:11:51.215: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n"
Dec  2 09:11:51.215: INFO: stdout: "\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-b2tvd\naffinity-nodeport-transition-nwvqp\naffinity-nodeport-transition-b2tvd"
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-nwvqp
Dec  2 09:11:51.215: INFO: Received response from host: affinity-nodeport-transition-b2tvd
Dec  2 09:11:51.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6730 exec execpod-affinityff48n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.95.7:30185/ ; done'
Dec  2 09:11:52.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:30185/\n"
Dec  2 09:11:52.073: INFO: stdout: "\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh\naffinity-nodeport-transition-b85nh"
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Received response from host: affinity-nodeport-transition-b85nh
Dec  2 09:11:52.073: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6730, will wait for the garbage collector to delete the pods
Dec  2 09:11:52.179: INFO: Deleting ReplicationController affinity-nodeport-transition took: 18.391343ms
Dec  2 09:11:52.281: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.183286ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:12:03.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6730" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:30.240 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":16,"skipped":273,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:12:04.425: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 09:12:04.895469      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-4878", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4878
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec  2 09:12:05.094: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec  2 09:12:46.365: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 09:13:02.376: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:13:46.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4878" for this suite.

• [SLOW TEST:102.824 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":17,"skipped":283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:13:47.250: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:13:47.799419      23 warnings.go:70] No static IP address has been configured for the namespace "projected-8333", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 09:13:48.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd" in namespace "projected-8333" to be "Succeeded or Failed"
Dec  2 09:13:48.013: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.693626ms
Dec  2 09:13:50.029: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026966964s
Dec  2 09:13:52.039: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036740314s
Dec  2 09:13:54.056: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05339892s
Dec  2 09:13:56.072: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069785365s
STEP: Saw pod success
Dec  2 09:13:56.072: INFO: Pod "downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd" satisfied condition "Succeeded or Failed"
Dec  2 09:13:56.078: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd container client-container: <nil>
STEP: delete the pod
Dec  2 09:13:56.217: INFO: Waiting for pod downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd to disappear
Dec  2 09:13:56.224: INFO: Pod downwardapi-volume-bc348a2e-38ae-43d0-bd05-923749782cdd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:13:56.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8333" for this suite.

• [SLOW TEST:15.258 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":328,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:14:02.508: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename ingressclass
W1202 09:14:03.002093      23 warnings.go:70] No static IP address has been configured for the namespace "ingressclass-7668", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-7668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  2 09:14:03.255: INFO: starting watch
STEP: patching
STEP: updating
Dec  2 09:14:03.277: INFO: waiting for watch events with expected annotations
Dec  2 09:14:03.278: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:14:03.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7668" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":19,"skipped":341,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:14:04.367: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename proxy
W1202 09:14:04.855380      23 warnings.go:70] No static IP address has been configured for the namespace "proxy-5158", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6tgfr in namespace proxy-5158
I1202 09:14:05.597886      23 runners.go:190] Created replication controller with name: proxy-service-6tgfr, namespace: proxy-5158, replica count: 1
I1202 09:14:06.649461      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:14:07.649838      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:14:08.650910      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:14:09.653433      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:14:10.653738      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1202 09:14:11.654451      23 runners.go:190] proxy-service-6tgfr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 09:14:11.663: INFO: setup took 6.59402664s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  2 09:14:11.677: INFO: (0) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 13.382922ms)
Dec  2 09:14:11.677: INFO: (0) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 12.810745ms)
Dec  2 09:14:11.677: INFO: (0) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 12.932765ms)
Dec  2 09:14:11.682: INFO: (0) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 17.79279ms)
Dec  2 09:14:11.682: INFO: (0) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 18.501224ms)
Dec  2 09:14:11.682: INFO: (0) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 18.385058ms)
Dec  2 09:14:11.683: INFO: (0) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 18.789202ms)
Dec  2 09:14:11.683: INFO: (0) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 19.312529ms)
Dec  2 09:14:11.683: INFO: (0) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 19.696214ms)
Dec  2 09:14:11.685: INFO: (0) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 20.879697ms)
Dec  2 09:14:11.685: INFO: (0) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 21.198947ms)
Dec  2 09:14:11.685: INFO: (0) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 21.462408ms)
Dec  2 09:14:11.685: INFO: (0) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 21.237847ms)
Dec  2 09:14:11.685: INFO: (0) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 21.240426ms)
Dec  2 09:14:11.686: INFO: (0) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 22.652043ms)
Dec  2 09:14:11.688: INFO: (0) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 24.549136ms)
Dec  2 09:14:11.698: INFO: (1) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 9.195151ms)
Dec  2 09:14:11.699: INFO: (1) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 10.578821ms)
Dec  2 09:14:11.699: INFO: (1) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 10.202356ms)
Dec  2 09:14:11.700: INFO: (1) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 10.692673ms)
Dec  2 09:14:11.700: INFO: (1) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 11.72384ms)
Dec  2 09:14:11.702: INFO: (1) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 12.834856ms)
Dec  2 09:14:11.703: INFO: (1) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 13.727997ms)
Dec  2 09:14:11.703: INFO: (1) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 13.983727ms)
Dec  2 09:14:11.703: INFO: (1) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 14.434567ms)
Dec  2 09:14:11.704: INFO: (1) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 14.905952ms)
Dec  2 09:14:11.705: INFO: (1) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 16.131555ms)
Dec  2 09:14:11.708: INFO: (1) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 19.071044ms)
Dec  2 09:14:11.708: INFO: (1) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 19.031633ms)
Dec  2 09:14:11.708: INFO: (1) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 19.671394ms)
Dec  2 09:14:11.710: INFO: (1) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 21.333169ms)
Dec  2 09:14:11.711: INFO: (1) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 22.876246ms)
Dec  2 09:14:11.721: INFO: (2) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 9.208325ms)
Dec  2 09:14:11.721: INFO: (2) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 9.174598ms)
Dec  2 09:14:11.721: INFO: (2) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 9.813546ms)
Dec  2 09:14:11.723: INFO: (2) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 10.909906ms)
Dec  2 09:14:11.723: INFO: (2) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 10.969319ms)
Dec  2 09:14:11.725: INFO: (2) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.794193ms)
Dec  2 09:14:11.725: INFO: (2) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 12.835128ms)
Dec  2 09:14:11.725: INFO: (2) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 12.801051ms)
Dec  2 09:14:11.725: INFO: (2) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.843274ms)
Dec  2 09:14:11.725: INFO: (2) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 13.385685ms)
Dec  2 09:14:11.727: INFO: (2) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 15.192096ms)
Dec  2 09:14:11.728: INFO: (2) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 15.919432ms)
Dec  2 09:14:11.728: INFO: (2) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 16.489168ms)
Dec  2 09:14:11.729: INFO: (2) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 16.765284ms)
Dec  2 09:14:11.730: INFO: (2) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 18.587124ms)
Dec  2 09:14:11.730: INFO: (2) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 18.506991ms)
Dec  2 09:14:11.739: INFO: (3) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.817985ms)
Dec  2 09:14:11.742: INFO: (3) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.990001ms)
Dec  2 09:14:11.743: INFO: (3) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 11.323487ms)
Dec  2 09:14:11.743: INFO: (3) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 10.952278ms)
Dec  2 09:14:11.746: INFO: (3) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 15.130485ms)
Dec  2 09:14:11.746: INFO: (3) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 14.428118ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 20.389703ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 21.386528ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 20.24995ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 21.065299ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 20.585337ms)
Dec  2 09:14:11.752: INFO: (3) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 20.414291ms)
Dec  2 09:14:11.753: INFO: (3) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 20.344315ms)
Dec  2 09:14:11.753: INFO: (3) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 20.62315ms)
Dec  2 09:14:11.753: INFO: (3) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 20.859374ms)
Dec  2 09:14:11.753: INFO: (3) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 21.420549ms)
Dec  2 09:14:11.780: INFO: (4) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 26.90811ms)
Dec  2 09:14:11.780: INFO: (4) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 26.694443ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 26.822392ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 25.274984ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 26.556529ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 24.994098ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 27.082393ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 25.082789ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 25.323088ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 25.182414ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 24.78639ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 25.979881ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 25.590065ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 25.641754ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 26.042424ms)
Dec  2 09:14:11.781: INFO: (4) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 27.675554ms)
Dec  2 09:14:11.793: INFO: (5) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 10.017342ms)
Dec  2 09:14:11.794: INFO: (5) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 11.260376ms)
Dec  2 09:14:11.794: INFO: (5) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.908434ms)
Dec  2 09:14:11.794: INFO: (5) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 10.424056ms)
Dec  2 09:14:11.797: INFO: (5) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 13.938286ms)
Dec  2 09:14:11.797: INFO: (5) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 13.655042ms)
Dec  2 09:14:11.798: INFO: (5) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 14.151957ms)
Dec  2 09:14:11.798: INFO: (5) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 13.932034ms)
Dec  2 09:14:11.798: INFO: (5) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 14.022136ms)
Dec  2 09:14:11.798: INFO: (5) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 14.6558ms)
Dec  2 09:14:11.798: INFO: (5) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 15.742562ms)
Dec  2 09:14:11.799: INFO: (5) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.568979ms)
Dec  2 09:14:11.800: INFO: (5) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 16.15272ms)
Dec  2 09:14:11.800: INFO: (5) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 15.921138ms)
Dec  2 09:14:11.802: INFO: (5) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 18.347153ms)
Dec  2 09:14:11.803: INFO: (5) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 19.116149ms)
Dec  2 09:14:11.811: INFO: (6) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 8.305846ms)
Dec  2 09:14:11.811: INFO: (6) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 7.405356ms)
Dec  2 09:14:11.812: INFO: (6) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.531379ms)
Dec  2 09:14:11.813: INFO: (6) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 8.556507ms)
Dec  2 09:14:11.813: INFO: (6) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 10.112867ms)
Dec  2 09:14:11.813: INFO: (6) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.557047ms)
Dec  2 09:14:11.816: INFO: (6) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 11.362213ms)
Dec  2 09:14:11.816: INFO: (6) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 11.875722ms)
Dec  2 09:14:11.816: INFO: (6) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.049499ms)
Dec  2 09:14:11.816: INFO: (6) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 12.904963ms)
Dec  2 09:14:11.818: INFO: (6) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 14.138356ms)
Dec  2 09:14:11.818: INFO: (6) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 13.612804ms)
Dec  2 09:14:11.819: INFO: (6) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 15.540595ms)
Dec  2 09:14:11.819: INFO: (6) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 15.228579ms)
Dec  2 09:14:11.821: INFO: (6) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 17.696748ms)
Dec  2 09:14:11.821: INFO: (6) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 17.627094ms)
Dec  2 09:14:11.831: INFO: (7) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 7.787987ms)
Dec  2 09:14:11.832: INFO: (7) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 8.570054ms)
Dec  2 09:14:11.832: INFO: (7) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 9.503425ms)
Dec  2 09:14:11.834: INFO: (7) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 8.989783ms)
Dec  2 09:14:11.834: INFO: (7) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.055336ms)
Dec  2 09:14:11.837: INFO: (7) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 13.182636ms)
Dec  2 09:14:11.837: INFO: (7) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 12.421505ms)
Dec  2 09:14:11.837: INFO: (7) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 12.625016ms)
Dec  2 09:14:11.837: INFO: (7) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.835136ms)
Dec  2 09:14:11.837: INFO: (7) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 13.776819ms)
Dec  2 09:14:11.838: INFO: (7) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 14.13824ms)
Dec  2 09:14:11.840: INFO: (7) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 15.509807ms)
Dec  2 09:14:11.841: INFO: (7) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 17.576204ms)
Dec  2 09:14:11.841: INFO: (7) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 16.651232ms)
Dec  2 09:14:11.844: INFO: (7) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 18.916038ms)
Dec  2 09:14:11.844: INFO: (7) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 19.522602ms)
Dec  2 09:14:11.854: INFO: (8) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 10.039556ms)
Dec  2 09:14:11.855: INFO: (8) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 10.156846ms)
Dec  2 09:14:11.855: INFO: (8) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 11.248569ms)
Dec  2 09:14:11.855: INFO: (8) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.984497ms)
Dec  2 09:14:11.855: INFO: (8) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 10.819004ms)
Dec  2 09:14:11.856: INFO: (8) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 10.98092ms)
Dec  2 09:14:11.857: INFO: (8) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 11.916419ms)
Dec  2 09:14:11.857: INFO: (8) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 12.870846ms)
Dec  2 09:14:11.858: INFO: (8) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 12.094372ms)
Dec  2 09:14:11.859: INFO: (8) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 13.047377ms)
Dec  2 09:14:11.859: INFO: (8) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 14.374852ms)
Dec  2 09:14:11.860: INFO: (8) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 14.385365ms)
Dec  2 09:14:11.860: INFO: (8) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 14.669796ms)
Dec  2 09:14:11.861: INFO: (8) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.961412ms)
Dec  2 09:14:11.862: INFO: (8) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 17.135456ms)
Dec  2 09:14:11.866: INFO: (8) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 21.620059ms)
Dec  2 09:14:11.873: INFO: (9) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 7.075447ms)
Dec  2 09:14:11.875: INFO: (9) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 8.600235ms)
Dec  2 09:14:11.875: INFO: (9) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.826472ms)
Dec  2 09:14:11.879: INFO: (9) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.664848ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 15.546774ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 16.02819ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 15.921344ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 16.223733ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 15.996032ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 16.147188ms)
Dec  2 09:14:11.882: INFO: (9) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 16.180464ms)
Dec  2 09:14:11.883: INFO: (9) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 16.478959ms)
Dec  2 09:14:11.883: INFO: (9) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 16.394889ms)
Dec  2 09:14:11.884: INFO: (9) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 17.286067ms)
Dec  2 09:14:11.884: INFO: (9) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 17.397206ms)
Dec  2 09:14:11.885: INFO: (9) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 18.338274ms)
Dec  2 09:14:11.893: INFO: (10) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.121598ms)
Dec  2 09:14:11.894: INFO: (10) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 8.545089ms)
Dec  2 09:14:11.895: INFO: (10) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 9.25322ms)
Dec  2 09:14:11.895: INFO: (10) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 9.966062ms)
Dec  2 09:14:11.896: INFO: (10) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 10.301815ms)
Dec  2 09:14:11.897: INFO: (10) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 10.772684ms)
Dec  2 09:14:11.899: INFO: (10) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 12.791288ms)
Dec  2 09:14:11.899: INFO: (10) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 12.732206ms)
Dec  2 09:14:11.899: INFO: (10) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.898028ms)
Dec  2 09:14:11.899: INFO: (10) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 13.129007ms)
Dec  2 09:14:11.900: INFO: (10) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 14.649253ms)
Dec  2 09:14:11.901: INFO: (10) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 15.894082ms)
Dec  2 09:14:11.901: INFO: (10) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.994224ms)
Dec  2 09:14:11.901: INFO: (10) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 16.156415ms)
Dec  2 09:14:11.902: INFO: (10) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 15.979058ms)
Dec  2 09:14:11.904: INFO: (10) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 18.205051ms)
Dec  2 09:14:11.913: INFO: (11) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 8.00818ms)
Dec  2 09:14:11.914: INFO: (11) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 8.905377ms)
Dec  2 09:14:11.914: INFO: (11) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 7.936063ms)
Dec  2 09:14:11.916: INFO: (11) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 11.931306ms)
Dec  2 09:14:11.920: INFO: (11) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 15.455839ms)
Dec  2 09:14:11.921: INFO: (11) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 15.331642ms)
Dec  2 09:14:11.921: INFO: (11) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 16.273759ms)
Dec  2 09:14:11.921: INFO: (11) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 15.814847ms)
Dec  2 09:14:11.921: INFO: (11) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 15.972787ms)
Dec  2 09:14:11.920: INFO: (11) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.596181ms)
Dec  2 09:14:11.921: INFO: (11) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 16.829963ms)
Dec  2 09:14:11.926: INFO: (11) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 21.222265ms)
Dec  2 09:14:11.926: INFO: (11) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 21.98344ms)
Dec  2 09:14:11.928: INFO: (11) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 23.607336ms)
Dec  2 09:14:11.928: INFO: (11) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 23.334394ms)
Dec  2 09:14:11.928: INFO: (11) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 23.559461ms)
Dec  2 09:14:11.936: INFO: (12) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 8.119894ms)
Dec  2 09:14:11.937: INFO: (12) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 8.503534ms)
Dec  2 09:14:11.939: INFO: (12) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 10.102092ms)
Dec  2 09:14:11.940: INFO: (12) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 10.659172ms)
Dec  2 09:14:11.940: INFO: (12) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 11.124771ms)
Dec  2 09:14:11.944: INFO: (12) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 14.945659ms)
Dec  2 09:14:11.944: INFO: (12) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 15.177232ms)
Dec  2 09:14:11.944: INFO: (12) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 15.071326ms)
Dec  2 09:14:11.946: INFO: (12) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 17.400442ms)
Dec  2 09:14:11.946: INFO: (12) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 17.536506ms)
Dec  2 09:14:11.946: INFO: (12) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 17.572914ms)
Dec  2 09:14:11.947: INFO: (12) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 17.650238ms)
Dec  2 09:14:11.949: INFO: (12) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 20.55352ms)
Dec  2 09:14:11.949: INFO: (12) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 20.787597ms)
Dec  2 09:14:11.949: INFO: (12) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 20.509784ms)
Dec  2 09:14:11.951: INFO: (12) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 21.810661ms)
Dec  2 09:14:11.961: INFO: (13) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 9.521965ms)
Dec  2 09:14:11.961: INFO: (13) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 9.414762ms)
Dec  2 09:14:11.964: INFO: (13) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.457089ms)
Dec  2 09:14:11.964: INFO: (13) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.314828ms)
Dec  2 09:14:11.964: INFO: (13) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 12.860208ms)
Dec  2 09:14:11.965: INFO: (13) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 13.747308ms)
Dec  2 09:14:11.965: INFO: (13) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 13.845709ms)
Dec  2 09:14:11.967: INFO: (13) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 15.609009ms)
Dec  2 09:14:11.967: INFO: (13) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 15.678989ms)
Dec  2 09:14:11.967: INFO: (13) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.963019ms)
Dec  2 09:14:11.967: INFO: (13) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 15.842551ms)
Dec  2 09:14:11.967: INFO: (13) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 16.398394ms)
Dec  2 09:14:11.968: INFO: (13) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 17.146715ms)
Dec  2 09:14:11.970: INFO: (13) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 18.540712ms)
Dec  2 09:14:11.975: INFO: (13) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 23.527182ms)
Dec  2 09:14:11.975: INFO: (13) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 23.550607ms)
Dec  2 09:14:11.983: INFO: (14) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 8.051855ms)
Dec  2 09:14:11.984: INFO: (14) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 8.709494ms)
Dec  2 09:14:11.984: INFO: (14) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 8.328365ms)
Dec  2 09:14:11.984: INFO: (14) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 9.56034ms)
Dec  2 09:14:11.986: INFO: (14) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 10.117478ms)
Dec  2 09:14:11.987: INFO: (14) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 10.75346ms)
Dec  2 09:14:11.988: INFO: (14) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 12.290165ms)
Dec  2 09:14:11.988: INFO: (14) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.66977ms)
Dec  2 09:14:11.990: INFO: (14) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 14.63997ms)
Dec  2 09:14:11.990: INFO: (14) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 14.225172ms)
Dec  2 09:14:11.990: INFO: (14) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 14.487562ms)
Dec  2 09:14:11.990: INFO: (14) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 15.087716ms)
Dec  2 09:14:11.990: INFO: (14) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 13.634129ms)
Dec  2 09:14:11.992: INFO: (14) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.528661ms)
Dec  2 09:14:11.993: INFO: (14) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 16.697195ms)
Dec  2 09:14:11.994: INFO: (14) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 17.39809ms)
Dec  2 09:14:12.002: INFO: (15) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 8.074952ms)
Dec  2 09:14:12.004: INFO: (15) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 8.134957ms)
Dec  2 09:14:12.005: INFO: (15) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.206341ms)
Dec  2 09:14:12.005: INFO: (15) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 10.074806ms)
Dec  2 09:14:12.008: INFO: (15) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.905103ms)
Dec  2 09:14:12.009: INFO: (15) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 12.686382ms)
Dec  2 09:14:12.009: INFO: (15) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 12.285726ms)
Dec  2 09:14:12.009: INFO: (15) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 13.143019ms)
Dec  2 09:14:12.010: INFO: (15) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 13.260101ms)
Dec  2 09:14:12.010: INFO: (15) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 15.596398ms)
Dec  2 09:14:12.010: INFO: (15) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 16.245397ms)
Dec  2 09:14:12.012: INFO: (15) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 17.891124ms)
Dec  2 09:14:12.013: INFO: (15) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 18.438861ms)
Dec  2 09:14:12.013: INFO: (15) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 16.592603ms)
Dec  2 09:14:12.015: INFO: (15) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 20.291806ms)
Dec  2 09:14:12.018: INFO: (15) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 22.686085ms)
Dec  2 09:14:12.027: INFO: (16) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 7.340409ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.848497ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 13.325688ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 11.330088ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 11.713209ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 12.532884ms)
Dec  2 09:14:12.032: INFO: (16) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 11.959921ms)
Dec  2 09:14:12.033: INFO: (16) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 12.327399ms)
Dec  2 09:14:12.033: INFO: (16) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 12.525396ms)
Dec  2 09:14:12.039: INFO: (16) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 19.671594ms)
Dec  2 09:14:12.039: INFO: (16) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 20.514309ms)
Dec  2 09:14:12.039: INFO: (16) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 19.701573ms)
Dec  2 09:14:12.039: INFO: (16) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 20.77867ms)
Dec  2 09:14:12.040: INFO: (16) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 20.724987ms)
Dec  2 09:14:12.040: INFO: (16) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 20.282127ms)
Dec  2 09:14:12.040: INFO: (16) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 19.873982ms)
Dec  2 09:14:12.052: INFO: (17) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.147778ms)
Dec  2 09:14:12.052: INFO: (17) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 9.489623ms)
Dec  2 09:14:12.052: INFO: (17) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 11.787247ms)
Dec  2 09:14:12.053: INFO: (17) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 10.26992ms)
Dec  2 09:14:12.054: INFO: (17) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 11.549246ms)
Dec  2 09:14:12.054: INFO: (17) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 13.127739ms)
Dec  2 09:14:12.054: INFO: (17) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 13.079805ms)
Dec  2 09:14:12.055: INFO: (17) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 12.862803ms)
Dec  2 09:14:12.055: INFO: (17) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 13.490792ms)
Dec  2 09:14:12.058: INFO: (17) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 17.370235ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 17.616552ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 16.677562ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 18.081384ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 16.235846ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 16.908199ms)
Dec  2 09:14:12.059: INFO: (17) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 18.423497ms)
Dec  2 09:14:12.068: INFO: (18) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 8.655657ms)
Dec  2 09:14:12.071: INFO: (18) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 10.668436ms)
Dec  2 09:14:12.071: INFO: (18) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 9.757192ms)
Dec  2 09:14:12.071: INFO: (18) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 10.301703ms)
Dec  2 09:14:12.071: INFO: (18) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 9.547284ms)
Dec  2 09:14:12.072: INFO: (18) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 11.835123ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 17.240357ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 17.789057ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 18.246291ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 17.53498ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 17.443257ms)
Dec  2 09:14:12.079: INFO: (18) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 17.756738ms)
Dec  2 09:14:12.078: INFO: (18) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 17.943743ms)
Dec  2 09:14:12.079: INFO: (18) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 16.87477ms)
Dec  2 09:14:12.081: INFO: (18) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 19.835823ms)
Dec  2 09:14:12.081: INFO: (18) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 20.050069ms)
Dec  2 09:14:12.090: INFO: (19) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">... (200; 7.685254ms)
Dec  2 09:14:12.091: INFO: (19) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:1080/proxy/rewriteme">test<... (200; 8.847598ms)
Dec  2 09:14:12.091: INFO: (19) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 8.883434ms)
Dec  2 09:14:12.092: INFO: (19) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:462/proxy/: tls qux (200; 10.710754ms)
Dec  2 09:14:12.093: INFO: (19) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname2/proxy/: tls qux (200; 11.225182ms)
Dec  2 09:14:12.094: INFO: (19) /api/v1/namespaces/proxy-5158/pods/http:proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 12.322137ms)
Dec  2 09:14:12.094: INFO: (19) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn/proxy/rewriteme">test</a> (200; 12.537154ms)
Dec  2 09:14:12.094: INFO: (19) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:162/proxy/: bar (200; 12.495344ms)
Dec  2 09:14:12.096: INFO: (19) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/: <a href="/api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:443/proxy/tlsrewritem... (200; 14.019127ms)
Dec  2 09:14:12.096: INFO: (19) /api/v1/namespaces/proxy-5158/pods/proxy-service-6tgfr-g9wcn:160/proxy/: foo (200; 13.606292ms)
Dec  2 09:14:12.096: INFO: (19) /api/v1/namespaces/proxy-5158/pods/https:proxy-service-6tgfr-g9wcn:460/proxy/: tls baz (200; 14.7099ms)
Dec  2 09:14:12.098: INFO: (19) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname1/proxy/: foo (200; 15.798593ms)
Dec  2 09:14:12.098: INFO: (19) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname1/proxy/: foo (200; 15.921662ms)
Dec  2 09:14:12.103: INFO: (19) /api/v1/namespaces/proxy-5158/services/https:proxy-service-6tgfr:tlsportname1/proxy/: tls baz (200; 21.61287ms)
Dec  2 09:14:12.103: INFO: (19) /api/v1/namespaces/proxy-5158/services/http:proxy-service-6tgfr:portname2/proxy/: bar (200; 21.365257ms)
Dec  2 09:14:12.104: INFO: (19) /api/v1/namespaces/proxy-5158/services/proxy-service-6tgfr:portname2/proxy/: bar (200; 21.644999ms)
STEP: deleting ReplicationController proxy-service-6tgfr in namespace proxy-5158, will wait for the garbage collector to delete the pods
Dec  2 09:14:12.179: INFO: Deleting ReplicationController proxy-service-6tgfr took: 15.09606ms
Dec  2 09:14:12.280: INFO: Terminating ReplicationController proxy-service-6tgfr pods took: 100.906063ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:14:16.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5158" for this suite.

• [SLOW TEST:12.213 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":20,"skipped":354,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:14:16.581: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubelet-test
W1202 09:14:17.073302      23 warnings.go:70] No static IP address has been configured for the namespace "kubelet-test-145", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:14:25.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-145" for this suite.

• [SLOW TEST:9.271 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:14:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 09:14:26.310425      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-3368", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3368
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-81d7546b-abf2-4ebc-83ce-967391143d9c
STEP: Creating the pod
Dec  2 09:14:26.553: INFO: The status of Pod pod-configmaps-feffcd1f-7944-405e-8277-daf11e8e4abf is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:14:28.564: INFO: The status of Pod pod-configmaps-feffcd1f-7944-405e-8277-daf11e8e4abf is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:14:30.568: INFO: The status of Pod pod-configmaps-feffcd1f-7944-405e-8277-daf11e8e4abf is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-81d7546b-abf2-4ebc-83ce-967391143d9c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:14:32.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3368" for this suite.

• [SLOW TEST:7.280 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":384,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:14:33.133: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 09:14:33.616204      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-3829", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3829
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3829
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3829
Dec  2 09:14:34.346: INFO: Found 0 stateful pods, waiting for 1
Dec  2 09:14:44.359: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  2 09:14:44.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:14:44.906: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:14:44.906: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:14:44.906: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 09:14:44.914: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  2 09:14:54.927: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 09:14:54.927: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 09:14:54.967: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:14:54.967: INFO: ss-0  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  }]
Dec  2 09:14:54.967: INFO: 
Dec  2 09:14:54.967: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  2 09:14:55.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990231022s
Dec  2 09:14:56.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977749813s
Dec  2 09:14:58.007: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962968702s
Dec  2 09:14:59.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950462598s
Dec  2 09:15:00.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934885644s
Dec  2 09:15:01.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.877490912s
Dec  2 09:15:02.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.863084082s
Dec  2 09:15:03.117: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.850290566s
Dec  2 09:15:04.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 840.397634ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3829
Dec  2 09:15:05.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:05.394: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 09:15:05.394: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 09:15:05.394: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 09:15:05.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:05.616: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  2 09:15:05.616: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 09:15:05.616: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 09:15:05.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:06.056: INFO: rc: 1
Dec  2 09:15:06.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec  2 09:15:16.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:16.359: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  2 09:15:16.359: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 09:15:16.359: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 09:15:16.369: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:15:16.369: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:15:16.369: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 09:15:26.386: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:15:26.386: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:15:26.386: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  2 09:15:26.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:15:26.663: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:15:26.663: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:15:26.663: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 09:15:26.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:15:26.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:15:26.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:15:26.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 09:15:26.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:15:27.141: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:15:27.141: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:15:27.141: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 09:15:27.141: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 09:15:27.151: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec  2 09:15:37.172: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 09:15:37.172: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 09:15:37.172: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 09:15:37.199: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:37.199: INFO: ss-0  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  }]
Dec  2 09:15:37.199: INFO: ss-1  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:54 +0000 UTC  }]
Dec  2 09:15:37.199: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:37.199: INFO: 
Dec  2 09:15:37.199: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  2 09:15:38.214: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:38.214: INFO: ss-0  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  }]
Dec  2 09:15:38.215: INFO: ss-1  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:54 +0000 UTC  }]
Dec  2 09:15:38.215: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:38.215: INFO: 
Dec  2 09:15:38.215: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  2 09:15:39.222: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:39.222: INFO: ss-0  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:34 +0000 UTC  }]
Dec  2 09:15:39.222: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:39.222: INFO: 
Dec  2 09:15:39.222: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  2 09:15:40.235: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:40.235: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:40.235: INFO: 
Dec  2 09:15:40.235: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:41.247: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:41.247: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:41.247: INFO: 
Dec  2 09:15:41.247: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:42.262: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:42.262: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:42.263: INFO: 
Dec  2 09:15:42.263: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:43.274: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:43.274: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:43.274: INFO: 
Dec  2 09:15:43.274: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:44.283: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:44.283: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:44.284: INFO: 
Dec  2 09:15:44.284: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:45.296: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:45.296: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:45.296: INFO: 
Dec  2 09:15:45.296: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  2 09:15:46.308: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:15:46.308: INFO: ss-2  a11041ca-3d0d-4c61-a7c9-2e5b598977b5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:15:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:14:55 +0000 UTC  }]
Dec  2 09:15:46.308: INFO: 
Dec  2 09:15:46.308: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3829
Dec  2 09:15:47.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:48.548: INFO: rc: 1
Dec  2 09:15:48.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec  2 09:15:58.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:15:58.661: INFO: rc: 1
Dec  2 09:15:58.661: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:08.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:08.802: INFO: rc: 1
Dec  2 09:16:08.802: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:18.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:18.889: INFO: rc: 1
Dec  2 09:16:18.890: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:28.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:28.992: INFO: rc: 1
Dec  2 09:16:28.992: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:38.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:39.102: INFO: rc: 1
Dec  2 09:16:39.102: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:49.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:49.192: INFO: rc: 1
Dec  2 09:16:49.192: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:16:59.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:16:59.297: INFO: rc: 1
Dec  2 09:16:59.297: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:09.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:09.396: INFO: rc: 1
Dec  2 09:17:09.396: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:19.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:19.490: INFO: rc: 1
Dec  2 09:17:19.490: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:29.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:29.596: INFO: rc: 1
Dec  2 09:17:29.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:39.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:39.708: INFO: rc: 1
Dec  2 09:17:39.708: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:49.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:49.799: INFO: rc: 1
Dec  2 09:17:49.799: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:17:59.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:17:59.896: INFO: rc: 1
Dec  2 09:17:59.896: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:18:09.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:18:09.983: INFO: rc: 1
Dec  2 09:18:09.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:18:19.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:18:20.073: INFO: rc: 1
Dec  2 09:18:20.073: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:18:30.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:18:30.157: INFO: rc: 1
Dec  2 09:18:30.157: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:18:40.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:18:40.253: INFO: rc: 1
Dec  2 09:18:40.253: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:18:50.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:18:50.353: INFO: rc: 1
Dec  2 09:18:50.353: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:00.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:00.470: INFO: rc: 1
Dec  2 09:19:00.470: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:10.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:10.607: INFO: rc: 1
Dec  2 09:19:10.607: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:20.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:20.696: INFO: rc: 1
Dec  2 09:19:20.696: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:30.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:30.805: INFO: rc: 1
Dec  2 09:19:30.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:40.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:40.895: INFO: rc: 1
Dec  2 09:19:40.895: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:19:50.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:19:50.987: INFO: rc: 1
Dec  2 09:19:50.987: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:00.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:01.092: INFO: rc: 1
Dec  2 09:20:01.092: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:11.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:11.197: INFO: rc: 1
Dec  2 09:20:11.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:21.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:21.302: INFO: rc: 1
Dec  2 09:20:21.302: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:31.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:31.390: INFO: rc: 1
Dec  2 09:20:31.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:41.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:41.505: INFO: rc: 1
Dec  2 09:20:41.506: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  2 09:20:51.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-3829 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:20:51.600: INFO: rc: 1
Dec  2 09:20:51.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Dec  2 09:20:51.600: INFO: Scaling statefulset ss to 0
Dec  2 09:20:51.647: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 09:20:51.653: INFO: Deleting all statefulset in ns statefulset-3829
Dec  2 09:20:51.658: INFO: Scaling statefulset ss to 0
Dec  2 09:20:51.678: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 09:20:51.685: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:20:51.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3829" for this suite.

• [SLOW TEST:379.123 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":23,"skipped":386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:20:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption
W1202 09:20:52.720301      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-4116", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Dec  2 09:20:54.943: INFO: pods: 0 < 3
Dec  2 09:20:56.956: INFO: running pods: 0 < 3
Dec  2 09:20:58.956: INFO: running pods: 0 < 3
Dec  2 09:21:00.956: INFO: running pods: 2 < 3
Dec  2 09:21:02.959: INFO: running pods: 2 < 3
Dec  2 09:21:04.954: INFO: running pods: 2 < 3
Dec  2 09:21:06.957: INFO: running pods: 2 < 3
Dec  2 09:21:08.957: INFO: running pods: 2 < 3
Dec  2 09:21:10.959: INFO: running pods: 2 < 3
Dec  2 09:21:12.958: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec  2 09:21:19.145: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:21:21.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4116" for this suite.

• [SLOW TEST:29.457 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":24,"skipped":420,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:21:21.725: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename endpointslice
W1202 09:21:22.322488      23 warnings.go:70] No static IP address has been configured for the namespace "endpointslice-4182", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-4182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  2 09:21:22.551: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  2 09:21:22.572: INFO: starting watch
STEP: patching
STEP: updating
Dec  2 09:21:22.600: INFO: waiting for watch events with expected annotations
Dec  2 09:21:22.601: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:21:22.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4182" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":25,"skipped":421,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:21:23.197: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:21:23.700938      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-4999", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec  2 09:21:23.898: INFO: namespace kubectl-4999
Dec  2 09:21:23.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4999 create -f -'
Dec  2 09:21:25.882: INFO: stderr: ""
Dec  2 09:21:25.882: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  2 09:21:26.904: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:26.904: INFO: Found 0 / 1
Dec  2 09:21:27.894: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:27.894: INFO: Found 0 / 1
Dec  2 09:21:28.897: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:28.897: INFO: Found 0 / 1
Dec  2 09:21:29.893: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:29.893: INFO: Found 0 / 1
Dec  2 09:21:30.894: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:30.894: INFO: Found 0 / 1
Dec  2 09:21:31.890: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:31.890: INFO: Found 0 / 1
Dec  2 09:21:32.896: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:32.896: INFO: Found 0 / 1
Dec  2 09:21:33.892: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:33.892: INFO: Found 0 / 1
Dec  2 09:21:34.894: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:34.894: INFO: Found 0 / 1
Dec  2 09:21:35.893: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:35.893: INFO: Found 1 / 1
Dec  2 09:21:35.893: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  2 09:21:35.905: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:21:35.905: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  2 09:21:35.905: INFO: wait on agnhost-primary startup in kubectl-4999 
Dec  2 09:21:35.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4999 logs agnhost-primary-wg5gn agnhost-primary'
Dec  2 09:21:36.526: INFO: stderr: ""
Dec  2 09:21:36.526: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec  2 09:21:36.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4999 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec  2 09:21:37.097: INFO: stderr: ""
Dec  2 09:21:37.097: INFO: stdout: "service/rm2 exposed\n"
Dec  2 09:21:37.114: INFO: Service rm2 in namespace kubectl-4999 found.
STEP: exposing service
Dec  2 09:21:39.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4999 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec  2 09:21:39.743: INFO: stderr: ""
Dec  2 09:21:39.743: INFO: stdout: "service/rm3 exposed\n"
Dec  2 09:21:39.752: INFO: Service rm3 in namespace kubectl-4999 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:21:41.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4999" for this suite.

• [SLOW TEST:19.031 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":26,"skipped":430,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:21:42.228: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 09:21:42.731135      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-1214", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  2 09:21:42.949: INFO: Waiting up to 5m0s for pod "pod-33135489-f181-4f2c-8aff-8789523f0803" in namespace "emptydir-1214" to be "Succeeded or Failed"
Dec  2 09:21:42.955: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803": Phase="Pending", Reason="", readiness=false. Elapsed: 6.648877ms
Dec  2 09:21:44.966: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017021084s
Dec  2 09:21:46.982: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033460845s
Dec  2 09:21:48.996: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046994005s
Dec  2 09:21:51.008: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059521524s
STEP: Saw pod success
Dec  2 09:21:51.008: INFO: Pod "pod-33135489-f181-4f2c-8aff-8789523f0803" satisfied condition "Succeeded or Failed"
Dec  2 09:21:51.015: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-33135489-f181-4f2c-8aff-8789523f0803 container test-container: <nil>
STEP: delete the pod
Dec  2 09:21:51.071: INFO: Waiting for pod pod-33135489-f181-4f2c-8aff-8789523f0803 to disappear
Dec  2 09:21:51.077: INFO: Pod pod-33135489-f181-4f2c-8aff-8789523f0803 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:21:51.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1214" for this suite.

• [SLOW TEST:9.325 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:21:51.554: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 09:21:51.978284      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-3271", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:21:52.175: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  2 09:21:57.193: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  2 09:21:57.193: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 09:21:57.234: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3271  f92d1567-63de-4e17-9ed3-b521b25ebcd8 52157485 1 2022-12-02 09:21:57 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-12-02 09:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b60aa28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec  2 09:21:57.240: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec  2 09:21:57.241: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec  2 09:21:57.241: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3271  23ac6d92-031a-46b7-a8b7-6b897ac2ded4 52157486 1 2022-12-02 09:21:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f92d1567-63de-4e17-9ed3-b521b25ebcd8 0xc00b60ad87 0xc00b60ad88}] []  [{e2e.test Update apps/v1 2022-12-02 09:21:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:21:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-02 09:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f92d1567-63de-4e17-9ed3-b521b25ebcd8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b60ae48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  2 09:21:57.249: INFO: Pod "test-cleanup-controller-h4b8p" is available:
&Pod{ObjectMeta:{test-cleanup-controller-h4b8p test-cleanup-controller- deployment-3271  866645e8-e9f5-4185-8371-0c947767c9c6 52157483 0 2022-12-02 09:21:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 23ac6d92-031a-46b7-a8b7-6b897ac2ded4 0xc00b60b157 0xc00b60b158}] []  [{kube-controller-manager Update v1 2022-12-02 09:21:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23ac6d92-031a-46b7-a8b7-6b897ac2ded4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 09:21:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.28.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rwq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rwq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.28.2,StartTime:2022-12-02 09:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 09:21:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://0c19c3d843bf86464e83c745e640fc6ea134da255853751b8b5347d28367858c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.28.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:21:57.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3271" for this suite.

• [SLOW TEST:6.152 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":28,"skipped":476,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:21:57.706: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:21:58.174134      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-4033", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:21:58.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 create -f -'
Dec  2 09:21:58.742: INFO: stderr: ""
Dec  2 09:21:58.742: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec  2 09:21:58.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 create -f -'
Dec  2 09:21:59.546: INFO: stderr: ""
Dec  2 09:21:59.546: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  2 09:22:00.562: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:00.562: INFO: Found 0 / 1
Dec  2 09:22:01.556: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:01.556: INFO: Found 0 / 1
Dec  2 09:22:02.558: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:02.558: INFO: Found 0 / 1
Dec  2 09:22:03.556: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:03.556: INFO: Found 0 / 1
Dec  2 09:22:04.555: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:04.556: INFO: Found 0 / 1
Dec  2 09:22:05.561: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:05.561: INFO: Found 0 / 1
Dec  2 09:22:06.559: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:06.559: INFO: Found 1 / 1
Dec  2 09:22:06.559: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  2 09:22:06.567: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 09:22:06.567: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  2 09:22:06.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 describe pod agnhost-primary-q2btk'
Dec  2 09:22:06.694: INFO: stderr: ""
Dec  2 09:22:06.694: INFO: stdout: "Name:         agnhost-primary-q2btk\nNamespace:    kubectl-4033\nPriority:     0\nNode:         d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa/11.0.95.5\nStart Time:   Fri, 02 Dec 2022 09:21:58 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           11.34.27.2\nIPs:\n  IP:           11.34.27.2\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://530a3780bbfa813239a62cc165b4bf4b63b42641452c92100219b32db959a934\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       docker://sha256:a05bd3a9140b72c5a17eb6881a75c5003b270c0b3e32a995fb10ec96004279d2\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 02 Dec 2022 09:22:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6ncvg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-6ncvg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  7s    default-scheduler  Successfully assigned kubectl-4033/agnhost-primary-q2btk to d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Dec  2 09:22:06.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 describe rc agnhost-primary'
Dec  2 09:22:06.829: INFO: stderr: ""
Dec  2 09:22:06.829: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4033\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: agnhost-primary-q2btk\n"
Dec  2 09:22:06.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 describe service agnhost-primary'
Dec  2 09:22:06.952: INFO: stderr: ""
Dec  2 09:22:06.952: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4033\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.218.110\nIPs:               10.100.218.110\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         11.34.27.2:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  2 09:22:06.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 describe node 5e47d0ea-e90b-466b-b6de-2748d512ebf3'
Dec  2 09:22:07.245: INFO: stderr: ""
Dec  2 09:22:07.245: INFO: stdout: "Name:               5e47d0ea-e90b-466b-b6de-2748d512ebf3\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    bosh.id=69016753-fc20-4cfe-8fef-41cc180f7a59\n                    bosh.zone=WL-ZHH-Prd-01\n                    failure-domain.beta.kubernetes.io/zone=WL-ZHH-Prd-01\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=11.0.95.7\n                    kubernetes.io/os=linux\n                    pks-system/cluster.name=696b7562-0ddb-4977-8d03-3d8b3339c594\n                    pks-system/cluster.uuid=service-instance_a47f2da7-1f39-4727-b90e-53d02dc4410b\n                    spec.ip=11.0.95.7\n                    topology.kubernetes.io/zone=WL-ZHH-Prd-01\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 01 Dec 2022 00:27:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  5e47d0ea-e90b-466b-b6de-2748d512ebf3\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 02 Dec 2022 09:22:02 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 02 Dec 2022 09:20:22 +0000   Thu, 01 Dec 2022 00:27:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 02 Dec 2022 09:20:22 +0000   Thu, 01 Dec 2022 00:27:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 02 Dec 2022 09:20:22 +0000   Thu, 01 Dec 2022 00:27:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 02 Dec 2022 09:20:22 +0000   Thu, 01 Dec 2022 00:27:45 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  11.0.95.7\n  InternalIP:  11.0.95.7\n  Hostname:    11.0.95.7\nCapacity:\n  cpu:                4\n  ephemeral-storage:  32845776Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16425556Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  30270667112\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16323156Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3fc11687db0249239593faba0aaa2578\n  System UUID:                422AA141-CFF0-FDFF-FF38-179A46E66192\n  Boot ID:                    b3d8c7d3-c7c8-4751-aa73-5e45a3e4817d\n  Kernel Version:             4.15.0-194-generic\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.9\n  Kubelet Version:            v1.22.15+vmware.1\n  Kube-Proxy Version:         v1.22.15+vmware.1\nProviderID:                   vsphere://422aa141-cff0-fdff-ff38-179a46e66192\nNon-terminated Pods:          (51 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits    Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------    ---------------  -------------  ---\n  argo                        workflow-controller-c8c847777-jqldr                        0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  benchmarks                  test4                                                      0 (0%)        0 (0%)        0 (0%)           0 (0%)         16h\n  cert-manager                cert-manager-6bdfc96d57-rlph5                              0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  cert-manager                cert-manager-cainjector-786868d5bb-xwszl                   0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-1           harbor-portal-58c8bb4658-fkwvh                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-100         harbor-portal-58c8bb4658-qv4fm                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-2           harbor-portal-58c8bb4658-5ttt4                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-2           harbor-trivy-0                                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-600         harbor-exporter-6fcd6687f5-8kx6c                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-600         harbor-jobservice-67df6d9455-jhpzg                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-600         harbor-portal-58c8bb4658-jlhwl                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-700         harbor-exporter-6fcd6687f5-r6pkx                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-700         harbor-jobservice-67df6d9455-tf88h                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-700         harbor-portal-58c8bb4658-p9gn2                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-701         harbor-exporter-6fcd6687f5-bs6vf                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-702         harbor-exporter-6fcd6687f5-xxgh9                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-702         harbor-portal-58c8bb4658-lcwzz                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-702         harbor-redis-0                                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-702         harbor-trivy-0                                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-703         harbor-portal-58c8bb4658-v5wsh                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-705         harbor-jobservice-67df6d9455-7pgtc                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-706         harbor-jobservice-67df6d9455-xnt5m                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-706         harbor-portal-58c8bb4658-mrwcm                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor-restored-710         harbor-portal-58c8bb4658-klrzj                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor                      harbor-exporter-6fcd6687f5-v6tpb                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor                      harbor-jobservice-67df6d9455-fkkxh                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  harbor                      harbor-portal-58c8bb4658-p4vsw                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  ingress-gateway-management  ingress-gateway-management-c6c49cf-fv6pt                   0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  istio-system                kiali-operator-9d8cbb844-4xl4k                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  kube-plus-docs              docs-67854bcfd4-w5qzf                                      0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  kube-system                 coredns-6c4c59f5b4-qzbdz                                   100m (2%)     0 (0%)        70Mi (0%)        170Mi (1%)     32h\n  kube-system                 metrics-server-757c77bdc5-ntt9r                            0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  logging                     logging-operator-56b5fcc8bd-dq52h                          0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  logging                     loki-0                                                     0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  logging                     promtail-rjgl7                                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  monitoring                  alertmanager-main-2                                        204m (5%)     2200m (55%)   278Mi (1%)       1174Mi (7%)    32h\n  monitoring                  blackbox-exporter-685b55655c-lvb65                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  monitoring                  node-exporter-wh467                                        0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  monitoring                  prometheus-k8s-0                                           450m (11%)    4100m (102%)  690Mi (4%)       5074Mi (31%)   32h\n  monitoring                  prometheus-operator-6984ff874-mmwsj                        0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  event-controller-7db654d86f-cbbkq                          0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  fluent-bit-4ftmx                                           0 (0%)        0 (0%)        100Mi (0%)       100Mi (0%)     32h\n  pks-system                  metric-controller-7b78cd6c98-wpvtw                         0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  node-exporter-v9sds                                        10m (0%)      0 (0%)        50Mi (0%)        150Mi (0%)     32h\n  pks-system                  observability-manager-fc646c85f-86sdt                      0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  sink-controller-78ccbc6f4d-4bpkg                           0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  telegraf-wrn6w                                             0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  pks-system                  validator-5675b6544f-gjtj8                                 0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25    0 (0%)        0 (0%)        0 (0%)           0 (0%)         18m\n  velero                      restic-s8c5w                                               0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\n  velero                      velero-69f6b65985-tl2tq                                    0 (0%)        0 (0%)        0 (0%)           0 (0%)         32h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                764m (19%)   6300m (157%)\n  memory             1188Mi (7%)  6668Mi (41%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Dec  2 09:22:07.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4033 describe namespace kubectl-4033'
Dec  2 09:22:07.367: INFO: stderr: ""
Dec  2 09:22:07.367: INFO: stdout: "Name:         kubectl-4033\nLabels:       e2e-framework=kubectl\n              e2e-run=6ce530b9-2879-4c46-af98-d08e31ab28b1\n              kubernetes.io/metadata.name=kubectl-4033\nAnnotations:  ncp/extpoolid: 6f4d33b8-7002-4309-a23b-678749c904f7\n              ncp/snat_ip: 10.144.82.115\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:22:07.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4033" for this suite.

• [SLOW TEST:10.111 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1094
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":29,"skipped":479,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:22:07.818: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename tables
W1202 09:22:08.284897      23 warnings.go:70] No static IP address has been configured for the namespace "tables-8798", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:22:08.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8798" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":30,"skipped":490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:22:08.969: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 09:22:09.578653      23 warnings.go:70] No static IP address has been configured for the namespace "services-6846", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:22:09.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6846" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":31,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:22:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:22:11.782013      23 warnings.go:70] No static IP address has been configured for the namespace "projected-5165", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5165
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-62e3a024-20e8-499d-81f9-e6fbeea01aff
STEP: Creating secret with name s-test-opt-upd-114a123b-eb70-45c5-a096-34e0211c2d91
STEP: Creating the pod
Dec  2 09:22:12.034: INFO: The status of Pod pod-projected-secrets-1e1733f5-add5-489c-9a57-6ee2076ad83c is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:22:14.042: INFO: The status of Pod pod-projected-secrets-1e1733f5-add5-489c-9a57-6ee2076ad83c is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:22:16.045: INFO: The status of Pod pod-projected-secrets-1e1733f5-add5-489c-9a57-6ee2076ad83c is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:22:18.047: INFO: The status of Pod pod-projected-secrets-1e1733f5-add5-489c-9a57-6ee2076ad83c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-62e3a024-20e8-499d-81f9-e6fbeea01aff
STEP: Updating secret s-test-opt-upd-114a123b-eb70-45c5-a096-34e0211c2d91
STEP: Creating secret with name s-test-opt-create-a3dd303a-8c7d-45d0-a168-f991acb97bc3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:23:47.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5165" for this suite.

• [SLOW TEST:96.411 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":573,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:23:47.639: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:23:48.161440      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-3091", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-58abfa2a-bf63-4f1d-876c-bfefd345e9e1
STEP: Creating a pod to test consume secrets
Dec  2 09:23:48.363: INFO: Waiting up to 5m0s for pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb" in namespace "secrets-3091" to be "Succeeded or Failed"
Dec  2 09:23:48.373: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.286206ms
Dec  2 09:23:50.387: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022978617s
Dec  2 09:23:52.401: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037790622s
Dec  2 09:23:54.419: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05576031s
Dec  2 09:23:56.434: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070887421s
STEP: Saw pod success
Dec  2 09:23:56.435: INFO: Pod "pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb" satisfied condition "Succeeded or Failed"
Dec  2 09:23:56.441: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 09:23:56.494: INFO: Waiting for pod pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb to disappear
Dec  2 09:23:56.501: INFO: Pod pod-secrets-dcc3c4fd-7ef6-4a22-ae72-b58bd56010bb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:23:56.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3091" for this suite.

• [SLOW TEST:9.687 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":573,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:23:57.327: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:23:57.818178      23 warnings.go:70] No static IP address has been configured for the namespace "projected-4946", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9b1007b7-c2c0-460b-b311-9ec89caafbf7
STEP: Creating a pod to test consume configMaps
Dec  2 09:23:58.029: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40" in namespace "projected-4946" to be "Succeeded or Failed"
Dec  2 09:23:58.035: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027785ms
Dec  2 09:24:00.123: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094026966s
Dec  2 09:24:02.137: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10766543s
Dec  2 09:24:04.152: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.12312794s
Dec  2 09:24:06.164: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.135097521s
STEP: Saw pod success
Dec  2 09:24:06.164: INFO: Pod "pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40" satisfied condition "Succeeded or Failed"
Dec  2 09:24:06.171: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 09:24:06.222: INFO: Waiting for pod pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40 to disappear
Dec  2 09:24:06.232: INFO: Pod pod-projected-configmaps-fb9cad23-064a-4df5-ba57-759e1007de40 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:24:06.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4946" for this suite.

• [SLOW TEST:9.383 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":576,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:24:06.711: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 09:24:07.259694      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-6433", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-da9c99b4-705d-4679-95b5-62b9362111c3 in namespace container-probe-6433
Dec  2 09:24:13.504: INFO: Started pod liveness-da9c99b4-705d-4679-95b5-62b9362111c3 in namespace container-probe-6433
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 09:24:13.512: INFO: Initial restart count of pod liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is 0
Dec  2 09:24:29.628: INFO: Restart count of pod container-probe-6433/liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is now 1 (16.116164523s elapsed)
Dec  2 09:24:49.754: INFO: Restart count of pod container-probe-6433/liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is now 2 (36.24269461s elapsed)
Dec  2 09:25:09.892: INFO: Restart count of pod container-probe-6433/liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is now 3 (56.380322812s elapsed)
Dec  2 09:25:30.015: INFO: Restart count of pod container-probe-6433/liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is now 4 (1m16.503045078s elapsed)
Dec  2 09:25:50.142: INFO: Restart count of pod container-probe-6433/liveness-da9c99b4-705d-4679-95b5-62b9362111c3 is now 5 (1m36.630100416s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:25:50.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6433" for this suite.

• [SLOW TEST:104.026 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":578,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:25:50.737: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 09:25:51.224209      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-3255", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  2 09:25:51.432: INFO: Waiting up to 5m0s for pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375" in namespace "downward-api-3255" to be "Succeeded or Failed"
Dec  2 09:25:51.458: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 25.192038ms
Dec  2 09:25:53.470: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037688885s
Dec  2 09:25:55.487: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05477627s
Dec  2 09:25:57.501: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068694591s
Dec  2 09:25:59.514: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081884327s
Dec  2 09:26:01.528: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095306608s
Dec  2 09:26:03.542: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Pending", Reason="", readiness=false. Elapsed: 12.109252177s
Dec  2 09:26:05.557: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.125088353s
STEP: Saw pod success
Dec  2 09:26:05.558: INFO: Pod "downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375" satisfied condition "Succeeded or Failed"
Dec  2 09:26:05.564: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375 container dapi-container: <nil>
STEP: delete the pod
Dec  2 09:26:05.621: INFO: Waiting for pod downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375 to disappear
Dec  2 09:26:05.630: INFO: Pod downward-api-e8730510-dbd9-434f-bee7-cdfd258c7375 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:26:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3255" for this suite.

• [SLOW TEST:15.402 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:26:06.140: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:26:06.587222      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-5084", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec  2 09:26:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 create -f -'
Dec  2 09:26:09.429: INFO: stderr: ""
Dec  2 09:26:09.429: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  2 09:26:09.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:09.509: INFO: stderr: ""
Dec  2 09:26:09.509: INFO: stdout: "update-demo-nautilus-lgv4f update-demo-nautilus-xc8j7 "
Dec  2 09:26:09.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-lgv4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:09.600: INFO: stderr: ""
Dec  2 09:26:09.600: INFO: stdout: ""
Dec  2 09:26:09.600: INFO: update-demo-nautilus-lgv4f is created but not running
Dec  2 09:26:14.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:14.693: INFO: stderr: ""
Dec  2 09:26:14.693: INFO: stdout: "update-demo-nautilus-lgv4f update-demo-nautilus-xc8j7 "
Dec  2 09:26:14.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-lgv4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:14.819: INFO: stderr: ""
Dec  2 09:26:14.819: INFO: stdout: ""
Dec  2 09:26:14.819: INFO: update-demo-nautilus-lgv4f is created but not running
Dec  2 09:26:19.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:19.905: INFO: stderr: ""
Dec  2 09:26:19.905: INFO: stdout: "update-demo-nautilus-lgv4f update-demo-nautilus-xc8j7 "
Dec  2 09:26:19.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-lgv4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:19.985: INFO: stderr: ""
Dec  2 09:26:19.985: INFO: stdout: ""
Dec  2 09:26:19.985: INFO: update-demo-nautilus-lgv4f is created but not running
Dec  2 09:26:24.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:25.113: INFO: stderr: ""
Dec  2 09:26:25.113: INFO: stdout: "update-demo-nautilus-lgv4f update-demo-nautilus-xc8j7 "
Dec  2 09:26:25.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-lgv4f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:25.207: INFO: stderr: ""
Dec  2 09:26:25.207: INFO: stdout: "true"
Dec  2 09:26:25.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-lgv4f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:26:25.289: INFO: stderr: ""
Dec  2 09:26:25.289: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:26:25.289: INFO: validating pod update-demo-nautilus-lgv4f
Dec  2 09:26:25.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:26:25.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:26:25.311: INFO: update-demo-nautilus-lgv4f is verified up and running
Dec  2 09:26:25.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:25.399: INFO: stderr: ""
Dec  2 09:26:25.399: INFO: stdout: "true"
Dec  2 09:26:25.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:26:25.498: INFO: stderr: ""
Dec  2 09:26:25.498: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:26:25.498: INFO: validating pod update-demo-nautilus-xc8j7
Dec  2 09:26:25.520: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:26:25.520: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:26:25.520: INFO: update-demo-nautilus-xc8j7 is verified up and running
STEP: scaling down the replication controller
Dec  2 09:26:25.524: INFO: scanned /root for discovery docs: <nil>
Dec  2 09:26:25.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec  2 09:26:26.652: INFO: stderr: ""
Dec  2 09:26:26.652: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  2 09:26:26.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:26.745: INFO: stderr: ""
Dec  2 09:26:26.745: INFO: stdout: "update-demo-nautilus-lgv4f update-demo-nautilus-xc8j7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  2 09:26:31.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:31.881: INFO: stderr: ""
Dec  2 09:26:31.881: INFO: stdout: "update-demo-nautilus-xc8j7 "
Dec  2 09:26:31.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:31.975: INFO: stderr: ""
Dec  2 09:26:31.975: INFO: stdout: "true"
Dec  2 09:26:31.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:26:32.071: INFO: stderr: ""
Dec  2 09:26:32.071: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:26:32.071: INFO: validating pod update-demo-nautilus-xc8j7
Dec  2 09:26:32.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:26:32.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:26:32.080: INFO: update-demo-nautilus-xc8j7 is verified up and running
STEP: scaling up the replication controller
Dec  2 09:26:32.083: INFO: scanned /root for discovery docs: <nil>
Dec  2 09:26:32.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec  2 09:26:33.215: INFO: stderr: ""
Dec  2 09:26:33.215: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  2 09:26:33.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:33.310: INFO: stderr: ""
Dec  2 09:26:33.310: INFO: stdout: "update-demo-nautilus-sm9mm update-demo-nautilus-xc8j7 "
Dec  2 09:26:33.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-sm9mm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:33.382: INFO: stderr: ""
Dec  2 09:26:33.382: INFO: stdout: ""
Dec  2 09:26:33.382: INFO: update-demo-nautilus-sm9mm is created but not running
Dec  2 09:26:38.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:38.483: INFO: stderr: ""
Dec  2 09:26:38.484: INFO: stdout: "update-demo-nautilus-sm9mm update-demo-nautilus-xc8j7 "
Dec  2 09:26:38.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-sm9mm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:38.580: INFO: stderr: ""
Dec  2 09:26:38.580: INFO: stdout: ""
Dec  2 09:26:38.580: INFO: update-demo-nautilus-sm9mm is created but not running
Dec  2 09:26:43.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:26:43.675: INFO: stderr: ""
Dec  2 09:26:43.675: INFO: stdout: "update-demo-nautilus-sm9mm update-demo-nautilus-xc8j7 "
Dec  2 09:26:43.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-sm9mm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:43.775: INFO: stderr: ""
Dec  2 09:26:43.775: INFO: stdout: "true"
Dec  2 09:26:43.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-sm9mm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:26:43.881: INFO: stderr: ""
Dec  2 09:26:43.881: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:26:43.881: INFO: validating pod update-demo-nautilus-sm9mm
Dec  2 09:26:43.891: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:26:43.891: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:26:43.891: INFO: update-demo-nautilus-sm9mm is verified up and running
Dec  2 09:26:43.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:26:43.964: INFO: stderr: ""
Dec  2 09:26:43.964: INFO: stdout: "true"
Dec  2 09:26:43.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods update-demo-nautilus-xc8j7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:26:44.044: INFO: stderr: ""
Dec  2 09:26:44.044: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:26:44.044: INFO: validating pod update-demo-nautilus-xc8j7
Dec  2 09:26:44.054: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:26:44.054: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:26:44.054: INFO: update-demo-nautilus-xc8j7 is verified up and running
STEP: using delete to clean up resources
Dec  2 09:26:44.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 delete --grace-period=0 --force -f -'
Dec  2 09:26:44.143: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 09:26:44.143: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  2 09:26:44.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get rc,svc -l name=update-demo --no-headers'
Dec  2 09:26:44.243: INFO: stderr: "No resources found in kubectl-5084 namespace.\n"
Dec  2 09:26:44.243: INFO: stdout: ""
Dec  2 09:26:44.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5084 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  2 09:26:44.351: INFO: stderr: ""
Dec  2 09:26:44.351: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:26:44.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5084" for this suite.

• [SLOW TEST:38.661 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":37,"skipped":620,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:26:44.802: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:26:45.273672      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-3669", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-3669/secret-test-9470884f-f306-43da-bc1c-e95106a32548
STEP: Creating a pod to test consume secrets
Dec  2 09:26:45.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030" in namespace "secrets-3669" to be "Succeeded or Failed"
Dec  2 09:26:45.486: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030": Phase="Pending", Reason="", readiness=false. Elapsed: 7.047121ms
Dec  2 09:26:47.496: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017489309s
Dec  2 09:26:49.508: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028798786s
Dec  2 09:26:51.518: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039684036s
Dec  2 09:26:53.530: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050909071s
STEP: Saw pod success
Dec  2 09:26:53.530: INFO: Pod "pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030" satisfied condition "Succeeded or Failed"
Dec  2 09:26:53.536: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030 container env-test: <nil>
STEP: delete the pod
Dec  2 09:26:53.592: INFO: Waiting for pod pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030 to disappear
Dec  2 09:26:53.598: INFO: Pod pod-configmaps-284617e9-67d1-4931-9f5f-22e03b7f2030 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:26:53.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3669" for this suite.

• [SLOW TEST:9.295 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:26:54.097: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 09:26:54.521196      23 warnings.go:70] No static IP address has been configured for the namespace "gc-1220", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  2 09:27:00.769: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W1202 09:27:00.769261      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  2 09:27:00.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1220" for this suite.

• [SLOW TEST:7.232 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":39,"skipped":658,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:01.334: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 09:27:01.851434      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-1591", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:27:18.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1591" for this suite.

• [SLOW TEST:17.447 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":40,"skipped":674,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:18.781: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replication-controller
W1202 09:27:19.278538      23 warnings.go:70] No static IP address has been configured for the namespace "replication-controller-8705", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c
Dec  2 09:27:19.480: INFO: Pod name my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c: Found 0 pods out of 1
Dec  2 09:27:24.499: INFO: Pod name my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c: Found 1 pods out of 1
Dec  2 09:27:24.499: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c" are running
Dec  2 09:27:32.521: INFO: Pod "my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c-hcq68" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:27:19 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:27:19 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:27:19 +0000 UTC Reason: Message:}])
Dec  2 09:27:32.521: INFO: Trying to dial the pod
Dec  2 09:27:37.554: INFO: Controller my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c: Got expected result from replica 1 [my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c-hcq68]: "my-hostname-basic-ee42249e-13fc-4189-b420-d56914a19b6c-hcq68", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:27:37.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8705" for this suite.

• [SLOW TEST:19.258 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":41,"skipped":694,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:27:38.511293      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-8058", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Dec  2 09:27:38.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8058 create -f -'
Dec  2 09:27:40.266: INFO: stderr: ""
Dec  2 09:27:40.266: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec  2 09:27:40.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8058 diff -f -'
Dec  2 09:27:40.585: INFO: rc: 1
Dec  2 09:27:40.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8058 delete -f -'
Dec  2 09:27:40.683: INFO: stderr: ""
Dec  2 09:27:40.683: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:27:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8058" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":42,"skipped":711,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:41.198: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 09:27:41.700239      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-8984", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Dec  2 09:27:41.927: INFO: Waiting up to 5m0s for pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7" in namespace "svcaccounts-8984" to be "Succeeded or Failed"
Dec  2 09:27:41.934: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.274808ms
Dec  2 09:27:43.949: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021773113s
Dec  2 09:27:45.961: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034050796s
Dec  2 09:27:47.973: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046278713s
Dec  2 09:27:49.986: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058885344s
STEP: Saw pod success
Dec  2 09:27:49.986: INFO: Pod "test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7" satisfied condition "Succeeded or Failed"
Dec  2 09:27:49.994: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 09:27:50.044: INFO: Waiting for pod test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7 to disappear
Dec  2 09:27:50.055: INFO: Pod test-pod-daccacbe-9d56-45c8-b739-e42eab54c7b7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:27:50.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8984" for this suite.

• [SLOW TEST:9.354 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":43,"skipped":722,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:50.552: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:27:51.072026      23 warnings.go:70] No static IP address has been configured for the namespace "projected-4787", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-9e721b4c-a01c-41ac-95a5-f9e0b51d31da
STEP: Creating a pod to test consume secrets
Dec  2 09:27:51.293: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2" in namespace "projected-4787" to be "Succeeded or Failed"
Dec  2 09:27:51.300: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.61487ms
Dec  2 09:27:53.311: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018159067s
Dec  2 09:27:55.324: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031179931s
Dec  2 09:27:57.334: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04111597s
Dec  2 09:27:59.351: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.057708045s
STEP: Saw pod success
Dec  2 09:27:59.351: INFO: Pod "pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2" satisfied condition "Succeeded or Failed"
Dec  2 09:27:59.358: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  2 09:27:59.399: INFO: Waiting for pod pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2 to disappear
Dec  2 09:27:59.407: INFO: Pod pod-projected-secrets-ae515e61-27fc-497b-8bb8-b4a6b90a4ba2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:27:59.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4787" for this suite.

• [SLOW TEST:9.338 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:27:59.892: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename subpath
W1202 09:28:00.474464      23 warnings.go:70] No static IP address has been configured for the namespace "subpath-3495", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-xgvw
STEP: Creating a pod to test atomic-volume-subpath
Dec  2 09:28:00.743: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xgvw" in namespace "subpath-3495" to be "Succeeded or Failed"
Dec  2 09:28:00.751: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Pending", Reason="", readiness=false. Elapsed: 7.988478ms
Dec  2 09:28:02.764: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021416992s
Dec  2 09:28:04.776: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 4.032695474s
Dec  2 09:28:06.793: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 6.04987417s
Dec  2 09:28:08.806: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 8.063518521s
Dec  2 09:28:10.821: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 10.077624691s
Dec  2 09:28:12.838: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 12.09470619s
Dec  2 09:28:14.850: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 14.106791772s
Dec  2 09:28:16.859: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 16.11630253s
Dec  2 09:28:18.875: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 18.132294312s
Dec  2 09:28:20.889: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 20.146127163s
Dec  2 09:28:22.903: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=true. Elapsed: 22.160147702s
Dec  2 09:28:24.916: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Running", Reason="", readiness=false. Elapsed: 24.17323994s
Dec  2 09:28:26.926: INFO: Pod "pod-subpath-test-downwardapi-xgvw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.182759046s
STEP: Saw pod success
Dec  2 09:28:26.926: INFO: Pod "pod-subpath-test-downwardapi-xgvw" satisfied condition "Succeeded or Failed"
Dec  2 09:28:26.933: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-subpath-test-downwardapi-xgvw container test-container-subpath-downwardapi-xgvw: <nil>
STEP: delete the pod
Dec  2 09:28:26.979: INFO: Waiting for pod pod-subpath-test-downwardapi-xgvw to disappear
Dec  2 09:28:26.986: INFO: Pod pod-subpath-test-downwardapi-xgvw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xgvw
Dec  2 09:28:26.986: INFO: Deleting pod "pod-subpath-test-downwardapi-xgvw" in namespace "subpath-3495"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:28:26.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3495" for this suite.

• [SLOW TEST:27.589 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":45,"skipped":749,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:28:27.481: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename init-container
W1202 09:28:27.959843      23 warnings.go:70] No static IP address has been configured for the namespace "init-container-6000", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  2 09:28:28.160: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:28:45.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6000" for this suite.

• [SLOW TEST:18.570 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":46,"skipped":752,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:28:46.055: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename server-version
W1202 09:28:46.492411      23 warnings.go:70] No static IP address has been configured for the namespace "server-version-8013", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-8013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Dec  2 09:28:46.678: INFO: Major version: 1
STEP: Confirm minor version
Dec  2 09:28:46.678: INFO: cleanMinorVersion: 22
Dec  2 09:28:46.678: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:28:46.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8013" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":47,"skipped":765,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:28:47.122: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename cronjob
W1202 09:28:47.609207      23 warnings.go:70] No static IP address has been configured for the namespace "cronjob-5724", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-5724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:34:01.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5724" for this suite.

• [SLOW TEST:315.245 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":48,"skipped":773,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:34:02.369: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 09:34:02.818708      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-4935", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 09:34:03.698164      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-4935", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 09:34:04.145329      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-4935-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 09:34:04.534: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 09:34:06.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:34:08.578: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:34:10.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570444, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 09:34:14.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:34:14.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4935" for this suite.
STEP: Destroying namespace "webhook-4935-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.319 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":49,"skipped":783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:34:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:34:16.163340      23 warnings.go:70] No static IP address has been configured for the namespace "projected-8116", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 09:34:16.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac" in namespace "projected-8116" to be "Succeeded or Failed"
Dec  2 09:34:16.377: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106159ms
Dec  2 09:34:18.389: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020845958s
Dec  2 09:34:20.409: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040889401s
Dec  2 09:34:22.422: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05376482s
Dec  2 09:34:24.434: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065808881s
STEP: Saw pod success
Dec  2 09:34:24.434: INFO: Pod "downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac" satisfied condition "Succeeded or Failed"
Dec  2 09:34:24.441: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac container client-container: <nil>
STEP: delete the pod
Dec  2 09:34:24.511: INFO: Waiting for pod downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac to disappear
Dec  2 09:34:24.517: INFO: Pod downwardapi-volume-b2bcb885-5852-46d2-ae50-3d0daf6bc8ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:34:24.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8116" for this suite.

• [SLOW TEST:9.320 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":808,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:34:25.009: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 09:34:25.464411      23 warnings.go:70] No static IP address has been configured for the namespace "pods-4349", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4349
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Dec  2 09:34:25.693: INFO: created test-pod-1
Dec  2 09:34:25.713: INFO: created test-pod-2
Dec  2 09:34:25.732: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Dec  2 09:34:25.732: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4349' to be running and ready
Dec  2 09:34:25.762: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:25.762: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:25.762: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:25.762: INFO: 0 / 3 pods in namespace 'pods-4349' are running and ready (0 seconds elapsed)
Dec  2 09:34:25.762: INFO: expected 0 pod replicas in namespace 'pods-4349', 0 are Running and Ready.
Dec  2 09:34:25.762: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:34:25.762: INFO: test-pod-1  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:25.762: INFO: test-pod-2  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:25.762: INFO: test-pod-3  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:25.762: INFO: 
Dec  2 09:34:27.789: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:27.789: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:27.789: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:27.789: INFO: 0 / 3 pods in namespace 'pods-4349' are running and ready (2 seconds elapsed)
Dec  2 09:34:27.789: INFO: expected 0 pod replicas in namespace 'pods-4349', 0 are Running and Ready.
Dec  2 09:34:27.789: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:34:27.790: INFO: test-pod-1  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:27.790: INFO: test-pod-2  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:27.790: INFO: test-pod-3  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:27.790: INFO: 
Dec  2 09:34:29.784: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:29.784: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:29.784: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  2 09:34:29.784: INFO: 0 / 3 pods in namespace 'pods-4349' are running and ready (4 seconds elapsed)
Dec  2 09:34:29.784: INFO: expected 0 pod replicas in namespace 'pods-4349', 0 are Running and Ready.
Dec  2 09:34:29.784: INFO: POD         NODE                                  PHASE    GRACE  CONDITIONS
Dec  2 09:34:29.784: INFO: test-pod-1  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:29.784: INFO: test-pod-2  d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:29.784: INFO: test-pod-3  5e47d0ea-e90b-466b-b6de-2748d512ebf3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 09:34:25 +0000 UTC  }]
Dec  2 09:34:29.784: INFO: 
Dec  2 09:34:31.789: INFO: 3 / 3 pods in namespace 'pods-4349' are running and ready (6 seconds elapsed)
Dec  2 09:34:31.789: INFO: expected 0 pod replicas in namespace 'pods-4349', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Dec  2 09:34:31.841: INFO: Pod quantity 3 is different from expected quantity 0
Dec  2 09:34:32.853: INFO: Pod quantity 3 is different from expected quantity 0
Dec  2 09:34:33.853: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:34:34.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4349" for this suite.

• [SLOW TEST:10.419 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":51,"skipped":817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:34:35.430: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 09:34:35.872295      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-5222", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5222
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec  2 09:34:36.566: INFO: Found 0 stateful pods, waiting for 3
Dec  2 09:34:46.587: INFO: Found 2 stateful pods, waiting for 3
Dec  2 09:34:56.590: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:34:56.590: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:34:56.590: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 09:35:06.593: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:35:06.594: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:35:06.594: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 09:35:06.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-5222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:35:06.868: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:35:06.868: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:35:06.868: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec  2 09:35:16.948: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  2 09:35:26.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-5222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:35:27.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 09:35:27.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 09:35:27.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 09:35:37.261: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:35:37.261: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:35:37.261: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:35:47.291: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:35:47.291: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:35:47.291: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:35:57.292: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:35:57.293: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:35:57.293: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:36:07.292: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:36:07.292: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 09:36:17.287: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:36:27.290: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
STEP: Rolling back to a previous revision
Dec  2 09:36:37.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-5222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 09:36:38.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 09:36:38.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 09:36:38.596: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 09:36:48.673: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  2 09:36:58.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-5222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 09:36:58.989: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 09:36:58.989: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 09:36:58.989: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 09:37:09.057: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:37:09.057: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:09.057: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:09.057: INFO: Waiting for Pod statefulset-5222/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:19.086: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:37:19.087: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:19.087: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:29.086: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:37:29.086: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:29.086: INFO: Waiting for Pod statefulset-5222/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:39.094: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
Dec  2 09:37:39.094: INFO: Waiting for Pod statefulset-5222/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec  2 09:37:49.090: INFO: Waiting for StatefulSet statefulset-5222/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 09:37:59.089: INFO: Deleting all statefulset in ns statefulset-5222
Dec  2 09:37:59.097: INFO: Scaling statefulset ss2 to 0
Dec  2 09:38:19.161: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 09:38:19.168: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:38:19.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5222" for this suite.

• [SLOW TEST:224.266 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":52,"skipped":860,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:38:19.696: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename certificates
W1202 09:38:20.260989      23 warnings.go:70] No static IP address has been configured for the namespace "certificates-5127", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-5127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  2 09:38:21.189: INFO: starting watch
STEP: patching
STEP: updating
Dec  2 09:38:21.209: INFO: waiting for watch events with expected annotations
Dec  2 09:38:21.209: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:38:21.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5127" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":53,"skipped":861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:38:21.847: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 09:38:22.438315      23 warnings.go:70] No static IP address has been configured for the namespace "pods-9938", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:38:22.643: INFO: The status of Pod server-envvars-de2c9e36-2bf9-4c38-9d89-f501f2e0ba7a is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:38:24.654: INFO: The status of Pod server-envvars-de2c9e36-2bf9-4c38-9d89-f501f2e0ba7a is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:38:26.658: INFO: The status of Pod server-envvars-de2c9e36-2bf9-4c38-9d89-f501f2e0ba7a is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:38:28.659: INFO: The status of Pod server-envvars-de2c9e36-2bf9-4c38-9d89-f501f2e0ba7a is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:38:30.656: INFO: The status of Pod server-envvars-de2c9e36-2bf9-4c38-9d89-f501f2e0ba7a is Running (Ready = true)
Dec  2 09:38:31.188: INFO: Waiting up to 5m0s for pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5" in namespace "pods-9938" to be "Succeeded or Failed"
Dec  2 09:38:31.196: INFO: Pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11581ms
Dec  2 09:38:33.211: INFO: Pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023586719s
Dec  2 09:38:35.224: INFO: Pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03635321s
Dec  2 09:38:37.237: INFO: Pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048766216s
STEP: Saw pod success
Dec  2 09:38:37.237: INFO: Pod "client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5" satisfied condition "Succeeded or Failed"
Dec  2 09:38:37.243: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5 container env3cont: <nil>
STEP: delete the pod
Dec  2 09:38:37.308: INFO: Waiting for pod client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5 to disappear
Dec  2 09:38:37.316: INFO: Pod client-envvars-a9fe352a-bd52-46bc-98a0-806e9d6f88b5 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:38:37.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9938" for this suite.

• [SLOW TEST:15.906 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:38:37.755: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:38:38.241998      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-5093", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Dec  2 09:38:38.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5093 api-versions'
Dec  2 09:38:38.536: INFO: stderr: ""
Dec  2 09:38:38.536: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nargoproj.io/v1alpha1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.internal.knative.dev/v1alpha1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbitnami.com/v1alpha1\ncaching.internal.knative.dev/v1alpha1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\ninstall.istio.io/v1alpha1\nkeycloak.org/v1alpha1\nkiali.io/v1alpha1\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.internal.knative.dev/v1alpha1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnsx.vmware.com/v1\nops.alexellis.io/v1\npksapi.io/v1beta1\npolicy/v1\npolicy/v1beta1\nprojectcontour.io/v1\nprojectcontour.io/v1alpha1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecurity.istio.io/v1beta1\nserving.knative.dev/v1\nserving.knative.dev/v1alpha1\nserving.knative.dev/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntelemetry.istio.io/v1alpha1\nv1\nvelero.io/v1\nvmware.com/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:38:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5093" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":55,"skipped":961,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:38:38.991: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:38:39.514506      23 warnings.go:70] No static IP address has been configured for the namespace "projected-7432", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 09:38:39.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860" in namespace "projected-7432" to be "Succeeded or Failed"
Dec  2 09:38:39.733: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860": Phase="Pending", Reason="", readiness=false. Elapsed: 7.131069ms
Dec  2 09:38:41.747: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020800252s
Dec  2 09:38:43.758: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031906219s
Dec  2 09:38:45.768: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04254788s
Dec  2 09:38:47.779: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.052947233s
STEP: Saw pod success
Dec  2 09:38:47.779: INFO: Pod "downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860" satisfied condition "Succeeded or Failed"
Dec  2 09:38:47.787: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860 container client-container: <nil>
STEP: delete the pod
Dec  2 09:38:47.834: INFO: Waiting for pod downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860 to disappear
Dec  2 09:38:47.840: INFO: Pod downwardapi-volume-e738f92d-1b34-4054-ba38-80248a28c860 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:38:47.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7432" for this suite.

• [SLOW TEST:9.363 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":971,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:38:48.355: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 09:38:48.849596      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9258", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 09:38:49.590037      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9258", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 09:38:50.062003      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9258-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 09:38:50.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 09:38:52.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:38:54.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:38:56.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:38:58.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:39:00.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:39:02.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:39:04.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:39:06.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 09:39:09.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
W1202 09:39:20.437604      23 warnings.go:70] No static IP address has been configured for the namespace "exempted-namesapce", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:39:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9258" for this suite.
STEP: Destroying namespace "webhook-9258-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:37.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":57,"skipped":974,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:39:25.419: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-lifecycle-hook
W1202 09:39:25.918933      23 warnings.go:70] No static IP address has been configured for the namespace "container-lifecycle-hook-7511", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  2 09:39:26.148: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:28.160: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:30.160: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:32.164: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:34.160: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  2 09:39:34.186: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:36.203: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:38.198: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  2 09:39:38.272: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  2 09:39:38.279: INFO: Pod pod-with-poststart-http-hook still exists
Dec  2 09:39:40.280: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  2 09:39:40.291: INFO: Pod pod-with-poststart-http-hook still exists
Dec  2 09:39:42.280: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  2 09:39:42.289: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:39:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7511" for this suite.

• [SLOW TEST:17.391 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":989,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:39:42.811: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:39:43.317800      23 warnings.go:70] No static IP address has been configured for the namespace "projected-8604", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  2 09:39:43.544: INFO: The status of Pod labelsupdate67fc0f38-8394-4852-a2e7-e95e4d80d603 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:45.558: INFO: The status of Pod labelsupdate67fc0f38-8394-4852-a2e7-e95e4d80d603 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:39:47.556: INFO: The status of Pod labelsupdate67fc0f38-8394-4852-a2e7-e95e4d80d603 is Running (Ready = true)
Dec  2 09:39:48.103: INFO: Successfully updated pod "labelsupdate67fc0f38-8394-4852-a2e7-e95e4d80d603"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:39:50.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8604" for this suite.

• [SLOW TEST:7.758 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:39:50.570: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 09:39:51.051284      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-8946", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:39:51.231: INFO: Creating ReplicaSet my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef
Dec  2 09:39:51.246: INFO: Pod name my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef: Found 0 pods out of 1
Dec  2 09:39:56.266: INFO: Pod name my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef: Found 1 pods out of 1
Dec  2 09:39:56.266: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef" is running
Dec  2 09:39:58.286: INFO: Pod "my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef-6vn58" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:39:51 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:39:51 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:39:51 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-02 09:39:51 +0000 UTC Reason: Message:}])
Dec  2 09:39:58.286: INFO: Trying to dial the pod
Dec  2 09:40:03.342: INFO: Controller my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef: Got expected result from replica 1 [my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef-6vn58]: "my-hostname-basic-cd85305c-8825-4aff-bf94-519ce17d89ef-6vn58", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:03.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8946" for this suite.

• [SLOW TEST:13.384 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":60,"skipped":1022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:03.955: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 09:40:04.438759      23 warnings.go:70] No static IP address has been configured for the namespace "pods-3941", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  2 09:40:04.650: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:24.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3941" for this suite.

• [SLOW TEST:21.430 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1052,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:25.385: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:40:25.880343      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-1778", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:26.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1778" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":62,"skipped":1059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:26.616: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 09:40:27.122529      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-1015", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1015
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Dec  2 09:40:27.344: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  2 09:40:32.365: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:38.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1015" for this suite.

• [SLOW TEST:12.363 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":63,"skipped":1098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:38.986: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename watch
W1202 09:40:39.507155      23 warnings.go:70] No static IP address has been configured for the namespace "watch-3652", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  2 09:40:39.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3652  fbd5e594-20f6-4fcf-93ea-276ed51d4b79 52165819 0 2022-12-02 09:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-02 09:40:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 09:40:39.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3652  fbd5e594-20f6-4fcf-93ea-276ed51d4b79 52165820 0 2022-12-02 09:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-02 09:40:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  2 09:40:39.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3652  fbd5e594-20f6-4fcf-93ea-276ed51d4b79 52165821 0 2022-12-02 09:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-02 09:40:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 09:40:39.749: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3652  fbd5e594-20f6-4fcf-93ea-276ed51d4b79 52165822 0 2022-12-02 09:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-02 09:40:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3652" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":64,"skipped":1130,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:40.287: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 09:40:40.788571      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-8486", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Dec  2 09:40:40.996: INFO: Waiting up to 5m0s for pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2" in namespace "var-expansion-8486" to be "Succeeded or Failed"
Dec  2 09:40:41.003: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6463ms
Dec  2 09:40:43.012: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016704934s
Dec  2 09:40:45.025: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029615438s
Dec  2 09:40:47.035: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038853091s
Dec  2 09:40:49.050: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054288167s
STEP: Saw pod success
Dec  2 09:40:49.050: INFO: Pod "var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2" satisfied condition "Succeeded or Failed"
Dec  2 09:40:49.061: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2 container dapi-container: <nil>
STEP: delete the pod
Dec  2 09:40:49.110: INFO: Waiting for pod var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2 to disappear
Dec  2 09:40:49.118: INFO: Pod var-expansion-e46576dd-187c-47a8-88be-2875f182a7b2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:40:49.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8486" for this suite.

• [SLOW TEST:9.335 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1136,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:40:49.622: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:40:50.151076      23 warnings.go:70] No static IP address has been configured for the namespace "projected-2829", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 09:40:50.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c" in namespace "projected-2829" to be "Succeeded or Failed"
Dec  2 09:40:50.391: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.432224ms
Dec  2 09:40:52.406: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027850617s
Dec  2 09:40:54.421: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042612267s
Dec  2 09:40:56.435: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055966851s
Dec  2 09:40:58.451: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072164371s
Dec  2 09:41:00.465: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085964548s
Dec  2 09:41:02.474: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.095537929s
Dec  2 09:41:04.491: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.112291937s
Dec  2 09:41:06.507: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.128121373s
STEP: Saw pod success
Dec  2 09:41:06.507: INFO: Pod "downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c" satisfied condition "Succeeded or Failed"
Dec  2 09:41:06.514: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c container client-container: <nil>
STEP: delete the pod
Dec  2 09:41:06.567: INFO: Waiting for pod downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c to disappear
Dec  2 09:41:06.574: INFO: Pod downwardapi-volume-d6da4828-c6ea-446e-bab8-bef34560ed9c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:41:06.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2829" for this suite.

• [SLOW TEST:17.491 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1145,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:41:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename discovery
W1202 09:41:07.601973      23 warnings.go:70] No static IP address has been configured for the namespace "discovery-9968", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-9968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:41:08.155: INFO: Checking APIGroup: apiregistration.k8s.io
Dec  2 09:41:08.157: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec  2 09:41:08.157: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec  2 09:41:08.157: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec  2 09:41:08.157: INFO: Checking APIGroup: apps
Dec  2 09:41:08.159: INFO: PreferredVersion.GroupVersion: apps/v1
Dec  2 09:41:08.159: INFO: Versions found [{apps/v1 v1}]
Dec  2 09:41:08.159: INFO: apps/v1 matches apps/v1
Dec  2 09:41:08.159: INFO: Checking APIGroup: events.k8s.io
Dec  2 09:41:08.160: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec  2 09:41:08.160: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.160: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec  2 09:41:08.160: INFO: Checking APIGroup: authentication.k8s.io
Dec  2 09:41:08.161: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec  2 09:41:08.161: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec  2 09:41:08.161: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec  2 09:41:08.161: INFO: Checking APIGroup: authorization.k8s.io
Dec  2 09:41:08.162: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec  2 09:41:08.162: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec  2 09:41:08.162: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec  2 09:41:08.162: INFO: Checking APIGroup: autoscaling
Dec  2 09:41:08.163: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec  2 09:41:08.163: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec  2 09:41:08.163: INFO: autoscaling/v1 matches autoscaling/v1
Dec  2 09:41:08.163: INFO: Checking APIGroup: batch
Dec  2 09:41:08.164: INFO: PreferredVersion.GroupVersion: batch/v1
Dec  2 09:41:08.164: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec  2 09:41:08.164: INFO: batch/v1 matches batch/v1
Dec  2 09:41:08.164: INFO: Checking APIGroup: certificates.k8s.io
Dec  2 09:41:08.166: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec  2 09:41:08.166: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec  2 09:41:08.166: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec  2 09:41:08.166: INFO: Checking APIGroup: networking.k8s.io
Dec  2 09:41:08.167: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec  2 09:41:08.167: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec  2 09:41:08.167: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec  2 09:41:08.167: INFO: Checking APIGroup: policy
Dec  2 09:41:08.168: INFO: PreferredVersion.GroupVersion: policy/v1
Dec  2 09:41:08.168: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Dec  2 09:41:08.168: INFO: policy/v1 matches policy/v1
Dec  2 09:41:08.168: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec  2 09:41:08.169: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec  2 09:41:08.169: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec  2 09:41:08.169: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec  2 09:41:08.169: INFO: Checking APIGroup: storage.k8s.io
Dec  2 09:41:08.170: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec  2 09:41:08.170: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.170: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec  2 09:41:08.170: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec  2 09:41:08.171: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec  2 09:41:08.171: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec  2 09:41:08.171: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec  2 09:41:08.171: INFO: Checking APIGroup: apiextensions.k8s.io
Dec  2 09:41:08.172: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec  2 09:41:08.172: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec  2 09:41:08.172: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec  2 09:41:08.172: INFO: Checking APIGroup: scheduling.k8s.io
Dec  2 09:41:08.173: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec  2 09:41:08.173: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec  2 09:41:08.174: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec  2 09:41:08.174: INFO: Checking APIGroup: coordination.k8s.io
Dec  2 09:41:08.175: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec  2 09:41:08.175: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec  2 09:41:08.175: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec  2 09:41:08.175: INFO: Checking APIGroup: node.k8s.io
Dec  2 09:41:08.176: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec  2 09:41:08.176: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.176: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec  2 09:41:08.176: INFO: Checking APIGroup: discovery.k8s.io
Dec  2 09:41:08.177: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec  2 09:41:08.177: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.177: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec  2 09:41:08.177: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec  2 09:41:08.178: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Dec  2 09:41:08.179: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.179: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Dec  2 09:41:08.179: INFO: Checking APIGroup: acme.cert-manager.io
Dec  2 09:41:08.182: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Dec  2 09:41:08.182: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Dec  2 09:41:08.182: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Dec  2 09:41:08.182: INFO: Checking APIGroup: cert-manager.io
Dec  2 09:41:08.184: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Dec  2 09:41:08.184: INFO: Versions found [{cert-manager.io/v1 v1}]
Dec  2 09:41:08.184: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Dec  2 09:41:08.184: INFO: Checking APIGroup: monitoring.coreos.com
Dec  2 09:41:08.185: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Dec  2 09:41:08.185: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Dec  2 09:41:08.185: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Dec  2 09:41:08.185: INFO: Checking APIGroup: nsx.vmware.com
Dec  2 09:41:08.186: INFO: PreferredVersion.GroupVersion: nsx.vmware.com/v1
Dec  2 09:41:08.186: INFO: Versions found [{nsx.vmware.com/v1 v1}]
Dec  2 09:41:08.186: INFO: nsx.vmware.com/v1 matches nsx.vmware.com/v1
Dec  2 09:41:08.186: INFO: Checking APIGroup: ops.alexellis.io
Dec  2 09:41:08.188: INFO: PreferredVersion.GroupVersion: ops.alexellis.io/v1
Dec  2 09:41:08.188: INFO: Versions found [{ops.alexellis.io/v1 v1}]
Dec  2 09:41:08.188: INFO: ops.alexellis.io/v1 matches ops.alexellis.io/v1
Dec  2 09:41:08.188: INFO: Checking APIGroup: projectcontour.io
Dec  2 09:41:08.189: INFO: PreferredVersion.GroupVersion: projectcontour.io/v1
Dec  2 09:41:08.189: INFO: Versions found [{projectcontour.io/v1 v1} {projectcontour.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.189: INFO: projectcontour.io/v1 matches projectcontour.io/v1
Dec  2 09:41:08.189: INFO: Checking APIGroup: serving.knative.dev
Dec  2 09:41:08.190: INFO: PreferredVersion.GroupVersion: serving.knative.dev/v1
Dec  2 09:41:08.190: INFO: Versions found [{serving.knative.dev/v1 v1} {serving.knative.dev/v1beta1 v1beta1} {serving.knative.dev/v1alpha1 v1alpha1}]
Dec  2 09:41:08.191: INFO: serving.knative.dev/v1 matches serving.knative.dev/v1
Dec  2 09:41:08.191: INFO: Checking APIGroup: velero.io
Dec  2 09:41:08.192: INFO: PreferredVersion.GroupVersion: velero.io/v1
Dec  2 09:41:08.192: INFO: Versions found [{velero.io/v1 v1}]
Dec  2 09:41:08.192: INFO: velero.io/v1 matches velero.io/v1
Dec  2 09:41:08.192: INFO: Checking APIGroup: argoproj.io
Dec  2 09:41:08.193: INFO: PreferredVersion.GroupVersion: argoproj.io/v1alpha1
Dec  2 09:41:08.194: INFO: Versions found [{argoproj.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.194: INFO: argoproj.io/v1alpha1 matches argoproj.io/v1alpha1
Dec  2 09:41:08.194: INFO: Checking APIGroup: autoscaling.internal.knative.dev
Dec  2 09:41:08.196: INFO: PreferredVersion.GroupVersion: autoscaling.internal.knative.dev/v1alpha1
Dec  2 09:41:08.196: INFO: Versions found [{autoscaling.internal.knative.dev/v1alpha1 v1alpha1}]
Dec  2 09:41:08.196: INFO: autoscaling.internal.knative.dev/v1alpha1 matches autoscaling.internal.knative.dev/v1alpha1
Dec  2 09:41:08.197: INFO: Checking APIGroup: bitnami.com
Dec  2 09:41:08.199: INFO: PreferredVersion.GroupVersion: bitnami.com/v1alpha1
Dec  2 09:41:08.199: INFO: Versions found [{bitnami.com/v1alpha1 v1alpha1}]
Dec  2 09:41:08.199: INFO: bitnami.com/v1alpha1 matches bitnami.com/v1alpha1
Dec  2 09:41:08.199: INFO: Checking APIGroup: caching.internal.knative.dev
Dec  2 09:41:08.202: INFO: PreferredVersion.GroupVersion: caching.internal.knative.dev/v1alpha1
Dec  2 09:41:08.202: INFO: Versions found [{caching.internal.knative.dev/v1alpha1 v1alpha1}]
Dec  2 09:41:08.202: INFO: caching.internal.knative.dev/v1alpha1 matches caching.internal.knative.dev/v1alpha1
Dec  2 09:41:08.202: INFO: Checking APIGroup: install.istio.io
Dec  2 09:41:08.204: INFO: PreferredVersion.GroupVersion: install.istio.io/v1alpha1
Dec  2 09:41:08.204: INFO: Versions found [{install.istio.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.204: INFO: install.istio.io/v1alpha1 matches install.istio.io/v1alpha1
Dec  2 09:41:08.204: INFO: Checking APIGroup: keycloak.org
Dec  2 09:41:08.207: INFO: PreferredVersion.GroupVersion: keycloak.org/v1alpha1
Dec  2 09:41:08.207: INFO: Versions found [{keycloak.org/v1alpha1 v1alpha1}]
Dec  2 09:41:08.207: INFO: keycloak.org/v1alpha1 matches keycloak.org/v1alpha1
Dec  2 09:41:08.207: INFO: Checking APIGroup: kiali.io
Dec  2 09:41:08.208: INFO: PreferredVersion.GroupVersion: kiali.io/v1alpha1
Dec  2 09:41:08.208: INFO: Versions found [{kiali.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.208: INFO: kiali.io/v1alpha1 matches kiali.io/v1alpha1
Dec  2 09:41:08.209: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
Dec  2 09:41:08.211: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
Dec  2 09:41:08.211: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.212: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
Dec  2 09:41:08.212: INFO: Checking APIGroup: logging.banzaicloud.io
Dec  2 09:41:08.214: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
Dec  2 09:41:08.214: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.214: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
Dec  2 09:41:08.215: INFO: Checking APIGroup: networking.internal.knative.dev
Dec  2 09:41:08.218: INFO: PreferredVersion.GroupVersion: networking.internal.knative.dev/v1alpha1
Dec  2 09:41:08.219: INFO: Versions found [{networking.internal.knative.dev/v1alpha1 v1alpha1}]
Dec  2 09:41:08.219: INFO: networking.internal.knative.dev/v1alpha1 matches networking.internal.knative.dev/v1alpha1
Dec  2 09:41:08.219: INFO: Checking APIGroup: telemetry.istio.io
Dec  2 09:41:08.221: INFO: PreferredVersion.GroupVersion: telemetry.istio.io/v1alpha1
Dec  2 09:41:08.221: INFO: Versions found [{telemetry.istio.io/v1alpha1 v1alpha1}]
Dec  2 09:41:08.221: INFO: telemetry.istio.io/v1alpha1 matches telemetry.istio.io/v1alpha1
Dec  2 09:41:08.222: INFO: Checking APIGroup: vmware.com
Dec  2 09:41:08.224: INFO: PreferredVersion.GroupVersion: vmware.com/v1alpha1
Dec  2 09:41:08.224: INFO: Versions found [{vmware.com/v1alpha1 v1alpha1}]
Dec  2 09:41:08.224: INFO: vmware.com/v1alpha1 matches vmware.com/v1alpha1
Dec  2 09:41:08.224: INFO: Checking APIGroup: networking.istio.io
Dec  2 09:41:08.226: INFO: PreferredVersion.GroupVersion: networking.istio.io/v1beta1
Dec  2 09:41:08.226: INFO: Versions found [{networking.istio.io/v1beta1 v1beta1} {networking.istio.io/v1alpha3 v1alpha3}]
Dec  2 09:41:08.226: INFO: networking.istio.io/v1beta1 matches networking.istio.io/v1beta1
Dec  2 09:41:08.226: INFO: Checking APIGroup: pksapi.io
Dec  2 09:41:08.227: INFO: PreferredVersion.GroupVersion: pksapi.io/v1beta1
Dec  2 09:41:08.227: INFO: Versions found [{pksapi.io/v1beta1 v1beta1}]
Dec  2 09:41:08.227: INFO: pksapi.io/v1beta1 matches pksapi.io/v1beta1
Dec  2 09:41:08.227: INFO: Checking APIGroup: security.istio.io
Dec  2 09:41:08.230: INFO: PreferredVersion.GroupVersion: security.istio.io/v1beta1
Dec  2 09:41:08.230: INFO: Versions found [{security.istio.io/v1beta1 v1beta1}]
Dec  2 09:41:08.230: INFO: security.istio.io/v1beta1 matches security.istio.io/v1beta1
Dec  2 09:41:08.230: INFO: Checking APIGroup: metrics.k8s.io
Dec  2 09:41:08.231: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec  2 09:41:08.231: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec  2 09:41:08.231: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:41:08.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9968" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":67,"skipped":1154,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:41:08.674: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 09:41:09.107049      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-2256", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  2 09:41:09.321: INFO: The status of Pod labelsupdate5aa3cc94-cbb4-4ed9-9e15-0a9944ea9b2f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:11.333: INFO: The status of Pod labelsupdate5aa3cc94-cbb4-4ed9-9e15-0a9944ea9b2f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:13.331: INFO: The status of Pod labelsupdate5aa3cc94-cbb4-4ed9-9e15-0a9944ea9b2f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:15.342: INFO: The status of Pod labelsupdate5aa3cc94-cbb4-4ed9-9e15-0a9944ea9b2f is Running (Ready = true)
Dec  2 09:41:15.916: INFO: Successfully updated pod "labelsupdate5aa3cc94-cbb4-4ed9-9e15-0a9944ea9b2f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:41:17.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2256" for this suite.

• [SLOW TEST:9.827 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1163,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:41:18.503: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:41:18.952907      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-8261", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8261
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-f3bfb55c-bdd6-4c76-9cfe-d69f6c275096
STEP: Creating secret with name s-test-opt-upd-e55a7ce5-113d-489c-909d-6981def94d8c
STEP: Creating the pod
Dec  2 09:41:19.205: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:21.216: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:23.219: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:25.217: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:27.218: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:29.222: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:31.221: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:41:33.218: INFO: The status of Pod pod-secrets-c59a1a7b-cbea-4099-a89a-5461ba7018c7 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-f3bfb55c-bdd6-4c76-9cfe-d69f6c275096
STEP: Updating secret s-test-opt-upd-e55a7ce5-113d-489c-909d-6981def94d8c
STEP: Creating secret with name s-test-opt-create-22209a5f-60ef-4bf0-8901-3bd7d49a37d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:41:43.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8261" for this suite.

• [SLOW TEST:25.525 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1169,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:41:44.028: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 09:41:44.468393      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-913", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 09:41:45.260566      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-913", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
W1202 09:41:45.744495      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-913-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 09:41:46.344: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 09:41:48.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570906, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570906, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570906, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805570906, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 09:41:51.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:41:52.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-913" for this suite.
STEP: Destroying namespace "webhook-913-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":70,"skipped":1169,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:41:53.638: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 09:41:54.098076      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-8175", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8175
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:41:54.287: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  2 09:42:10.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-8175 --namespace=crd-publish-openapi-8175 create -f -'
Dec  2 09:42:12.932: INFO: stderr: ""
Dec  2 09:42:12.932: INFO: stdout: "e2e-test-crd-publish-openapi-6178-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  2 09:42:12.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-8175 --namespace=crd-publish-openapi-8175 delete e2e-test-crd-publish-openapi-6178-crds test-cr'
Dec  2 09:42:13.042: INFO: stderr: ""
Dec  2 09:42:13.042: INFO: stdout: "e2e-test-crd-publish-openapi-6178-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec  2 09:42:13.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-8175 --namespace=crd-publish-openapi-8175 apply -f -'
Dec  2 09:42:14.692: INFO: stderr: ""
Dec  2 09:42:14.692: INFO: stdout: "e2e-test-crd-publish-openapi-6178-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  2 09:42:14.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-8175 --namespace=crd-publish-openapi-8175 delete e2e-test-crd-publish-openapi-6178-crds test-cr'
Dec  2 09:42:14.813: INFO: stderr: ""
Dec  2 09:42:14.813: INFO: stdout: "e2e-test-crd-publish-openapi-6178-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  2 09:42:14.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-8175 explain e2e-test-crd-publish-openapi-6178-crds'
Dec  2 09:42:15.231: INFO: stderr: ""
Dec  2 09:42:15.231: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6178-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:42:25.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8175" for this suite.

• [SLOW TEST:32.748 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":71,"skipped":1176,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:42:26.386: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:42:27.286630      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-7604", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-569eb990-b1a3-42df-8bfa-3037f8032cc9
STEP: Creating a pod to test consume secrets
Dec  2 09:42:27.522: INFO: Waiting up to 5m0s for pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd" in namespace "secrets-7604" to be "Succeeded or Failed"
Dec  2 09:42:27.534: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.042962ms
Dec  2 09:42:29.541: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018368989s
Dec  2 09:42:31.553: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03062192s
Dec  2 09:42:33.566: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043573318s
Dec  2 09:42:35.582: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059941434s
STEP: Saw pod success
Dec  2 09:42:35.582: INFO: Pod "pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd" satisfied condition "Succeeded or Failed"
Dec  2 09:42:35.589: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd container secret-env-test: <nil>
STEP: delete the pod
Dec  2 09:42:35.653: INFO: Waiting for pod pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd to disappear
Dec  2 09:42:35.663: INFO: Pod pod-secrets-3b2b6263-13c2-4ed2-ae76-45144a07eebd no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:42:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7604" for this suite.

• [SLOW TEST:9.883 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1191,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:42:36.269: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 09:42:36.729022      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-5059", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-4620ba75-6de7-44bf-8c7c-bda3fe1e5d1e in namespace container-probe-5059
Dec  2 09:42:40.959: INFO: Started pod liveness-4620ba75-6de7-44bf-8c7c-bda3fe1e5d1e in namespace container-probe-5059
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 09:42:40.968: INFO: Initial restart count of pod liveness-4620ba75-6de7-44bf-8c7c-bda3fe1e5d1e is 0
Dec  2 09:42:59.086: INFO: Restart count of pod container-probe-5059/liveness-4620ba75-6de7-44bf-8c7c-bda3fe1e5d1e is now 1 (18.117794285s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:42:59.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5059" for this suite.

• [SLOW TEST:23.397 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1192,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:42:59.666: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 09:43:00.147679      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-1349", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  2 09:43:00.399: INFO: Waiting up to 5m0s for pod "pod-0bce7094-b879-4292-ba37-315ed70d5164" in namespace "emptydir-1349" to be "Succeeded or Failed"
Dec  2 09:43:00.409: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164": Phase="Pending", Reason="", readiness=false. Elapsed: 9.632971ms
Dec  2 09:43:02.421: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021863711s
Dec  2 09:43:04.428: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028457367s
Dec  2 09:43:06.436: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035988429s
Dec  2 09:43:08.444: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044235705s
STEP: Saw pod success
Dec  2 09:43:08.444: INFO: Pod "pod-0bce7094-b879-4292-ba37-315ed70d5164" satisfied condition "Succeeded or Failed"
Dec  2 09:43:08.452: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-0bce7094-b879-4292-ba37-315ed70d5164 container test-container: <nil>
STEP: delete the pod
Dec  2 09:43:08.499: INFO: Waiting for pod pod-0bce7094-b879-4292-ba37-315ed70d5164 to disappear
Dec  2 09:43:08.509: INFO: Pod pod-0bce7094-b879-4292-ba37-315ed70d5164 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:43:08.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1349" for this suite.

• [SLOW TEST:9.315 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1198,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:43:08.981: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 09:43:09.481846      23 warnings.go:70] No static IP address has been configured for the namespace "dns-2338", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 09:43:16.214: INFO: DNS probes using dns-test-8323ab98-b3c6-4a0a-b00a-01c42194f6c4 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 09:43:20.899: INFO: File wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:20.905: INFO: File jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:20.905: INFO: Lookups using dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 failed for: [wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local]

Dec  2 09:43:25.913: INFO: File wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:25.924: INFO: File jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:25.924: INFO: Lookups using dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 failed for: [wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local]

Dec  2 09:43:30.916: INFO: File wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:30.926: INFO: File jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:30.926: INFO: Lookups using dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 failed for: [wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local]

Dec  2 09:43:35.919: INFO: File wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:35.927: INFO: File jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:35.927: INFO: Lookups using dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 failed for: [wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local]

Dec  2 09:43:40.917: INFO: File wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:40.929: INFO: File jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local from pod  dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  2 09:43:40.929: INFO: Lookups using dns-2338/dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 failed for: [wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local]

Dec  2 09:43:45.931: INFO: DNS probes using dns-test-b836c838-1f1d-4811-95b0-6450fd3d0ef0 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2338.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2338.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 09:43:52.551: INFO: DNS probes using dns-test-6eb68253-0a09-481b-86d8-14bb2b5f49e9 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:43:53.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2338" for this suite.

• [SLOW TEST:44.603 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":75,"skipped":1201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:43:53.585: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename job
W1202 09:43:54.026796      23 warnings.go:70] No static IP address has been configured for the namespace "job-953", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec  2 09:44:02.769: INFO: Successfully updated pod "adopt-release-4q5pl"
STEP: Checking that the Job readopts the Pod
Dec  2 09:44:02.769: INFO: Waiting up to 15m0s for pod "adopt-release-4q5pl" in namespace "job-953" to be "adopted"
Dec  2 09:44:02.778: INFO: Pod "adopt-release-4q5pl": Phase="Running", Reason="", readiness=true. Elapsed: 8.729584ms
Dec  2 09:44:04.789: INFO: Pod "adopt-release-4q5pl": Phase="Running", Reason="", readiness=true. Elapsed: 2.020597656s
Dec  2 09:44:04.789: INFO: Pod "adopt-release-4q5pl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec  2 09:44:05.313: INFO: Successfully updated pod "adopt-release-4q5pl"
STEP: Checking that the Job releases the Pod
Dec  2 09:44:05.313: INFO: Waiting up to 15m0s for pod "adopt-release-4q5pl" in namespace "job-953" to be "released"
Dec  2 09:44:05.326: INFO: Pod "adopt-release-4q5pl": Phase="Running", Reason="", readiness=true. Elapsed: 13.358172ms
Dec  2 09:44:07.340: INFO: Pod "adopt-release-4q5pl": Phase="Running", Reason="", readiness=true. Elapsed: 2.027483539s
Dec  2 09:44:07.340: INFO: Pod "adopt-release-4q5pl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:44:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-953" for this suite.

• [SLOW TEST:14.278 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":76,"skipped":1235,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:44:07.863: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 09:44:08.285844      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-4729", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:44:08.465: INFO: Creating deployment "test-recreate-deployment"
Dec  2 09:44:08.477: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  2 09:44:08.489: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  2 09:44:10.505: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  2 09:44:10.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:12.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:14.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:16.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:18.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:20.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571048, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-578d7495c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:44:22.522: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  2 09:44:22.544: INFO: Updating deployment test-recreate-deployment
Dec  2 09:44:22.544: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 09:44:22.719: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4729  25ac902e-cbc2-44c2-8172-4b87b482e372 52167788 2 2022-12-02 09:44:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00e3cfe98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-02 09:44:22 +0000 UTC,LastTransitionTime:2022-12-02 09:44:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-12-02 09:44:22 +0000 UTC,LastTransitionTime:2022-12-02 09:44:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec  2 09:44:22.730: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-4729  513b2f80-c89f-4e80-8566-60c159f02d76 52167787 1 2022-12-02 09:44:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 25ac902e-cbc2-44c2-8172-4b87b482e372 0xc006a99eb0 0xc006a99eb1}] []  [{kube-controller-manager Update apps/v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25ac902e-cbc2-44c2-8172-4b87b482e372\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a99f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 09:44:22.730: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  2 09:44:22.730: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-578d7495c9  deployment-4729  c659b47b-d848-45e1-b93f-7100c05fe450 52167777 2 2022-12-02 09:44:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:578d7495c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 25ac902e-cbc2-44c2-8172-4b87b482e372 0xc006a99d97 0xc006a99d98}] []  [{kube-controller-manager Update apps/v1 2022-12-02 09:44:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25ac902e-cbc2-44c2-8172-4b87b482e372\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 578d7495c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:578d7495c9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a99e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 09:44:22.744: INFO: Pod "test-recreate-deployment-85d47dcb4-m7rxr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-m7rxr test-recreate-deployment-85d47dcb4- deployment-4729  186a63eb-0688-4744-acd8-f6eb42e5f3da 52167784 0 2022-12-02 09:44:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 513b2f80-c89f-4e80-8566-60c159f02d76 0xc0035c83c0 0xc0035c83c1}] []  [{kube-controller-manager Update v1 2022-12-02 09:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"513b2f80-c89f-4e80-8566-60c159f02d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddgbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddgbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:44:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:44:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4729" for this suite.

• [SLOW TEST:15.385 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":77,"skipped":1245,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:44:23.250: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename custom-resource-definition
W1202 09:44:23.718436      23 warnings.go:70] No static IP address has been configured for the namespace "custom-resource-definition-9510", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:44:23.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9510" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":78,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:44:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename subpath
W1202 09:44:24.853708      23 warnings.go:70] No static IP address has been configured for the namespace "subpath-7499", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-xml7
STEP: Creating a pod to test atomic-volume-subpath
Dec  2 09:44:25.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xml7" in namespace "subpath-7499" to be "Succeeded or Failed"
Dec  2 09:44:25.104: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190909ms
Dec  2 09:44:27.116: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019046146s
Dec  2 09:44:29.124: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026345327s
Dec  2 09:44:31.143: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045861438s
Dec  2 09:44:33.152: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 8.054650582s
Dec  2 09:44:35.164: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 10.066362843s
Dec  2 09:44:37.185: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 12.087524744s
Dec  2 09:44:39.196: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 14.098551564s
Dec  2 09:44:41.207: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 16.109533894s
Dec  2 09:44:43.219: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 18.122031768s
Dec  2 09:44:45.233: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 20.135283504s
Dec  2 09:44:47.246: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 22.148352422s
Dec  2 09:44:49.256: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 24.158113203s
Dec  2 09:44:51.267: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=true. Elapsed: 26.169570063s
Dec  2 09:44:53.278: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Running", Reason="", readiness=false. Elapsed: 28.180727242s
Dec  2 09:44:55.289: INFO: Pod "pod-subpath-test-projected-xml7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.192031338s
STEP: Saw pod success
Dec  2 09:44:55.290: INFO: Pod "pod-subpath-test-projected-xml7" satisfied condition "Succeeded or Failed"
Dec  2 09:44:55.298: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-subpath-test-projected-xml7 container test-container-subpath-projected-xml7: <nil>
STEP: delete the pod
Dec  2 09:44:55.363: INFO: Waiting for pod pod-subpath-test-projected-xml7 to disappear
Dec  2 09:44:55.372: INFO: Pod pod-subpath-test-projected-xml7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-xml7
Dec  2 09:44:55.372: INFO: Deleting pod "pod-subpath-test-projected-xml7" in namespace "subpath-7499"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:44:55.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7499" for this suite.

• [SLOW TEST:31.449 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1271,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:44:55.861: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename subpath
W1202 09:44:56.332446      23 warnings.go:70] No static IP address has been configured for the namespace "subpath-925", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-w9fr
STEP: Creating a pod to test atomic-volume-subpath
Dec  2 09:44:56.576: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w9fr" in namespace "subpath-925" to be "Succeeded or Failed"
Dec  2 09:44:56.585: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.271422ms
Dec  2 09:44:58.598: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022339921s
Dec  2 09:45:00.612: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036787479s
Dec  2 09:45:02.626: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 6.050562299s
Dec  2 09:45:04.640: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 8.064407066s
Dec  2 09:45:06.654: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 10.078672052s
Dec  2 09:45:08.665: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 12.089584298s
Dec  2 09:45:10.680: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 14.104225761s
Dec  2 09:45:12.689: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 16.113679992s
Dec  2 09:45:14.700: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 18.124659677s
Dec  2 09:45:16.716: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 20.140561359s
Dec  2 09:45:18.730: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 22.154338839s
Dec  2 09:45:20.743: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 24.16762162s
Dec  2 09:45:22.761: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=true. Elapsed: 26.185066586s
Dec  2 09:45:24.768: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Running", Reason="", readiness=false. Elapsed: 28.192376699s
Dec  2 09:45:26.782: INFO: Pod "pod-subpath-test-configmap-w9fr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.20591308s
STEP: Saw pod success
Dec  2 09:45:26.782: INFO: Pod "pod-subpath-test-configmap-w9fr" satisfied condition "Succeeded or Failed"
Dec  2 09:45:26.795: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-subpath-test-configmap-w9fr container test-container-subpath-configmap-w9fr: <nil>
STEP: delete the pod
Dec  2 09:45:26.863: INFO: Waiting for pod pod-subpath-test-configmap-w9fr to disappear
Dec  2 09:45:26.871: INFO: Pod pod-subpath-test-configmap-w9fr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w9fr
Dec  2 09:45:26.871: INFO: Deleting pod "pod-subpath-test-configmap-w9fr" in namespace "subpath-925"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:45:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-925" for this suite.

• [SLOW TEST:31.795 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":80,"skipped":1286,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:45:27.655: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 09:45:28.250142      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-6242", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  2 09:45:28.475: INFO: Waiting up to 5m0s for pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244" in namespace "emptydir-6242" to be "Succeeded or Failed"
Dec  2 09:45:28.482: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139893ms
Dec  2 09:45:30.494: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019049503s
Dec  2 09:45:32.505: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030394605s
Dec  2 09:45:34.515: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039611436s
Dec  2 09:45:36.524: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049096676s
Dec  2 09:45:38.536: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.060944795s
STEP: Saw pod success
Dec  2 09:45:38.536: INFO: Pod "pod-3790b2f1-53ec-420e-a918-557bf91bf244" satisfied condition "Succeeded or Failed"
Dec  2 09:45:38.542: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-3790b2f1-53ec-420e-a918-557bf91bf244 container test-container: <nil>
STEP: delete the pod
Dec  2 09:45:38.594: INFO: Waiting for pod pod-3790b2f1-53ec-420e-a918-557bf91bf244 to disappear
Dec  2 09:45:38.600: INFO: Pod pod-3790b2f1-53ec-420e-a918-557bf91bf244 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:45:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6242" for this suite.

• [SLOW TEST:11.482 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:45:39.139: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svc-latency
W1202 09:45:39.647061      23 warnings.go:70] No static IP address has been configured for the namespace "svc-latency-8121", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:45:39.829: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8121
I1202 09:45:39.861664      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8121, replica count: 1
I1202 09:45:40.912429      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:45:41.912597      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:45:42.913610      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 09:45:43.914273      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 09:45:44.460: INFO: Created: latency-svc-wk7w2
Dec  2 09:45:44.468: INFO: Got endpoints: latency-svc-wk7w2 [453.122555ms]
Dec  2 09:45:45.031: INFO: Created: latency-svc-2s672
Dec  2 09:45:45.031: INFO: Created: latency-svc-fl4xq
Dec  2 09:45:45.038: INFO: Got endpoints: latency-svc-2s672 [570.10378ms]
Dec  2 09:45:45.051: INFO: Got endpoints: latency-svc-fl4xq [582.422632ms]
Dec  2 09:45:45.060: INFO: Created: latency-svc-tqfxn
Dec  2 09:45:45.075: INFO: Got endpoints: latency-svc-tqfxn [606.150217ms]
Dec  2 09:45:45.093: INFO: Created: latency-svc-25bkf
Dec  2 09:45:45.110: INFO: Got endpoints: latency-svc-25bkf [640.505592ms]
Dec  2 09:45:45.135: INFO: Created: latency-svc-knwln
Dec  2 09:45:45.152: INFO: Got endpoints: latency-svc-knwln [682.97997ms]
Dec  2 09:45:45.195: INFO: Created: latency-svc-f7tj5
Dec  2 09:45:45.209: INFO: Got endpoints: latency-svc-f7tj5 [740.036805ms]
Dec  2 09:45:45.233: INFO: Created: latency-svc-hnr7h
Dec  2 09:45:45.250: INFO: Got endpoints: latency-svc-hnr7h [780.337963ms]
Dec  2 09:45:45.304: INFO: Created: latency-svc-g5cr4
Dec  2 09:45:45.321: INFO: Got endpoints: latency-svc-g5cr4 [851.520049ms]
Dec  2 09:45:45.345: INFO: Created: latency-svc-mpht2
Dec  2 09:45:45.362: INFO: Got endpoints: latency-svc-mpht2 [892.765604ms]
Dec  2 09:45:45.363: INFO: Created: latency-svc-tr5d5
Dec  2 09:45:45.370: INFO: Created: latency-svc-j8rl9
Dec  2 09:45:45.376: INFO: Got endpoints: latency-svc-tr5d5 [907.356471ms]
Dec  2 09:45:45.381: INFO: Created: latency-svc-g7sdd
Dec  2 09:45:45.410: INFO: Got endpoints: latency-svc-g7sdd [941.585518ms]
Dec  2 09:45:45.411: INFO: Got endpoints: latency-svc-j8rl9 [942.138179ms]
Dec  2 09:45:45.432: INFO: Created: latency-svc-664k6
Dec  2 09:45:45.449: INFO: Got endpoints: latency-svc-664k6 [979.760746ms]
Dec  2 09:45:45.449: INFO: Created: latency-svc-pq28z
Dec  2 09:45:45.464: INFO: Got endpoints: latency-svc-pq28z [995.187818ms]
Dec  2 09:45:45.613: INFO: Created: latency-svc-jw6zx
Dec  2 09:45:45.632: INFO: Got endpoints: latency-svc-jw6zx [1.162501681s]
Dec  2 09:45:45.636: INFO: Created: latency-svc-4hj65
Dec  2 09:45:45.653: INFO: Got endpoints: latency-svc-4hj65 [601.893895ms]
Dec  2 09:45:45.684: INFO: Created: latency-svc-n5gv2
Dec  2 09:45:45.693: INFO: Got endpoints: latency-svc-n5gv2 [654.956853ms]
Dec  2 09:45:45.714: INFO: Created: latency-svc-cdzg8
Dec  2 09:45:45.736: INFO: Got endpoints: latency-svc-cdzg8 [660.00206ms]
Dec  2 09:45:45.745: INFO: Created: latency-svc-26k2d
Dec  2 09:45:45.745: INFO: Created: latency-svc-lv5t8
Dec  2 09:45:45.752: INFO: Got endpoints: latency-svc-26k2d [642.182127ms]
Dec  2 09:45:45.769: INFO: Got endpoints: latency-svc-lv5t8 [617.506294ms]
Dec  2 09:45:45.802: INFO: Created: latency-svc-4h9rv
Dec  2 09:45:45.817: INFO: Got endpoints: latency-svc-4h9rv [608.315616ms]
Dec  2 09:45:45.855: INFO: Created: latency-svc-zwvpl
Dec  2 09:45:45.871: INFO: Got endpoints: latency-svc-zwvpl [621.034949ms]
Dec  2 09:45:45.937: INFO: Created: latency-svc-2h6sx
Dec  2 09:45:45.948: INFO: Got endpoints: latency-svc-2h6sx [537.591478ms]
Dec  2 09:45:45.971: INFO: Created: latency-svc-mj7kw
Dec  2 09:45:45.997: INFO: Got endpoints: latency-svc-mj7kw [675.631798ms]
Dec  2 09:45:45.998: INFO: Created: latency-svc-4sngk
Dec  2 09:45:46.017: INFO: Got endpoints: latency-svc-4sngk [567.804116ms]
Dec  2 09:45:46.021: INFO: Created: latency-svc-pnpct
Dec  2 09:45:46.027: INFO: Created: latency-svc-fw64w
Dec  2 09:45:46.034: INFO: Created: latency-svc-vz2cv
Dec  2 09:45:46.053: INFO: Got endpoints: latency-svc-pnpct [690.783035ms]
Dec  2 09:45:46.053: INFO: Got endpoints: latency-svc-fw64w [642.007582ms]
Dec  2 09:45:46.059: INFO: Got endpoints: latency-svc-vz2cv [683.375073ms]
Dec  2 09:45:46.131: INFO: Created: latency-svc-jgb8w
Dec  2 09:45:46.151: INFO: Got endpoints: latency-svc-jgb8w [686.497943ms]
Dec  2 09:45:46.182: INFO: Created: latency-svc-gnjrx
Dec  2 09:45:46.204: INFO: Got endpoints: latency-svc-gnjrx [572.516546ms]
Dec  2 09:45:46.209: INFO: Created: latency-svc-c4vcr
Dec  2 09:45:46.229: INFO: Got endpoints: latency-svc-c4vcr [576.387288ms]
Dec  2 09:45:46.292: INFO: Created: latency-svc-thwc7
Dec  2 09:45:46.308: INFO: Got endpoints: latency-svc-thwc7 [614.508436ms]
Dec  2 09:45:46.315: INFO: Created: latency-svc-pthc9
Dec  2 09:45:46.334: INFO: Got endpoints: latency-svc-pthc9 [581.749866ms]
Dec  2 09:45:46.372: INFO: Created: latency-svc-wjdtt
Dec  2 09:45:46.379: INFO: Created: latency-svc-dsm4s
Dec  2 09:45:46.380: INFO: Got endpoints: latency-svc-wjdtt [644.716289ms]
Dec  2 09:45:46.404: INFO: Got endpoints: latency-svc-dsm4s [586.96181ms]
Dec  2 09:45:46.414: INFO: Created: latency-svc-n4flq
Dec  2 09:45:46.444: INFO: Got endpoints: latency-svc-n4flq [674.927608ms]
Dec  2 09:45:46.446: INFO: Created: latency-svc-zwmq4
Dec  2 09:45:46.457: INFO: Got endpoints: latency-svc-zwmq4 [585.642246ms]
Dec  2 09:45:46.541: INFO: Created: latency-svc-l5qhb
Dec  2 09:45:46.553: INFO: Got endpoints: latency-svc-l5qhb [536.374093ms]
Dec  2 09:45:46.559: INFO: Created: latency-svc-ph7pf
Dec  2 09:45:46.577: INFO: Got endpoints: latency-svc-ph7pf [629.470157ms]
Dec  2 09:45:46.586: INFO: Created: latency-svc-txxr8
Dec  2 09:45:46.600: INFO: Got endpoints: latency-svc-txxr8 [547.165261ms]
Dec  2 09:45:46.610: INFO: Created: latency-svc-b492x
Dec  2 09:45:46.628: INFO: Got endpoints: latency-svc-b492x [575.386682ms]
Dec  2 09:45:46.639: INFO: Created: latency-svc-w955s
Dec  2 09:45:46.654: INFO: Created: latency-svc-9q9wh
Dec  2 09:45:46.669: INFO: Got endpoints: latency-svc-9q9wh [609.773945ms]
Dec  2 09:45:46.669: INFO: Got endpoints: latency-svc-w955s [672.671936ms]
Dec  2 09:45:46.692: INFO: Created: latency-svc-ln7tp
Dec  2 09:45:46.709: INFO: Created: latency-svc-c8l8w
Dec  2 09:45:46.719: INFO: Got endpoints: latency-svc-ln7tp [567.687228ms]
Dec  2 09:45:46.744: INFO: Got endpoints: latency-svc-c8l8w [539.267937ms]
Dec  2 09:45:46.756: INFO: Created: latency-svc-svxhc
Dec  2 09:45:46.775: INFO: Got endpoints: latency-svc-svxhc [545.925427ms]
Dec  2 09:45:46.823: INFO: Created: latency-svc-9vpv6
Dec  2 09:45:46.838: INFO: Got endpoints: latency-svc-9vpv6 [503.781665ms]
Dec  2 09:45:46.906: INFO: Created: latency-svc-jqh4c
Dec  2 09:45:46.912: INFO: Created: latency-svc-d7qxl
Dec  2 09:45:46.926: INFO: Got endpoints: latency-svc-jqh4c [545.960698ms]
Dec  2 09:45:46.928: INFO: Got endpoints: latency-svc-d7qxl [620.099885ms]
Dec  2 09:45:46.975: INFO: Created: latency-svc-bzl96
Dec  2 09:45:46.994: INFO: Got endpoints: latency-svc-bzl96 [537.214966ms]
Dec  2 09:45:47.000: INFO: Created: latency-svc-jfz5g
Dec  2 09:45:47.001: INFO: Created: latency-svc-l6694
Dec  2 09:45:47.020: INFO: Got endpoints: latency-svc-l6694 [615.120338ms]
Dec  2 09:45:47.020: INFO: Got endpoints: latency-svc-jfz5g [575.205468ms]
Dec  2 09:45:47.089: INFO: Created: latency-svc-q4zz2
Dec  2 09:45:47.106: INFO: Got endpoints: latency-svc-q4zz2 [528.3544ms]
Dec  2 09:45:47.107: INFO: Created: latency-svc-zwk6z
Dec  2 09:45:47.126: INFO: Got endpoints: latency-svc-zwk6z [572.732661ms]
Dec  2 09:45:47.142: INFO: Created: latency-svc-6nzvn
Dec  2 09:45:47.145: INFO: Created: latency-svc-h2glp
Dec  2 09:45:47.154: INFO: Got endpoints: latency-svc-6nzvn [526.129355ms]
Dec  2 09:45:47.172: INFO: Got endpoints: latency-svc-h2glp [571.847993ms]
Dec  2 09:45:47.180: INFO: Created: latency-svc-sstf7
Dec  2 09:45:47.200: INFO: Got endpoints: latency-svc-sstf7 [530.069867ms]
Dec  2 09:45:47.285: INFO: Created: latency-svc-4qk8c
Dec  2 09:45:47.301: INFO: Got endpoints: latency-svc-4qk8c [582.621692ms]
Dec  2 09:45:47.302: INFO: Created: latency-svc-88r2n
Dec  2 09:45:47.320: INFO: Got endpoints: latency-svc-88r2n [650.285084ms]
Dec  2 09:45:47.326: INFO: Created: latency-svc-xrjwh
Dec  2 09:45:47.351: INFO: Got endpoints: latency-svc-xrjwh [606.892018ms]
Dec  2 09:45:47.373: INFO: Created: latency-svc-2s4dx
Dec  2 09:45:47.381: INFO: Got endpoints: latency-svc-2s4dx [605.793461ms]
Dec  2 09:45:47.464: INFO: Created: latency-svc-2t4jk
Dec  2 09:45:47.480: INFO: Got endpoints: latency-svc-2t4jk [641.984879ms]
Dec  2 09:45:47.512: INFO: Created: latency-svc-4vktb
Dec  2 09:45:47.523: INFO: Got endpoints: latency-svc-4vktb [528.497168ms]
Dec  2 09:45:47.524: INFO: Created: latency-svc-qcstp
Dec  2 09:45:47.534: INFO: Got endpoints: latency-svc-qcstp [605.498529ms]
Dec  2 09:45:47.553: INFO: Created: latency-svc-r45wx
Dec  2 09:45:47.575: INFO: Created: latency-svc-7zj86
Dec  2 09:45:47.583: INFO: Got endpoints: latency-svc-r45wx [656.2413ms]
Dec  2 09:45:47.587: INFO: Got endpoints: latency-svc-7zj86 [567.302476ms]
Dec  2 09:45:47.590: INFO: Created: latency-svc-nf87p
Dec  2 09:45:47.605: INFO: Got endpoints: latency-svc-nf87p [584.983806ms]
Dec  2 09:45:47.643: INFO: Created: latency-svc-zrwmw
Dec  2 09:45:47.663: INFO: Got endpoints: latency-svc-zrwmw [554.444717ms]
Dec  2 09:45:47.732: INFO: Created: latency-svc-2v5hg
Dec  2 09:45:47.752: INFO: Got endpoints: latency-svc-2v5hg [578.559707ms]
Dec  2 09:45:47.762: INFO: Created: latency-svc-xtk7j
Dec  2 09:45:47.777: INFO: Got endpoints: latency-svc-xtk7j [576.492399ms]
Dec  2 09:45:47.843: INFO: Created: latency-svc-bqdp5
Dec  2 09:45:47.860: INFO: Got endpoints: latency-svc-bqdp5 [509.329964ms]
Dec  2 09:45:47.883: INFO: Created: latency-svc-tnvxw
Dec  2 09:45:47.902: INFO: Created: latency-svc-fttq9
Dec  2 09:45:47.903: INFO: Got endpoints: latency-svc-tnvxw [777.218882ms]
Dec  2 09:45:47.910: INFO: Got endpoints: latency-svc-fttq9 [608.219507ms]
Dec  2 09:45:47.950: INFO: Created: latency-svc-vg7zm
Dec  2 09:45:47.968: INFO: Got endpoints: latency-svc-vg7zm [813.331331ms]
Dec  2 09:45:47.993: INFO: Created: latency-svc-xmb7q
Dec  2 09:45:48.011: INFO: Created: latency-svc-vcpzx
Dec  2 09:45:48.017: INFO: Got endpoints: latency-svc-xmb7q [697.044266ms]
Dec  2 09:45:48.034: INFO: Got endpoints: latency-svc-vcpzx [652.91347ms]
Dec  2 09:45:48.090: INFO: Created: latency-svc-pwjww
Dec  2 09:45:48.090: INFO: Got endpoints: latency-svc-pwjww [610.443355ms]
Dec  2 09:45:48.128: INFO: Created: latency-svc-pkd7g
Dec  2 09:45:48.148: INFO: Got endpoints: latency-svc-pkd7g [560.672915ms]
Dec  2 09:45:48.173: INFO: Created: latency-svc-z46gp
Dec  2 09:45:48.181: INFO: Got endpoints: latency-svc-z46gp [658.237576ms]
Dec  2 09:45:48.191: INFO: Created: latency-svc-b4ht6
Dec  2 09:45:48.194: INFO: Created: latency-svc-8h85h
Dec  2 09:45:48.221: INFO: Got endpoints: latency-svc-b4ht6 [687.165166ms]
Dec  2 09:45:48.223: INFO: Created: latency-svc-wmcxt
Dec  2 09:45:48.225: INFO: Got endpoints: latency-svc-8h85h [642.367233ms]
Dec  2 09:45:48.235: INFO: Got endpoints: latency-svc-wmcxt [572.361726ms]
Dec  2 09:45:48.258: INFO: Created: latency-svc-vb2zq
Dec  2 09:45:48.274: INFO: Got endpoints: latency-svc-vb2zq [669.345351ms]
Dec  2 09:45:48.331: INFO: Created: latency-svc-djpl5
Dec  2 09:45:48.340: INFO: Got endpoints: latency-svc-djpl5 [563.071218ms]
Dec  2 09:45:48.371: INFO: Created: latency-svc-pgzd9
Dec  2 09:45:48.380: INFO: Got endpoints: latency-svc-pgzd9 [628.43956ms]
Dec  2 09:45:48.439: INFO: Created: latency-svc-gcv55
Dec  2 09:45:48.456: INFO: Got endpoints: latency-svc-gcv55 [545.837515ms]
Dec  2 09:45:48.459: INFO: Created: latency-svc-pb2r5
Dec  2 09:45:48.478: INFO: Got endpoints: latency-svc-pb2r5 [617.441321ms]
Dec  2 09:45:48.504: INFO: Created: latency-svc-4b2fs
Dec  2 09:45:48.513: INFO: Got endpoints: latency-svc-4b2fs [609.210791ms]
Dec  2 09:45:48.523: INFO: Created: latency-svc-2s85r
Dec  2 09:45:48.544: INFO: Got endpoints: latency-svc-2s85r [575.92553ms]
Dec  2 09:45:48.621: INFO: Created: latency-svc-q5f5s
Dec  2 09:45:48.631: INFO: Created: latency-svc-rfgtg
Dec  2 09:45:48.647: INFO: Got endpoints: latency-svc-q5f5s [630.136818ms]
Dec  2 09:45:48.647: INFO: Got endpoints: latency-svc-rfgtg [612.833462ms]
Dec  2 09:45:48.688: INFO: Created: latency-svc-hbzcz
Dec  2 09:45:48.704: INFO: Got endpoints: latency-svc-hbzcz [613.596499ms]
Dec  2 09:45:48.727: INFO: Created: latency-svc-k75rb
Dec  2 09:45:48.745: INFO: Got endpoints: latency-svc-k75rb [596.914579ms]
Dec  2 09:45:48.802: INFO: Created: latency-svc-skcl4
Dec  2 09:45:48.819: INFO: Got endpoints: latency-svc-skcl4 [637.843771ms]
Dec  2 09:45:48.819: INFO: Created: latency-svc-wwfpr
Dec  2 09:45:48.838: INFO: Created: latency-svc-jxtpm
Dec  2 09:45:48.840: INFO: Got endpoints: latency-svc-wwfpr [604.367928ms]
Dec  2 09:45:48.857: INFO: Got endpoints: latency-svc-jxtpm [631.478453ms]
Dec  2 09:45:48.864: INFO: Created: latency-svc-c5q4s
Dec  2 09:45:48.880: INFO: Got endpoints: latency-svc-c5q4s [539.454027ms]
Dec  2 09:45:48.933: INFO: Created: latency-svc-2qmpl
Dec  2 09:45:48.943: INFO: Got endpoints: latency-svc-2qmpl [669.332597ms]
Dec  2 09:45:49.009: INFO: Created: latency-svc-r9mlp
Dec  2 09:45:49.022: INFO: Got endpoints: latency-svc-r9mlp [800.790825ms]
Dec  2 09:45:49.049: INFO: Created: latency-svc-f4hvx
Dec  2 09:45:49.065: INFO: Got endpoints: latency-svc-f4hvx [587.126639ms]
Dec  2 09:45:49.087: INFO: Created: latency-svc-qngt8
Dec  2 09:45:49.088: INFO: Created: latency-svc-jwlqj
Dec  2 09:45:49.093: INFO: Created: latency-svc-9xngm
Dec  2 09:45:49.101: INFO: Got endpoints: latency-svc-qngt8 [720.446106ms]
Dec  2 09:45:49.111: INFO: Got endpoints: latency-svc-jwlqj [567.007279ms]
Dec  2 09:45:49.119: INFO: Got endpoints: latency-svc-9xngm [663.504054ms]
Dec  2 09:45:49.170: INFO: Created: latency-svc-cmtrk
Dec  2 09:45:49.188: INFO: Got endpoints: latency-svc-cmtrk [674.881175ms]
Dec  2 09:45:49.218: INFO: Created: latency-svc-x77ps
Dec  2 09:45:49.230: INFO: Got endpoints: latency-svc-x77ps [582.467425ms]
Dec  2 09:45:49.320: INFO: Created: latency-svc-zxg52
Dec  2 09:45:49.337: INFO: Got endpoints: latency-svc-zxg52 [632.789487ms]
Dec  2 09:45:49.342: INFO: Created: latency-svc-gg24n
Dec  2 09:45:49.351: INFO: Created: latency-svc-kpbz9
Dec  2 09:45:49.361: INFO: Got endpoints: latency-svc-gg24n [616.306704ms]
Dec  2 09:45:49.365: INFO: Got endpoints: latency-svc-kpbz9 [717.806921ms]
Dec  2 09:45:49.405: INFO: Created: latency-svc-4fv7w
Dec  2 09:45:49.422: INFO: Got endpoints: latency-svc-4fv7w [542.3955ms]
Dec  2 09:45:49.466: INFO: Created: latency-svc-9gnnl
Dec  2 09:45:49.484: INFO: Got endpoints: latency-svc-9gnnl [644.407731ms]
Dec  2 09:45:49.489: INFO: Created: latency-svc-whjcc
Dec  2 09:45:49.513: INFO: Got endpoints: latency-svc-whjcc [655.032612ms]
Dec  2 09:45:49.514: INFO: Created: latency-svc-4hndx
Dec  2 09:45:49.527: INFO: Got endpoints: latency-svc-4hndx [583.109314ms]
Dec  2 09:45:49.528: INFO: Created: latency-svc-47ss5
Dec  2 09:45:49.547: INFO: Got endpoints: latency-svc-47ss5 [727.421062ms]
Dec  2 09:45:49.563: INFO: Created: latency-svc-wdsl5
Dec  2 09:45:49.580: INFO: Got endpoints: latency-svc-wdsl5 [515.695313ms]
Dec  2 09:45:49.604: INFO: Created: latency-svc-mpzg5
Dec  2 09:45:49.621: INFO: Got endpoints: latency-svc-mpzg5 [599.564256ms]
Dec  2 09:45:49.654: INFO: Created: latency-svc-bfkr2
Dec  2 09:45:49.673: INFO: Got endpoints: latency-svc-bfkr2 [553.401025ms]
Dec  2 09:45:49.694: INFO: Created: latency-svc-bq8c9
Dec  2 09:45:49.703: INFO: Got endpoints: latency-svc-bq8c9 [602.092466ms]
Dec  2 09:45:49.707: INFO: Created: latency-svc-rmm9f
Dec  2 09:45:49.739: INFO: Got endpoints: latency-svc-rmm9f [628.615276ms]
Dec  2 09:45:49.785: INFO: Created: latency-svc-rlw25
Dec  2 09:45:49.796: INFO: Got endpoints: latency-svc-rlw25 [566.269898ms]
Dec  2 09:45:49.840: INFO: Created: latency-svc-7pz7r
Dec  2 09:45:49.860: INFO: Got endpoints: latency-svc-7pz7r [672.377765ms]
Dec  2 09:45:49.900: INFO: Created: latency-svc-5m7zq
Dec  2 09:45:49.917: INFO: Got endpoints: latency-svc-5m7zq [555.35892ms]
Dec  2 09:45:49.919: INFO: Created: latency-svc-xvj24
Dec  2 09:45:49.929: INFO: Got endpoints: latency-svc-xvj24 [563.501127ms]
Dec  2 09:45:49.948: INFO: Created: latency-svc-t7p62
Dec  2 09:45:49.965: INFO: Got endpoints: latency-svc-t7p62 [543.006748ms]
Dec  2 09:45:49.974: INFO: Created: latency-svc-dcfjf
Dec  2 09:45:49.990: INFO: Got endpoints: latency-svc-dcfjf [653.569834ms]
Dec  2 09:45:50.080: INFO: Created: latency-svc-6h8sr
Dec  2 09:45:50.086: INFO: Created: latency-svc-srm8d
Dec  2 09:45:50.100: INFO: Got endpoints: latency-svc-6h8sr [553.046459ms]
Dec  2 09:45:50.103: INFO: Got endpoints: latency-svc-srm8d [618.343637ms]
Dec  2 09:45:50.141: INFO: Created: latency-svc-hvpps
Dec  2 09:45:50.165: INFO: Created: latency-svc-k4lvr
Dec  2 09:45:50.165: INFO: Got endpoints: latency-svc-hvpps [544.027143ms]
Dec  2 09:45:50.174: INFO: Got endpoints: latency-svc-k4lvr [660.523073ms]
Dec  2 09:45:50.191: INFO: Created: latency-svc-vzzdh
Dec  2 09:45:50.205: INFO: Created: latency-svc-d9hlm
Dec  2 09:45:50.208: INFO: Got endpoints: latency-svc-vzzdh [679.884532ms]
Dec  2 09:45:50.222: INFO: Got endpoints: latency-svc-d9hlm [641.613244ms]
Dec  2 09:45:50.228: INFO: Created: latency-svc-56pd4
Dec  2 09:45:50.240: INFO: Got endpoints: latency-svc-56pd4 [537.486451ms]
Dec  2 09:45:50.266: INFO: Created: latency-svc-zn7sm
Dec  2 09:45:50.286: INFO: Got endpoints: latency-svc-zn7sm [612.948858ms]
Dec  2 09:45:50.300: INFO: Created: latency-svc-qwjx6
Dec  2 09:45:50.312: INFO: Got endpoints: latency-svc-qwjx6 [572.819463ms]
Dec  2 09:45:50.383: INFO: Created: latency-svc-96l9c
Dec  2 09:45:50.402: INFO: Got endpoints: latency-svc-96l9c [606.260836ms]
Dec  2 09:45:50.406: INFO: Created: latency-svc-9c5kw
Dec  2 09:45:50.416: INFO: Got endpoints: latency-svc-9c5kw [499.524494ms]
Dec  2 09:45:50.446: INFO: Created: latency-svc-6dd86
Dec  2 09:45:50.466: INFO: Got endpoints: latency-svc-6dd86 [605.254798ms]
Dec  2 09:45:50.466: INFO: Created: latency-svc-tn4l7
Dec  2 09:45:50.482: INFO: Got endpoints: latency-svc-tn4l7 [491.982789ms]
Dec  2 09:45:50.503: INFO: Created: latency-svc-shb7f
Dec  2 09:45:50.521: INFO: Got endpoints: latency-svc-shb7f [556.000901ms]
Dec  2 09:45:50.536: INFO: Created: latency-svc-m9jnh
Dec  2 09:45:50.553: INFO: Got endpoints: latency-svc-m9jnh [624.427201ms]
Dec  2 09:45:50.620: INFO: Created: latency-svc-nh27f
Dec  2 09:45:50.630: INFO: Got endpoints: latency-svc-nh27f [528.338003ms]
Dec  2 09:45:50.656: INFO: Created: latency-svc-prd6h
Dec  2 09:45:50.665: INFO: Got endpoints: latency-svc-prd6h [562.043112ms]
Dec  2 09:45:50.742: INFO: Created: latency-svc-psdzd
Dec  2 09:45:50.752: INFO: Got endpoints: latency-svc-psdzd [578.525807ms]
Dec  2 09:45:50.781: INFO: Created: latency-svc-4kz7n
Dec  2 09:45:50.799: INFO: Got endpoints: latency-svc-4kz7n [633.859859ms]
Dec  2 09:45:50.826: INFO: Created: latency-svc-7z6qb
Dec  2 09:45:50.847: INFO: Got endpoints: latency-svc-7z6qb [606.074004ms]
Dec  2 09:45:50.852: INFO: Created: latency-svc-lst74
Dec  2 09:45:50.870: INFO: Got endpoints: latency-svc-lst74 [584.266442ms]
Dec  2 09:45:50.881: INFO: Created: latency-svc-9w4xc
Dec  2 09:45:50.881: INFO: Created: latency-svc-bnqk6
Dec  2 09:45:50.906: INFO: Got endpoints: latency-svc-bnqk6 [683.317566ms]
Dec  2 09:45:50.907: INFO: Got endpoints: latency-svc-9w4xc [699.321452ms]
Dec  2 09:45:50.914: INFO: Created: latency-svc-2fssd
Dec  2 09:45:50.956: INFO: Got endpoints: latency-svc-2fssd [643.750892ms]
Dec  2 09:45:50.963: INFO: Created: latency-svc-dzmhn
Dec  2 09:45:50.991: INFO: Got endpoints: latency-svc-dzmhn [588.760913ms]
Dec  2 09:45:51.018: INFO: Created: latency-svc-wx9l8
Dec  2 09:45:51.047: INFO: Got endpoints: latency-svc-wx9l8 [631.15885ms]
Dec  2 09:45:51.048: INFO: Created: latency-svc-5tq29
Dec  2 09:45:51.071: INFO: Created: latency-svc-m6dfs
Dec  2 09:45:51.089: INFO: Got endpoints: latency-svc-5tq29 [623.141255ms]
Dec  2 09:45:51.126: INFO: Created: latency-svc-mpqb6
Dec  2 09:45:51.140: INFO: Got endpoints: latency-svc-m6dfs [657.525728ms]
Dec  2 09:45:51.155: INFO: Created: latency-svc-gwxtd
Dec  2 09:45:51.169: INFO: Created: latency-svc-rvm49
Dec  2 09:45:51.197: INFO: Got endpoints: latency-svc-mpqb6 [675.500753ms]
Dec  2 09:45:51.197: INFO: Created: latency-svc-6s2d4
Dec  2 09:45:51.242: INFO: Got endpoints: latency-svc-gwxtd [688.341371ms]
Dec  2 09:45:51.269: INFO: Created: latency-svc-8jhl9
Dec  2 09:45:51.291: INFO: Got endpoints: latency-svc-rvm49 [625.713237ms]
Dec  2 09:45:51.339: INFO: Got endpoints: latency-svc-6s2d4 [708.638562ms]
Dec  2 09:45:51.355: INFO: Created: latency-svc-bdvgd
Dec  2 09:45:51.398: INFO: Got endpoints: latency-svc-8jhl9 [646.063312ms]
Dec  2 09:45:51.403: INFO: Created: latency-svc-rl649
Dec  2 09:45:51.447: INFO: Got endpoints: latency-svc-bdvgd [600.14304ms]
Dec  2 09:45:51.481: INFO: Created: latency-svc-f9d2x
Dec  2 09:45:51.497: INFO: Got endpoints: latency-svc-rl649 [626.801363ms]
Dec  2 09:45:51.513: INFO: Created: latency-svc-xssd5
Dec  2 09:45:51.586: INFO: Got endpoints: latency-svc-f9d2x [679.148567ms]
Dec  2 09:45:51.591: INFO: Created: latency-svc-wx6hp
Dec  2 09:45:51.612: INFO: Created: latency-svc-mkp8r
Dec  2 09:45:51.619: INFO: Got endpoints: latency-svc-xssd5 [712.805092ms]
Dec  2 09:45:51.621: INFO: Created: latency-svc-nf4bk
Dec  2 09:45:51.646: INFO: Got endpoints: latency-svc-wx6hp [655.059773ms]
Dec  2 09:45:51.671: INFO: Created: latency-svc-d62cp
Dec  2 09:45:51.676: INFO: Created: latency-svc-qmghq
Dec  2 09:45:51.702: INFO: Got endpoints: latency-svc-mkp8r [745.657808ms]
Dec  2 09:45:51.703: INFO: Created: latency-svc-97xq6
Dec  2 09:45:51.736: INFO: Created: latency-svc-fds6t
Dec  2 09:45:51.749: INFO: Got endpoints: latency-svc-nf4bk [701.072051ms]
Dec  2 09:45:51.797: INFO: Got endpoints: latency-svc-qmghq [656.897022ms]
Dec  2 09:45:51.798: INFO: Created: latency-svc-2ktq6
Dec  2 09:45:51.841: INFO: Got endpoints: latency-svc-d62cp [752.232058ms]
Dec  2 09:45:51.870: INFO: Created: latency-svc-4wvwj
Dec  2 09:45:51.890: INFO: Got endpoints: latency-svc-97xq6 [693.207364ms]
Dec  2 09:45:51.931: INFO: Created: latency-svc-nk2lw
Dec  2 09:45:51.947: INFO: Got endpoints: latency-svc-fds6t [656.245542ms]
Dec  2 09:45:51.952: INFO: Created: latency-svc-mjmrd
Dec  2 09:45:51.991: INFO: Got endpoints: latency-svc-2ktq6 [749.427937ms]
Dec  2 09:45:52.048: INFO: Got endpoints: latency-svc-4wvwj [709.260073ms]
Dec  2 09:45:52.089: INFO: Got endpoints: latency-svc-nk2lw [641.699404ms]
Dec  2 09:45:52.148: INFO: Got endpoints: latency-svc-mjmrd [749.911983ms]
Dec  2 09:45:52.178: INFO: Created: latency-svc-6px9r
Dec  2 09:45:52.197: INFO: Got endpoints: latency-svc-6px9r [578.0113ms]
Dec  2 09:45:52.337: INFO: Created: latency-svc-ht9gv
Dec  2 09:45:52.354: INFO: Got endpoints: latency-svc-ht9gv [604.564119ms]
Dec  2 09:45:52.366: INFO: Created: latency-svc-994mj
Dec  2 09:45:52.390: INFO: Created: latency-svc-rtst7
Dec  2 09:45:52.392: INFO: Got endpoints: latency-svc-994mj [550.138344ms]
Dec  2 09:45:52.402: INFO: Created: latency-svc-7287q
Dec  2 09:45:52.412: INFO: Got endpoints: latency-svc-rtst7 [709.236126ms]
Dec  2 09:45:52.433: INFO: Got endpoints: latency-svc-7287q [636.177947ms]
Dec  2 09:45:52.434: INFO: Created: latency-svc-q9x6s
Dec  2 09:45:52.446: INFO: Got endpoints: latency-svc-q9x6s [857.060667ms]
Dec  2 09:45:52.505: INFO: Created: latency-svc-25dtv
Dec  2 09:45:52.526: INFO: Got endpoints: latency-svc-25dtv [879.597837ms]
Dec  2 09:45:52.577: INFO: Created: latency-svc-ncln6
Dec  2 09:45:52.588: INFO: Got endpoints: latency-svc-ncln6 [596.601488ms]
Dec  2 09:45:52.642: INFO: Created: latency-svc-48vtl
Dec  2 09:45:52.653: INFO: Got endpoints: latency-svc-48vtl [605.144203ms]
Dec  2 09:45:52.683: INFO: Created: latency-svc-8qn6f
Dec  2 09:45:52.686: INFO: Created: latency-svc-8brlx
Dec  2 09:45:52.695: INFO: Got endpoints: latency-svc-8qn6f [606.18749ms]
Dec  2 09:45:52.714: INFO: Got endpoints: latency-svc-8brlx [516.36377ms]
Dec  2 09:45:52.722: INFO: Created: latency-svc-7mzkg
Dec  2 09:45:52.725: INFO: Created: latency-svc-j2nll
Dec  2 09:45:52.754: INFO: Got endpoints: latency-svc-7mzkg [863.584466ms]
Dec  2 09:45:52.796: INFO: Got endpoints: latency-svc-j2nll [849.166235ms]
Dec  2 09:45:52.863: INFO: Created: latency-svc-pbhfr
Dec  2 09:45:52.874: INFO: Got endpoints: latency-svc-pbhfr [520.138243ms]
Dec  2 09:45:52.886: INFO: Created: latency-svc-28pnt
Dec  2 09:45:52.895: INFO: Got endpoints: latency-svc-28pnt [503.322802ms]
Dec  2 09:45:52.906: INFO: Created: latency-svc-694mx
Dec  2 09:45:52.941: INFO: Got endpoints: latency-svc-694mx [792.787227ms]
Dec  2 09:45:52.963: INFO: Created: latency-svc-qbn4n
Dec  2 09:45:52.989: INFO: Created: latency-svc-klshl
Dec  2 09:45:52.998: INFO: Got endpoints: latency-svc-qbn4n [564.992099ms]
Dec  2 09:45:53.001: INFO: Created: latency-svc-p4shc
Dec  2 09:45:53.040: INFO: Got endpoints: latency-svc-klshl [627.594404ms]
Dec  2 09:45:53.048: INFO: Created: latency-svc-gzf72
Dec  2 09:45:53.100: INFO: Got endpoints: latency-svc-p4shc [653.351362ms]
Dec  2 09:45:53.104: INFO: Created: latency-svc-6rqkj
Dec  2 09:45:53.147: INFO: Got endpoints: latency-svc-gzf72 [619.758707ms]
Dec  2 09:45:53.175: INFO: Created: latency-svc-tjbfm
Dec  2 09:45:53.200: INFO: Got endpoints: latency-svc-6rqkj [612.265211ms]
Dec  2 09:45:53.214: INFO: Created: latency-svc-svsv9
Dec  2 09:45:53.238: INFO: Created: latency-svc-ssj5f
Dec  2 09:45:53.255: INFO: Got endpoints: latency-svc-tjbfm [540.986823ms]
Dec  2 09:45:53.257: INFO: Created: latency-svc-dnhvg
Dec  2 09:45:53.267: INFO: Created: latency-svc-5qz7k
Dec  2 09:45:53.290: INFO: Got endpoints: latency-svc-svsv9 [536.347531ms]
Dec  2 09:45:53.354: INFO: Got endpoints: latency-svc-ssj5f [700.738148ms]
Dec  2 09:45:53.391: INFO: Got endpoints: latency-svc-dnhvg [696.233785ms]
Dec  2 09:45:53.439: INFO: Got endpoints: latency-svc-5qz7k [641.979936ms]
Dec  2 09:45:54.387: INFO: Created: latency-svc-7pwwg
Dec  2 09:45:54.398: INFO: Got endpoints: latency-svc-7pwwg [3.598592147s]
Dec  2 09:45:55.505: INFO: Created: latency-svc-hb7bj
Dec  2 09:45:55.529: INFO: Got endpoints: latency-svc-hb7bj [4.032238942s]
Dec  2 09:45:55.529: INFO: Latencies: [491.982789ms 499.524494ms 503.322802ms 503.781665ms 509.329964ms 515.695313ms 516.36377ms 520.138243ms 526.129355ms 528.338003ms 528.3544ms 528.497168ms 530.069867ms 536.347531ms 536.374093ms 537.214966ms 537.486451ms 537.591478ms 539.267937ms 539.454027ms 540.986823ms 542.3955ms 543.006748ms 544.027143ms 545.837515ms 545.925427ms 545.960698ms 547.165261ms 550.138344ms 553.046459ms 553.401025ms 554.444717ms 555.35892ms 556.000901ms 560.672915ms 562.043112ms 563.071218ms 563.501127ms 564.992099ms 566.269898ms 567.007279ms 567.302476ms 567.687228ms 567.804116ms 570.10378ms 571.847993ms 572.361726ms 572.516546ms 572.732661ms 572.819463ms 575.205468ms 575.386682ms 575.92553ms 576.387288ms 576.492399ms 578.0113ms 578.525807ms 578.559707ms 581.749866ms 582.422632ms 582.467425ms 582.621692ms 583.109314ms 584.266442ms 584.983806ms 585.642246ms 586.96181ms 587.126639ms 588.760913ms 596.601488ms 596.914579ms 599.564256ms 600.14304ms 601.893895ms 602.092466ms 604.367928ms 604.564119ms 605.144203ms 605.254798ms 605.498529ms 605.793461ms 606.074004ms 606.150217ms 606.18749ms 606.260836ms 606.892018ms 608.219507ms 608.315616ms 609.210791ms 609.773945ms 610.443355ms 612.265211ms 612.833462ms 612.948858ms 613.596499ms 614.508436ms 615.120338ms 616.306704ms 617.441321ms 617.506294ms 618.343637ms 619.758707ms 620.099885ms 621.034949ms 623.141255ms 624.427201ms 625.713237ms 626.801363ms 627.594404ms 628.43956ms 628.615276ms 629.470157ms 630.136818ms 631.15885ms 631.478453ms 632.789487ms 633.859859ms 636.177947ms 637.843771ms 640.505592ms 641.613244ms 641.699404ms 641.979936ms 641.984879ms 642.007582ms 642.182127ms 642.367233ms 643.750892ms 644.407731ms 644.716289ms 646.063312ms 650.285084ms 652.91347ms 653.351362ms 653.569834ms 654.956853ms 655.032612ms 655.059773ms 656.2413ms 656.245542ms 656.897022ms 657.525728ms 658.237576ms 660.00206ms 660.523073ms 663.504054ms 669.332597ms 669.345351ms 672.377765ms 672.671936ms 674.881175ms 674.927608ms 675.500753ms 675.631798ms 679.148567ms 679.884532ms 682.97997ms 683.317566ms 683.375073ms 686.497943ms 687.165166ms 688.341371ms 690.783035ms 693.207364ms 696.233785ms 697.044266ms 699.321452ms 700.738148ms 701.072051ms 708.638562ms 709.236126ms 709.260073ms 712.805092ms 717.806921ms 720.446106ms 727.421062ms 740.036805ms 745.657808ms 749.427937ms 749.911983ms 752.232058ms 777.218882ms 780.337963ms 792.787227ms 800.790825ms 813.331331ms 849.166235ms 851.520049ms 857.060667ms 863.584466ms 879.597837ms 892.765604ms 907.356471ms 941.585518ms 942.138179ms 979.760746ms 995.187818ms 1.162501681s 3.598592147s 4.032238942s]
Dec  2 09:45:55.530: INFO: 50 %ile: 618.343637ms
Dec  2 09:45:55.530: INFO: 90 %ile: 752.232058ms
Dec  2 09:45:55.530: INFO: 99 %ile: 3.598592147s
Dec  2 09:45:55.530: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:45:55.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8121" for this suite.

• [SLOW TEST:16.965 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":82,"skipped":1331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:45:56.106: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-webhook
W1202 09:45:56.633592      23 warnings.go:70] No static IP address has been configured for the namespace "crd-webhook-5634", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  2 09:45:57.457: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec  2 09:45:59.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85f9c96f6f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:46:01.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85f9c96f6f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:46:03.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571157, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85f9c96f6f\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 09:46:06.991: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:07.991: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:08.992: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:09.995: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:10.993: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:11.992: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:12.991: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:13.992: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
Dec  2 09:46:14.991: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:46:15.019: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:46:23.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5634" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:28.154 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":83,"skipped":1373,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:46:24.261: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:46:24.777243      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-1382", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-bb5af2ca-9474-471e-a14a-daa6c660614f
STEP: Creating a pod to test consume secrets
Dec  2 09:46:25.026: INFO: Waiting up to 5m0s for pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c" in namespace "secrets-1382" to be "Succeeded or Failed"
Dec  2 09:46:25.056: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.213351ms
Dec  2 09:46:27.073: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046756235s
Dec  2 09:46:29.086: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060443497s
Dec  2 09:46:31.096: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070182424s
Dec  2 09:46:33.107: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081528658s
Dec  2 09:46:35.120: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.09415588s
STEP: Saw pod success
Dec  2 09:46:35.120: INFO: Pod "pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c" satisfied condition "Succeeded or Failed"
Dec  2 09:46:35.127: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 09:46:35.183: INFO: Waiting for pod pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c to disappear
Dec  2 09:46:35.190: INFO: Pod pod-secrets-3a99c99b-7fd4-4c9f-b101-09931eefb44c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:46:35.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1382" for this suite.

• [SLOW TEST:11.399 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1386,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:46:35.661: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 09:46:36.521922      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-7317", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  2 09:46:36.728: INFO: Waiting up to 5m0s for pod "pod-01736133-3672-454f-9652-3cf1a1def6e4" in namespace "emptydir-7317" to be "Succeeded or Failed"
Dec  2 09:46:36.749: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.697816ms
Dec  2 09:46:38.760: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031793744s
Dec  2 09:46:40.771: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042561794s
Dec  2 09:46:42.781: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052579326s
Dec  2 09:46:44.793: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064027899s
Dec  2 09:46:46.804: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.07583024s
Dec  2 09:46:48.816: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08761746s
Dec  2 09:46:50.826: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.096862226s
STEP: Saw pod success
Dec  2 09:46:50.826: INFO: Pod "pod-01736133-3672-454f-9652-3cf1a1def6e4" satisfied condition "Succeeded or Failed"
Dec  2 09:46:50.835: INFO: Trying to get logs from node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa pod pod-01736133-3672-454f-9652-3cf1a1def6e4 container test-container: <nil>
STEP: delete the pod
Dec  2 09:46:50.882: INFO: Waiting for pod pod-01736133-3672-454f-9652-3cf1a1def6e4 to disappear
Dec  2 09:46:50.888: INFO: Pod pod-01736133-3672-454f-9652-3cf1a1def6e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:46:50.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7317" for this suite.

• [SLOW TEST:15.765 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1400,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:46:51.426: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename events
W1202 09:46:51.907469      23 warnings.go:70] No static IP address has been configured for the namespace "events-4477", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  2 09:47:04.149: INFO: &Pod{ObjectMeta:{send-events-ddcd2f4b-d8ab-421c-aab9-5f93718785db  events-4477  085739a6-6284-4468-b26c-53cededf9927 52170518 0 2022-12-02 09:46:52 +0000 UTC <nil> <nil> map[name:foo time:87828135] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-12-02 09:46:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 09:47:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.27.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g569w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g569w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:46:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:47:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:47:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:46:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.5,PodIP:11.34.27.2,StartTime:2022-12-02 09:46:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 09:47:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:docker://sha256:a05bd3a9140b72c5a17eb6881a75c5003b270c0b3e32a995fb10ec96004279d2,ContainerID:docker://48a7e1791dffe4583789b43ac90c14baf34e49639090aca28e9027f5a97ae503,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.27.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec  2 09:47:06.164: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  2 09:47:08.179: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:47:08.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4477" for this suite.

• [SLOW TEST:17.282 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":86,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:47:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename podtemplate
W1202 09:47:09.229836      23 warnings.go:70] No static IP address has been configured for the namespace "podtemplate-6342", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:47:09.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6342" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":87,"skipped":1438,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:47:10.018: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 09:47:10.875241      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-4098", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec  2 09:47:11.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 create -f -'
Dec  2 09:47:13.826: INFO: stderr: ""
Dec  2 09:47:13.826: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  2 09:47:13.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:47:13.946: INFO: stderr: ""
Dec  2 09:47:13.946: INFO: stdout: "update-demo-nautilus-624s7 update-demo-nautilus-c88xf "
Dec  2 09:47:13.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:14.027: INFO: stderr: ""
Dec  2 09:47:14.027: INFO: stdout: ""
Dec  2 09:47:14.027: INFO: update-demo-nautilus-624s7 is created but not running
Dec  2 09:47:19.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:47:19.115: INFO: stderr: ""
Dec  2 09:47:19.115: INFO: stdout: "update-demo-nautilus-624s7 update-demo-nautilus-c88xf "
Dec  2 09:47:19.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:19.221: INFO: stderr: ""
Dec  2 09:47:19.221: INFO: stdout: ""
Dec  2 09:47:19.221: INFO: update-demo-nautilus-624s7 is created but not running
Dec  2 09:47:24.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:47:24.307: INFO: stderr: ""
Dec  2 09:47:24.307: INFO: stdout: "update-demo-nautilus-624s7 update-demo-nautilus-c88xf "
Dec  2 09:47:24.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:24.384: INFO: stderr: ""
Dec  2 09:47:24.384: INFO: stdout: "true"
Dec  2 09:47:24.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:47:24.478: INFO: stderr: ""
Dec  2 09:47:24.478: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:47:24.478: INFO: validating pod update-demo-nautilus-624s7
Dec  2 09:47:24.488: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:47:24.488: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:47:24.488: INFO: update-demo-nautilus-624s7 is verified up and running
Dec  2 09:47:24.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-c88xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:24.567: INFO: stderr: ""
Dec  2 09:47:24.567: INFO: stdout: ""
Dec  2 09:47:24.567: INFO: update-demo-nautilus-c88xf is created but not running
Dec  2 09:47:29.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:47:29.653: INFO: stderr: ""
Dec  2 09:47:29.653: INFO: stdout: "update-demo-nautilus-624s7 update-demo-nautilus-c88xf "
Dec  2 09:47:29.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:29.741: INFO: stderr: ""
Dec  2 09:47:29.741: INFO: stdout: "true"
Dec  2 09:47:29.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:47:29.818: INFO: stderr: ""
Dec  2 09:47:29.818: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:47:29.818: INFO: validating pod update-demo-nautilus-624s7
Dec  2 09:47:29.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:47:29.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:47:29.826: INFO: update-demo-nautilus-624s7 is verified up and running
Dec  2 09:47:29.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-c88xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:29.920: INFO: stderr: ""
Dec  2 09:47:29.920: INFO: stdout: ""
Dec  2 09:47:29.920: INFO: update-demo-nautilus-c88xf is created but not running
Dec  2 09:47:34.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  2 09:47:35.011: INFO: stderr: ""
Dec  2 09:47:35.011: INFO: stdout: "update-demo-nautilus-624s7 update-demo-nautilus-c88xf "
Dec  2 09:47:35.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:35.118: INFO: stderr: ""
Dec  2 09:47:35.118: INFO: stdout: "true"
Dec  2 09:47:35.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-624s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:47:35.230: INFO: stderr: ""
Dec  2 09:47:35.230: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:47:35.230: INFO: validating pod update-demo-nautilus-624s7
Dec  2 09:47:35.236: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:47:35.236: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:47:35.236: INFO: update-demo-nautilus-624s7 is verified up and running
Dec  2 09:47:35.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-c88xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  2 09:47:35.312: INFO: stderr: ""
Dec  2 09:47:35.312: INFO: stdout: "true"
Dec  2 09:47:35.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods update-demo-nautilus-c88xf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  2 09:47:35.393: INFO: stderr: ""
Dec  2 09:47:35.393: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  2 09:47:35.393: INFO: validating pod update-demo-nautilus-c88xf
Dec  2 09:47:35.408: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  2 09:47:35.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  2 09:47:35.408: INFO: update-demo-nautilus-c88xf is verified up and running
STEP: using delete to clean up resources
Dec  2 09:47:35.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 delete --grace-period=0 --force -f -'
Dec  2 09:47:35.492: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 09:47:35.492: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  2 09:47:35.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get rc,svc -l name=update-demo --no-headers'
Dec  2 09:47:35.611: INFO: stderr: "No resources found in kubectl-4098 namespace.\n"
Dec  2 09:47:35.611: INFO: stdout: ""
Dec  2 09:47:35.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-4098 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  2 09:47:35.711: INFO: stderr: ""
Dec  2 09:47:35.711: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:47:35.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4098" for this suite.

• [SLOW TEST:26.240 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":88,"skipped":1440,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:47:36.259: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename init-container
W1202 09:47:36.780406      23 warnings.go:70] No static IP address has been configured for the namespace "init-container-389", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  2 09:47:36.956: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:47:46.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-389" for this suite.

• [SLOW TEST:10.607 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":89,"skipped":1446,"failed":0}
SSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:47:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption
W1202 09:47:47.442867      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-8458", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-8458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:47:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8458" for this suite.

• [SLOW TEST:5.410 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":90,"skipped":1451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:47:52.279: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 09:47:52.819929      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-1240", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:48:21.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1240" for this suite.

• [SLOW TEST:29.313 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":91,"skipped":1492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:48:21.593: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename endpointslice
W1202 09:48:22.156247      23 warnings.go:70] No static IP address has been configured for the namespace "endpointslice-1681", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Dec  2 09:48:49.079: INFO: EndpointSlice for Service endpointslice-1681/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:48:59.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1681" for this suite.

• [SLOW TEST:38.003 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":92,"skipped":1514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:48:59.596: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 09:49:00.091148      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-6732", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:49:00.314: INFO: Creating simple deployment test-new-deployment
Dec  2 09:49:00.352: INFO: deployment "test-new-deployment" doesn't have the required revision set
Dec  2 09:49:02.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:49:04.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 09:49:06.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805571340, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 09:49:08.475: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6732  8cdc3eac-1fe1-4cf3-8294-b66ecd98ba5a 52171772 3 2022-12-02 09:49:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-02 09:49:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00e52b688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-02 09:49:06 +0000 UTC,LastTransitionTime:2022-12-02 09:49:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-12-02 09:49:06 +0000 UTC,LastTransitionTime:2022-12-02 09:49:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  2 09:49:08.503: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-6732  4d0d7b35-0792-4a9c-8a76-1e16feee1286 52171777 3 2022-12-02 09:49:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8cdc3eac-1fe1-4cf3-8294-b66ecd98ba5a 0xc00e52baa7 0xc00e52baa8}] []  [{kube-controller-manager Update apps/v1 2022-12-02 09:49:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cdc3eac-1fe1-4cf3-8294-b66ecd98ba5a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 09:49:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00e52bb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  2 09:49:08.540: INFO: Pod "test-new-deployment-847dcfb7fb-hwdg7" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-hwdg7 test-new-deployment-847dcfb7fb- deployment-6732  d9c50879-e004-4235-92ff-c05bda0666fe 52171747 0 2022-12-02 09:49:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4d0d7b35-0792-4a9c-8a76-1e16feee1286 0xc00e52bf27 0xc00e52bf28}] []  [{kube-controller-manager Update v1 2022-12-02 09:49:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d0d7b35-0792-4a9c-8a76-1e16feee1286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 09:49:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.26.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnfxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnfxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.5,PodIP:11.34.26.2,StartTime:2022-12-02 09:49:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 09:49:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://b3f7717fbb390327453fdf43f8ba8d68e59f9069d3e65fce5dae447e5b48ef1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.26.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 09:49:08.540: INFO: Pod "test-new-deployment-847dcfb7fb-s2k8m" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-s2k8m test-new-deployment-847dcfb7fb- deployment-6732  ba40a738-34f9-4e16-9b98-3c12399b253a 52171783 0 2022-12-02 09:49:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4d0d7b35-0792-4a9c-8a76-1e16feee1286 0xc006a98107 0xc006a98108}] []  [{kube-controller-manager Update v1 2022-12-02 09:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d0d7b35-0792-4a9c-8a76-1e16feee1286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 09:49:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jt45s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jt45s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 09:49:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:,StartTime:2022-12-02 09:49:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:49:08.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6732" for this suite.

• [SLOW TEST:9.528 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":93,"skipped":1546,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:49:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 09:49:09.568506      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-867", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  2 09:49:09.787: INFO: Waiting up to 5m0s for pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c" in namespace "downward-api-867" to be "Succeeded or Failed"
Dec  2 09:49:09.794: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.285393ms
Dec  2 09:49:11.804: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016452367s
Dec  2 09:49:13.813: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025265289s
Dec  2 09:49:15.821: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033502845s
Dec  2 09:49:17.833: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045272788s
Dec  2 09:49:19.843: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.055706731s
STEP: Saw pod success
Dec  2 09:49:19.843: INFO: Pod "downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c" satisfied condition "Succeeded or Failed"
Dec  2 09:49:19.852: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c container dapi-container: <nil>
STEP: delete the pod
Dec  2 09:49:19.907: INFO: Waiting for pod downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c to disappear
Dec  2 09:49:19.915: INFO: Pod downward-api-a2765848-042c-44cd-8b9d-dfcc11cf185c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:49:19.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-867" for this suite.

• [SLOW TEST:11.246 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:49:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename taint-multiple-pods
W1202 09:49:20.924780      23 warnings.go:70] No static IP address has been configured for the namespace "taint-multiple-pods-148", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec  2 09:49:21.120: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 09:50:21.285: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:50:21.296: INFO: Starting informer...
STEP: Starting pods...
Dec  2 09:50:21.537: INFO: Pod1 is running on 5e47d0ea-e90b-466b-b6de-2748d512ebf3. Tainting Node
Dec  2 09:50:25.787: INFO: Pod2 is running on 5e47d0ea-e90b-466b-b6de-2748d512ebf3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec  2 09:50:51.178: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec  2 09:50:59.166: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:50:59.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-148" for this suite.

• [SLOW TEST:99.318 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":95,"skipped":1587,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:50:59.690: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption
W1202 09:51:00.235459      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-6682", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  2 09:51:00.475: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 09:52:00.707: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Dec  2 09:52:00.773: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  2 09:52:00.782: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  2 09:52:00.829: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  2 09:52:00.852: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec  2 09:52:00.889: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec  2 09:52:00.914: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:53:09.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6682" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:130.067 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":96,"skipped":1603,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:53:09.763: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-runtime
W1202 09:53:10.238129      23 warnings.go:70] No static IP address has been configured for the namespace "container-runtime-4988", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  2 09:53:17.574: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:53:17.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4988" for this suite.

• [SLOW TEST:8.369 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1611,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:53:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 09:53:19.025143      23 warnings.go:70] No static IP address has been configured for the namespace "pods-1584", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  2 09:53:19.249: INFO: The status of Pod pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:53:21.260: INFO: The status of Pod pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:53:23.258: INFO: The status of Pod pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:53:25.259: INFO: The status of Pod pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  2 09:53:25.801: INFO: Successfully updated pod "pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42"
Dec  2 09:53:25.801: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42" in namespace "pods-1584" to be "terminated due to deadline exceeded"
Dec  2 09:53:25.814: INFO: Pod "pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42": Phase="Running", Reason="", readiness=true. Elapsed: 13.181655ms
Dec  2 09:53:27.824: INFO: Pod "pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.023268041s
Dec  2 09:53:27.824: INFO: Pod "pod-update-activedeadlineseconds-287f1e0f-be8e-44a5-b999-df363b337b42" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:53:27.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1584" for this suite.

• [SLOW TEST:10.301 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1613,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:53:28.433: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 09:53:28.921796      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-8632", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 09:53:29.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25" in namespace "downward-api-8632" to be "Succeeded or Failed"
Dec  2 09:53:29.153: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25": Phase="Pending", Reason="", readiness=false. Elapsed: 30.452589ms
Dec  2 09:53:31.164: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041614252s
Dec  2 09:53:33.173: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050890755s
Dec  2 09:53:35.190: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067654207s
Dec  2 09:53:37.201: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.078159701s
STEP: Saw pod success
Dec  2 09:53:37.201: INFO: Pod "downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25" satisfied condition "Succeeded or Failed"
Dec  2 09:53:37.210: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25 container client-container: <nil>
STEP: delete the pod
Dec  2 09:53:37.276: INFO: Waiting for pod downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25 to disappear
Dec  2 09:53:37.284: INFO: Pod downwardapi-volume-2e60dccd-8b82-4475-9ea7-4e1184f35c25 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:53:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8632" for this suite.

• [SLOW TEST:9.432 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1614,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:53:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 09:53:38.357781      23 warnings.go:70] No static IP address has been configured for the namespace "dns-7914", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7914.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 5.242.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.242.5_udp@PTR;check="$$(dig +tcp +noall +answer +search 5.242.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.242.5_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7914.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7914.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7914.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7914.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 5.242.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.242.5_udp@PTR;check="$$(dig +tcp +noall +answer +search 5.242.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.242.5_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 09:53:45.751: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.762: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.853: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.880: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:45.935: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:53:50.951: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:50.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:50.977: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:51.043: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:51.052: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:51.130: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:53:55.948: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:55.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:55.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:56.053: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:56.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:53:56.156: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:54:00.948: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:00.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:00.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:01.044: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:01.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:01.128: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:54:05.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:05.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:06.055: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:06.066: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:06.154: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:54:10.950: INFO: Unable to read wheezy_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:10.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:11.039: INFO: Unable to read jessie_udp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:11.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-7914.svc.cluster.local from pod dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c: the server could not find the requested resource (get pods dns-test-f492be04-491b-4fd5-b510-c173b9176f3c)
Dec  2 09:54:11.159: INFO: Lookups using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c failed for: [wheezy_udp@dns-test-service.dns-7914.svc.cluster.local wheezy_tcp@dns-test-service.dns-7914.svc.cluster.local jessie_udp@dns-test-service.dns-7914.svc.cluster.local jessie_tcp@dns-test-service.dns-7914.svc.cluster.local]

Dec  2 09:54:16.133: INFO: DNS probes using dns-7914/dns-test-f492be04-491b-4fd5-b510-c173b9176f3c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:54:17.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7914" for this suite.

• [SLOW TEST:40.070 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":100,"skipped":1618,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:54:17.936: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 09:54:18.435993      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-891", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-891
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:54:18.691: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  2 09:54:23.698: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Dec  2 09:54:23.717: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Dec  2 09:54:23.728: INFO: observed ReplicaSet test-rs in namespace replicaset-891 with ReadyReplicas 1, AvailableReplicas 1
Dec  2 09:54:23.774: INFO: observed ReplicaSet test-rs in namespace replicaset-891 with ReadyReplicas 1, AvailableReplicas 1
Dec  2 09:54:23.819: INFO: observed ReplicaSet test-rs in namespace replicaset-891 with ReadyReplicas 1, AvailableReplicas 1
Dec  2 09:54:23.830: INFO: observed ReplicaSet test-rs in namespace replicaset-891 with ReadyReplicas 1, AvailableReplicas 1
Dec  2 09:54:27.307: INFO: observed ReplicaSet test-rs in namespace replicaset-891 with ReadyReplicas 2, AvailableReplicas 2
Dec  2 09:54:41.109: INFO: observed Replicaset test-rs in namespace replicaset-891 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:54:41.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-891" for this suite.

• [SLOW TEST:23.713 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":101,"skipped":1633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:54:41.651: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 09:54:42.119626      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-9783", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-08bf1203-0694-40c0-96b1-2bdd68179975
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:54:42.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9783" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":102,"skipped":1657,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:54:42.800: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:54:43.274491      23 warnings.go:70] No static IP address has been configured for the namespace "projected-6096", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-361ae209-b128-4bc8-ba79-805728d238ca
STEP: Creating a pod to test consume secrets
Dec  2 09:54:43.506: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818" in namespace "projected-6096" to be "Succeeded or Failed"
Dec  2 09:54:43.524: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Pending", Reason="", readiness=false. Elapsed: 18.366231ms
Dec  2 09:54:45.531: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02534682s
Dec  2 09:54:47.544: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037895421s
Dec  2 09:54:49.555: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04870398s
Dec  2 09:54:51.566: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060537113s
Dec  2 09:54:53.579: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.07310244s
STEP: Saw pod success
Dec  2 09:54:53.579: INFO: Pod "pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818" satisfied condition "Succeeded or Failed"
Dec  2 09:54:53.590: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818 container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 09:54:53.631: INFO: Waiting for pod pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818 to disappear
Dec  2 09:54:53.639: INFO: Pod pod-projected-secrets-0a2278a8-c61a-409d-ac3f-d46404d90818 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:54:53.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6096" for this suite.

• [SLOW TEST:11.298 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":1658,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:54:54.103: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 09:54:54.601440      23 warnings.go:70] No static IP address has been configured for the namespace "services-5879", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Dec  2 09:54:55.378: INFO: Found Service test-service-7tsfs in namespace services-5879 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec  2 09:54:55.378: INFO: Service test-service-7tsfs created
STEP: Getting /status
Dec  2 09:54:55.387: INFO: Service test-service-7tsfs has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Dec  2 09:54:55.398: INFO: observed Service test-service-7tsfs in namespace services-5879 with annotations: map[] & LoadBalancer: {[]}
Dec  2 09:54:55.398: INFO: Found Service test-service-7tsfs in namespace services-5879 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec  2 09:54:55.398: INFO: Service test-service-7tsfs has service status patched
STEP: updating the ServiceStatus
Dec  2 09:54:55.416: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Dec  2 09:54:55.419: INFO: Observed Service test-service-7tsfs in namespace services-5879 with annotations: map[] & Conditions: {[]}
Dec  2 09:54:55.419: INFO: Observed event: &Service{ObjectMeta:{test-service-7tsfs  services-5879  f30b9f62-8ebd-4fec-8593-b419e9591cfa 52176791 0 2022-12-02 09:54:55 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-12-02 09:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-02 09:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.227.67,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.227.67],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec  2 09:54:55.420: INFO: Found Service test-service-7tsfs in namespace services-5879 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  2 09:54:55.420: INFO: Service test-service-7tsfs has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Dec  2 09:54:55.941: INFO: observed Service test-service-7tsfs in namespace services-5879 with labels: map[test-service-static:true]
Dec  2 09:54:55.942: INFO: observed Service test-service-7tsfs in namespace services-5879 with labels: map[test-service-static:true]
Dec  2 09:54:55.942: INFO: observed Service test-service-7tsfs in namespace services-5879 with labels: map[test-service-static:true]
Dec  2 09:54:55.942: INFO: Found Service test-service-7tsfs in namespace services-5879 with labels: map[test-service:patched test-service-static:true]
Dec  2 09:54:55.942: INFO: Service test-service-7tsfs patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Dec  2 09:54:56.511: INFO: Observed event: ADDED
Dec  2 09:54:56.511: INFO: Observed event: MODIFIED
Dec  2 09:54:56.511: INFO: Observed event: MODIFIED
Dec  2 09:54:56.511: INFO: Observed event: MODIFIED
Dec  2 09:54:56.511: INFO: Found Service test-service-7tsfs in namespace services-5879 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec  2 09:54:56.511: INFO: Service test-service-7tsfs deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:54:56.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5879" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":104,"skipped":1662,"failed":0}
SSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:54:57.029: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption
W1202 09:54:57.606515      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-4314", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Dec  2 09:54:59.865: INFO: running pods: 0 < 1
Dec  2 09:55:01.872: INFO: running pods: 0 < 1
Dec  2 09:55:03.873: INFO: running pods: 0 < 1
Dec  2 09:55:05.879: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:55:07.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4314" for this suite.

• [SLOW TEST:11.422 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":105,"skipped":1667,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:55:08.456: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename prestop
W1202 09:55:08.983349      23 warnings.go:70] No static IP address has been configured for the namespace "prestop-5793", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-5793
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5793
STEP: Deleting pre-stop pod
Dec  2 09:55:36.301: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:55:36.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5793" for this suite.

• [SLOW TEST:28.423 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":106,"skipped":1687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:55:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir-wrapper
W1202 09:55:37.384578      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-wrapper-1581", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:55:37.624: INFO: The status of Pod pod-secrets-16d80be5-788d-4538-b35a-b477fd5e28d8 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:55:39.633: INFO: The status of Pod pod-secrets-16d80be5-788d-4538-b35a-b477fd5e28d8 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:55:41.654: INFO: The status of Pod pod-secrets-16d80be5-788d-4538-b35a-b477fd5e28d8 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:55:43.631: INFO: The status of Pod pod-secrets-16d80be5-788d-4538-b35a-b477fd5e28d8 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:55:45.636: INFO: The status of Pod pod-secrets-16d80be5-788d-4538-b35a-b477fd5e28d8 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:55:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1581" for this suite.

• [SLOW TEST:9.299 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":107,"skipped":1717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:55:46.180: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename endpointslice
W1202 09:55:46.684933      23 warnings.go:70] No static IP address has been configured for the namespace "endpointslice-8922", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:55:47.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8922" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":108,"skipped":1740,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:55:48.470: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename proxy
W1202 09:55:48.944125      23 warnings.go:70] No static IP address has been configured for the namespace "proxy-1705", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1705
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:55:49.125: INFO: Creating pod...
Dec  2 09:55:49.154: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:50.164: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:51.161: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:52.167: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:53.166: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:54.161: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:55.165: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:56.167: INFO: Pod Quantity: 1 Status: Pending
Dec  2 09:55:57.164: INFO: Pod Status: Running
Dec  2 09:55:57.164: INFO: Creating service...
Dec  2 09:55:57.688: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/DELETE
Dec  2 09:55:57.701: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  2 09:55:57.701: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/GET
Dec  2 09:55:57.710: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  2 09:55:57.710: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/HEAD
Dec  2 09:55:57.719: INFO: http.Client request:HEAD | StatusCode:200
Dec  2 09:55:57.719: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/OPTIONS
Dec  2 09:55:57.725: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  2 09:55:57.725: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/PATCH
Dec  2 09:55:57.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  2 09:55:57.735: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/POST
Dec  2 09:55:57.743: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  2 09:55:57.743: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/pods/agnhost/proxy/some/path/with/PUT
Dec  2 09:55:57.749: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec  2 09:55:57.749: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/DELETE
Dec  2 09:55:57.765: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  2 09:55:57.765: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/GET
Dec  2 09:55:57.780: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  2 09:55:57.780: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/HEAD
Dec  2 09:55:57.790: INFO: http.Client request:HEAD | StatusCode:200
Dec  2 09:55:57.790: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/OPTIONS
Dec  2 09:55:57.808: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  2 09:55:57.808: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/PATCH
Dec  2 09:55:57.823: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  2 09:55:57.823: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/POST
Dec  2 09:55:57.833: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  2 09:55:57.833: INFO: Starting http.Client for https://10.100.192.1:443/api/v1/namespaces/proxy-1705/services/test-service/proxy/some/path/with/PUT
Dec  2 09:55:57.850: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:55:57.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1705" for this suite.

• [SLOW TEST:9.860 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":109,"skipped":1741,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:55:58.332: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 09:55:58.832419      23 warnings.go:70] No static IP address has been configured for the namespace "projected-739", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  2 09:55:59.030: INFO: The status of Pod annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:56:01.044: INFO: The status of Pod annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:56:03.042: INFO: The status of Pod annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:56:05.044: INFO: The status of Pod annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 09:56:07.048: INFO: The status of Pod annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f is Running (Ready = true)
Dec  2 09:56:07.608: INFO: Successfully updated pod "annotationupdate02abc169-ac8c-4223-bd4c-6244c0c0e63f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:56:09.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-739" for this suite.

• [SLOW TEST:11.844 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":1750,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:56:10.180: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 09:56:10.672501      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-8986", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Dec  2 09:56:11.437: INFO: created pod pod-service-account-defaultsa
Dec  2 09:56:11.437: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  2 09:56:11.455: INFO: created pod pod-service-account-mountsa
Dec  2 09:56:11.455: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  2 09:56:11.474: INFO: created pod pod-service-account-nomountsa
Dec  2 09:56:11.474: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  2 09:56:11.496: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  2 09:56:11.496: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  2 09:56:11.525: INFO: created pod pod-service-account-mountsa-mountspec
Dec  2 09:56:11.525: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  2 09:56:11.548: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  2 09:56:11.548: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  2 09:56:11.570: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  2 09:56:11.571: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  2 09:56:11.600: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  2 09:56:11.600: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  2 09:56:11.617: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  2 09:56:11.620: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:56:11.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8986" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":111,"skipped":1753,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:56:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption
W1202 09:56:12.606514      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-595", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  2 09:56:12.854: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 09:57:13.029: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Dec  2 09:57:13.081: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  2 09:57:13.095: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  2 09:57:13.147: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  2 09:57:13.173: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec  2 09:57:13.207: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec  2 09:57:13.232: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 09:58:11.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-595" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:119.947 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":112,"skipped":1754,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 09:58:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 09:58:12.504183      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-5815", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 09:58:13.138096      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-5815", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 09:58:13.189: INFO: Create a RollingUpdate DaemonSet
Dec  2 09:58:13.203: INFO: Check that daemon pods launch on every node of the cluster
Dec  2 09:58:13.220: INFO: Number of nodes with available pods: 0
Dec  2 09:58:13.220: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:58:14.257: INFO: Number of nodes with available pods: 0
Dec  2 09:58:14.257: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:58:15.252: INFO: Number of nodes with available pods: 0
Dec  2 09:58:15.252: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 09:58:16.247: INFO: Number of nodes with available pods: 1
Dec  2 09:58:16.247: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:17.250: INFO: Number of nodes with available pods: 1
Dec  2 09:58:17.250: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:18.247: INFO: Number of nodes with available pods: 1
Dec  2 09:58:18.247: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:19.241: INFO: Number of nodes with available pods: 1
Dec  2 09:58:19.241: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:20.243: INFO: Number of nodes with available pods: 1
Dec  2 09:58:20.243: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:21.250: INFO: Number of nodes with available pods: 1
Dec  2 09:58:21.250: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:22.239: INFO: Number of nodes with available pods: 1
Dec  2 09:58:22.239: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:23.247: INFO: Number of nodes with available pods: 1
Dec  2 09:58:23.247: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:24.250: INFO: Number of nodes with available pods: 1
Dec  2 09:58:24.250: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:25.239: INFO: Number of nodes with available pods: 1
Dec  2 09:58:25.239: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:26.254: INFO: Number of nodes with available pods: 1
Dec  2 09:58:26.254: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:27.243: INFO: Number of nodes with available pods: 1
Dec  2 09:58:27.243: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:28.242: INFO: Number of nodes with available pods: 1
Dec  2 09:58:28.242: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:29.243: INFO: Number of nodes with available pods: 1
Dec  2 09:58:29.243: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:30.255: INFO: Number of nodes with available pods: 1
Dec  2 09:58:30.255: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:31.241: INFO: Number of nodes with available pods: 1
Dec  2 09:58:31.241: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:32.249: INFO: Number of nodes with available pods: 1
Dec  2 09:58:32.249: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:33.253: INFO: Number of nodes with available pods: 1
Dec  2 09:58:33.253: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:34.247: INFO: Number of nodes with available pods: 1
Dec  2 09:58:34.247: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:35.250: INFO: Number of nodes with available pods: 1
Dec  2 09:58:35.250: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:36.250: INFO: Number of nodes with available pods: 1
Dec  2 09:58:36.250: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 09:58:37.245: INFO: Number of nodes with available pods: 2
Dec  2 09:58:37.245: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 09:58:38.246: INFO: Number of nodes with available pods: 3
Dec  2 09:58:38.246: INFO: Number of running nodes: 3, number of available pods: 3
Dec  2 09:58:38.246: INFO: Update the DaemonSet to trigger a rollout
Dec  2 09:58:38.266: INFO: Updating DaemonSet daemon-set
Dec  2 09:58:51.319: INFO: Roll back the DaemonSet before rollout is complete
Dec  2 09:58:51.340: INFO: Updating DaemonSet daemon-set
Dec  2 09:58:51.341: INFO: Make sure DaemonSet rollback is complete
Dec  2 09:58:51.350: INFO: Wrong image for pod: daemon-set-4zhvq. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Dec  2 09:58:51.350: INFO: Pod daemon-set-4zhvq is not available
Dec  2 09:59:42.370: INFO: Pod daemon-set-sb745 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5815, will wait for the garbage collector to delete the pods
Dec  2 09:59:42.492: INFO: Deleting DaemonSet.extensions daemon-set took: 14.242658ms
Dec  2 09:59:42.593: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.855651ms
Dec  2 10:00:18.005: INFO: Number of nodes with available pods: 0
Dec  2 10:00:18.005: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 10:00:18.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52179267"},"items":null}

Dec  2 10:00:18.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52179267"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:00:18.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5815" for this suite.

• [SLOW TEST:126.654 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":113,"skipped":1763,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:00:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-runtime
W1202 10:00:19.256558      23 warnings.go:70] No static IP address has been configured for the namespace "container-runtime-6560", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  2 10:00:28.576: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:00:28.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6560" for this suite.

• [SLOW TEST:10.640 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":1773,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:00:29.356: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 10:00:29.957555      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-2932", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Dec  2 10:00:30.142: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-2932 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:00:30.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2932" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":115,"skipped":1794,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:00:30.832: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:00:31.370987      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-8458", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:00:32.321580      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-8458", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:00:32.864266      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-8458-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:00:33.272: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 10:00:35.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:00:37.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572034, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:00:40.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:00:40.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8458" for this suite.
STEP: Destroying namespace "webhook-8458-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.052 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":116,"skipped":1814,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:00:42.885: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:00:43.376383      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-5652", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5652
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-170223e7-0af1-4408-86d3-f2659ec85f9e
STEP: Creating configMap with name cm-test-opt-upd-69211717-b8b2-42a4-8121-52bf7a7824d8
STEP: Creating the pod
Dec  2 10:00:43.650: INFO: The status of Pod pod-configmaps-75c16205-6a33-4ae3-92ec-d9364008da68 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:00:45.662: INFO: The status of Pod pod-configmaps-75c16205-6a33-4ae3-92ec-d9364008da68 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:00:47.660: INFO: The status of Pod pod-configmaps-75c16205-6a33-4ae3-92ec-d9364008da68 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:00:49.662: INFO: The status of Pod pod-configmaps-75c16205-6a33-4ae3-92ec-d9364008da68 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:00:51.663: INFO: The status of Pod pod-configmaps-75c16205-6a33-4ae3-92ec-d9364008da68 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-170223e7-0af1-4408-86d3-f2659ec85f9e
STEP: Updating configmap cm-test-opt-upd-69211717-b8b2-42a4-8121-52bf7a7824d8
STEP: Creating configMap with name cm-test-opt-create-41f12f07-0d46-4945-b6ae-7b423c03cb0e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:00:55.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5652" for this suite.

• [SLOW TEST:13.532 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":1818,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:00:56.417: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pod-network-test
W1202 10:00:57.248774      23 warnings.go:70] No static IP address has been configured for the namespace "pod-network-test-2244", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2244
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  2 10:00:57.463: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  2 10:00:57.572: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:00:59.581: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:01.582: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:03.583: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:05.586: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:07.584: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:09.585: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:11.584: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:13.580: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:15.586: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:17.587: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:01:19.586: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  2 10:01:19.605: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:01:21.618: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:01:23.619: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:01:25.616: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:01:27.615: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:01:29.618: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  2 10:01:29.635: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec  2 10:01:31.644: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec  2 10:01:33.642: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  2 10:01:37.744: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  2 10:01:37.744: INFO: Going to poll 11.34.26.2 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:01:37.757: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.34.26.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2244 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:01:37.757: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:01:38.863: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  2 10:01:38.863: INFO: Going to poll 11.34.26.4 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:01:38.873: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.34.26.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2244 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:01:38.873: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:01:39.983: INFO: Found all 1 expected endpoints: [netserver-1]
Dec  2 10:01:39.983: INFO: Going to poll 11.34.26.3 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:01:39.995: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 11.34.26.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2244 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:01:39.995: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:01:41.097: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:01:41.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2244" for this suite.

• [SLOW TEST:45.232 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":1823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:01:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:01:42.149711      23 warnings.go:70] No static IP address has been configured for the namespace "projected-5482", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9135a1ea-a0fd-46e0-8c11-c86508299024
STEP: Creating a pod to test consume configMaps
Dec  2 10:01:42.388: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3" in namespace "projected-5482" to be "Succeeded or Failed"
Dec  2 10:01:42.397: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.308111ms
Dec  2 10:01:44.407: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018917865s
Dec  2 10:01:46.420: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031658171s
Dec  2 10:01:48.432: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043934862s
Dec  2 10:01:50.446: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.057999704s
STEP: Saw pod success
Dec  2 10:01:50.446: INFO: Pod "pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3" satisfied condition "Succeeded or Failed"
Dec  2 10:01:50.454: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:01:50.511: INFO: Waiting for pod pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3 to disappear
Dec  2 10:01:50.518: INFO: Pod pod-projected-configmaps-01e0a4d8-708d-4769-95ee-e8ee3e55ada3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:01:50.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5482" for this suite.

• [SLOW TEST:9.365 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":1857,"failed":0}
SSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:01:51.023: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename hostport
W1202 10:01:51.563051      23 warnings.go:70] No static IP address has been configured for the namespace "hostport-9679", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-9679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Dec  2 10:01:51.782: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:53.790: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:55.795: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:57.795: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:01:59.795: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:01.795: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:03.792: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:05.799: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:07.797: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:09.795: INFO: The status of Pod pod1 is Running (Ready = false)
Dec  2 10:02:11.799: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 11.0.95.5 on the node which pod1 resides and expect scheduled
Dec  2 10:02:11.822: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:13.833: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:15.832: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:17.832: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:19.834: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:21.835: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:23.831: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:25.839: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:27.835: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:29.834: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:31.837: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:33.834: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:35.837: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:37.833: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:39.835: INFO: The status of Pod pod2 is Running (Ready = false)
Dec  2 10:02:41.834: INFO: The status of Pod pod2 is Running (Ready = false)
Dec  2 10:02:43.838: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 11.0.95.5 but use UDP protocol on the node which pod2 resides
Dec  2 10:02:43.868: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:45.880: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:47.876: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:49.879: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:51.878: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:53.878: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:55.878: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:57.882: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:02:59.881: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:01.880: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:03.880: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:05.904: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:07.882: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:09.880: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:11.879: INFO: The status of Pod pod3 is Running (Ready = false)
Dec  2 10:03:13.880: INFO: The status of Pod pod3 is Running (Ready = true)
Dec  2 10:03:13.911: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:15.923: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:17.923: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:19.922: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:21.919: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:23.922: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:25.919: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:27.922: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:29.923: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:31.922: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:03:33.923: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Dec  2 10:03:33.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 11.0.95.5 http://127.0.0.1:54323/hostname] Namespace:hostport-9679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:03:33.929: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: checking connectivity from pod e2e-host-exec to serverIP: 11.0.95.5, port: 54323
Dec  2 10:03:34.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://11.0.95.5:54323/hostname] Namespace:hostport-9679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:03:34.666: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: checking connectivity from pod e2e-host-exec to serverIP: 11.0.95.5, port: 54323 UDP
Dec  2 10:03:34.857: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 11.0.95.5 54323] Namespace:hostport-9679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:03:34.857: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:03:39.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9679" for this suite.

• [SLOW TEST:109.529 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":120,"skipped":1860,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:03:40.553: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 10:03:41.032759      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-3846", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  2 10:03:41.244: INFO: Waiting up to 5m0s for pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d" in namespace "emptydir-3846" to be "Succeeded or Failed"
Dec  2 10:03:41.255: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.068181ms
Dec  2 10:03:43.264: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019899251s
Dec  2 10:03:45.277: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033309407s
Dec  2 10:03:47.290: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046269034s
Dec  2 10:03:49.302: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058720257s
STEP: Saw pod success
Dec  2 10:03:49.303: INFO: Pod "pod-394deea7-86f0-45e0-8e58-1588840cfe0d" satisfied condition "Succeeded or Failed"
Dec  2 10:03:49.310: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-394deea7-86f0-45e0-8e58-1588840cfe0d container test-container: <nil>
STEP: delete the pod
Dec  2 10:03:49.377: INFO: Waiting for pod pod-394deea7-86f0-45e0-8e58-1588840cfe0d to disappear
Dec  2 10:03:49.383: INFO: Pod pod-394deea7-86f0-45e0-8e58-1588840cfe0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:03:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3846" for this suite.

• [SLOW TEST:9.332 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":1870,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:03:49.886: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 10:03:50.488909      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-713", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
W1202 10:03:51.153644      23 warnings.go:70] No static IP address has been configured for the namespace "secret-namespace-1364", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1364
STEP: Creating secret with name secret-test-038541ec-80a4-45d7-9ced-6710b69986cb
STEP: Creating a pod to test consume secrets
Dec  2 10:03:51.359: INFO: Waiting up to 5m0s for pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf" in namespace "secrets-713" to be "Succeeded or Failed"
Dec  2 10:03:51.367: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101383ms
Dec  2 10:03:53.379: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019724307s
Dec  2 10:03:55.391: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032343193s
Dec  2 10:03:57.406: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047335287s
Dec  2 10:03:59.418: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059546717s
STEP: Saw pod success
Dec  2 10:03:59.419: INFO: Pod "pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf" satisfied condition "Succeeded or Failed"
Dec  2 10:03:59.426: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:03:59.487: INFO: Waiting for pod pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf to disappear
Dec  2 10:03:59.494: INFO: Pod pod-secrets-18d357ad-248f-4571-b230-b52c18d90dcf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:03:59.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-713" for this suite.
STEP: Destroying namespace "secret-namespace-1364" for this suite.

• [SLOW TEST:10.879 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":1885,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:04:00.766: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 10:04:01.356259      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-4065", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:04:01.548: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  2 10:04:06.563: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  2 10:04:08.587: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  2 10:04:10.599: INFO: Creating deployment "test-rollover-deployment"
Dec  2 10:04:10.621: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  2 10:04:12.645: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  2 10:04:12.662: INFO: Ensure that both replica sets have 1 created replica
Dec  2 10:04:12.680: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  2 10:04:12.699: INFO: Updating deployment test-rollover-deployment
Dec  2 10:04:12.699: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  2 10:04:14.728: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  2 10:04:14.745: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  2 10:04:14.761: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:14.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572253, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:16.779: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:16.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572253, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:18.782: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:18.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572258, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:20.777: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:20.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572258, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:22.783: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:22.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572258, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:24.782: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:24.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572258, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:26.778: INFO: all replica sets need to contain the pod-template-hash label
Dec  2 10:04:26.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572258, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572251, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-67bbb7bc8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:04:28.785: INFO: 
Dec  2 10:04:28.785: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 10:04:28.809: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4065  bf5a53b3-9b2f-4fd3-b6cb-07d612148231 52181338 2 2022-12-02 10:04:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-02 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4c098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-02 10:04:11 +0000 UTC,LastTransitionTime:2022-12-02 10:04:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-67bbb7bc8" has successfully progressed.,LastUpdateTime:2022-12-02 10:04:28 +0000 UTC,LastTransitionTime:2022-12-02 10:04:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  2 10:04:28.819: INFO: New ReplicaSet "test-rollover-deployment-67bbb7bc8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-67bbb7bc8  deployment-4065  1c4fb73e-3655-4ed0-a961-8aa923be90e1 52181327 2 2022-12-02 10:04:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:67bbb7bc8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bf5a53b3-9b2f-4fd3-b6cb-07d612148231 0xc003b4c560 0xc003b4c561}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf5a53b3-9b2f-4fd3-b6cb-07d612148231\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:04:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 67bbb7bc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:67bbb7bc8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4c5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:04:28.819: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  2 10:04:28.820: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4065  d346b3eb-8043-4004-9711-69f598980b30 52181337 2 2022-12-02 10:04:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bf5a53b3-9b2f-4fd3-b6cb-07d612148231 0xc003b4c437 0xc003b4c438}] []  [{e2e.test Update apps/v1 2022-12-02 10:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf5a53b3-9b2f-4fd3-b6cb-07d612148231\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:04:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b4c4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:04:28.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4065  5f897325-0fa8-4806-8bb8-76632e9666ed 52181244 2 2022-12-02 10:04:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bf5a53b3-9b2f-4fd3-b6cb-07d612148231 0xc003b4c667 0xc003b4c668}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf5a53b3-9b2f-4fd3-b6cb-07d612148231\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:04:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4c718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:04:28.832: INFO: Pod "test-rollover-deployment-67bbb7bc8-bzv4z" is available:
&Pod{ObjectMeta:{test-rollover-deployment-67bbb7bc8-bzv4z test-rollover-deployment-67bbb7bc8- deployment-4065  3a44b42a-a920-47f6-9470-89792d0fc99c 52181271 0 2022-12-02 10:04:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:67bbb7bc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-67bbb7bc8 1c4fb73e-3655-4ed0-a961-8aa923be90e1 0xc000b6d310 0xc000b6d311}] []  [{kube-controller-manager Update v1 2022-12-02 10:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c4fb73e-3655-4ed0-a961-8aa923be90e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:04:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5cggz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5cggz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.4,StartTime:2022-12-02 10:04:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:04:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:docker://sha256:a05bd3a9140b72c5a17eb6881a75c5003b270c0b3e32a995fb10ec96004279d2,ContainerID:docker://9035d878067d759bc8307d8d5df4b66300bf17130d88847ba0db48eb03a08e9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:04:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4065" for this suite.

• [SLOW TEST:28.565 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":123,"skipped":1899,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:04:29.333: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 10:04:29.848659      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-7456", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 10:04:30.714684      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-7456", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  2 10:04:30.796: INFO: Number of nodes with available pods: 0
Dec  2 10:04:30.796: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:04:31.823: INFO: Number of nodes with available pods: 0
Dec  2 10:04:31.823: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:04:32.825: INFO: Number of nodes with available pods: 0
Dec  2 10:04:32.825: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:04:33.822: INFO: Number of nodes with available pods: 0
Dec  2 10:04:33.822: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:04:34.827: INFO: Number of nodes with available pods: 1
Dec  2 10:04:34.827: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:35.828: INFO: Number of nodes with available pods: 1
Dec  2 10:04:35.828: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:36.823: INFO: Number of nodes with available pods: 1
Dec  2 10:04:36.823: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:37.828: INFO: Number of nodes with available pods: 1
Dec  2 10:04:37.828: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:38.823: INFO: Number of nodes with available pods: 1
Dec  2 10:04:38.823: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:39.820: INFO: Number of nodes with available pods: 1
Dec  2 10:04:39.820: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:40.827: INFO: Number of nodes with available pods: 1
Dec  2 10:04:40.827: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:41.828: INFO: Number of nodes with available pods: 1
Dec  2 10:04:41.829: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:42.824: INFO: Number of nodes with available pods: 1
Dec  2 10:04:42.824: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:43.823: INFO: Number of nodes with available pods: 1
Dec  2 10:04:43.823: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:44.826: INFO: Number of nodes with available pods: 1
Dec  2 10:04:44.826: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:45.821: INFO: Number of nodes with available pods: 1
Dec  2 10:04:45.821: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:04:46.826: INFO: Number of nodes with available pods: 2
Dec  2 10:04:46.826: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:47.821: INFO: Number of nodes with available pods: 2
Dec  2 10:04:47.821: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:48.819: INFO: Number of nodes with available pods: 2
Dec  2 10:04:48.819: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:49.826: INFO: Number of nodes with available pods: 2
Dec  2 10:04:49.826: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:50.822: INFO: Number of nodes with available pods: 2
Dec  2 10:04:50.822: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:51.822: INFO: Number of nodes with available pods: 2
Dec  2 10:04:51.822: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:52.829: INFO: Number of nodes with available pods: 2
Dec  2 10:04:52.829: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:53.821: INFO: Number of nodes with available pods: 2
Dec  2 10:04:53.821: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:54.819: INFO: Number of nodes with available pods: 2
Dec  2 10:04:54.819: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:55.821: INFO: Number of nodes with available pods: 2
Dec  2 10:04:55.821: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:56.826: INFO: Number of nodes with available pods: 2
Dec  2 10:04:56.826: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:57.815: INFO: Number of nodes with available pods: 2
Dec  2 10:04:57.816: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 10:04:58.824: INFO: Number of nodes with available pods: 3
Dec  2 10:04:58.824: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  2 10:04:58.879: INFO: Number of nodes with available pods: 2
Dec  2 10:04:58.879: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:04:59.901: INFO: Number of nodes with available pods: 2
Dec  2 10:04:59.901: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:05:00.911: INFO: Number of nodes with available pods: 2
Dec  2 10:05:00.912: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:05:01.910: INFO: Number of nodes with available pods: 2
Dec  2 10:05:01.910: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:05:02.907: INFO: Number of nodes with available pods: 2
Dec  2 10:05:02.907: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:05:03.909: INFO: Number of nodes with available pods: 3
Dec  2 10:05:03.909: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7456, will wait for the garbage collector to delete the pods
Dec  2 10:05:04.003: INFO: Deleting DaemonSet.extensions daemon-set took: 17.248895ms
Dec  2 10:05:04.105: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.417853ms
Dec  2 10:05:14.828: INFO: Number of nodes with available pods: 0
Dec  2 10:05:14.828: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 10:05:14.834: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52181701"},"items":null}

Dec  2 10:05:14.844: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52181701"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:14.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7456" for this suite.

• [SLOW TEST:46.155 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":124,"skipped":1902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:15.492: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename cronjob
W1202 10:05:16.010924      23 warnings.go:70] No static IP address has been configured for the namespace "cronjob-8903", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  2 10:05:16.240: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  2 10:05:16.252: INFO: starting watch
STEP: patching
STEP: updating
Dec  2 10:05:16.293: INFO: waiting for watch events with expected annotations
Dec  2 10:05:16.293: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8903" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":125,"skipped":1926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:16.918: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:05:17.481494      23 warnings.go:70] No static IP address has been configured for the namespace "projected-8309", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:05:17.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d" in namespace "projected-8309" to be "Succeeded or Failed"
Dec  2 10:05:17.705: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.582914ms
Dec  2 10:05:19.714: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020002407s
Dec  2 10:05:21.723: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029371751s
Dec  2 10:05:23.732: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038160566s
Dec  2 10:05:25.743: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04963625s
STEP: Saw pod success
Dec  2 10:05:25.744: INFO: Pod "downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d" satisfied condition "Succeeded or Failed"
Dec  2 10:05:25.755: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d container client-container: <nil>
STEP: delete the pod
Dec  2 10:05:25.805: INFO: Waiting for pod downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d to disappear
Dec  2 10:05:25.815: INFO: Pod downwardapi-volume-9d14e36b-43cf-4118-8902-80d7f677066d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:25.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8309" for this suite.

• [SLOW TEST:9.505 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":1988,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:26.424: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:05:27.080578      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-1255", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  2 10:05:27.364: INFO: Waiting up to 5m0s for pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f" in namespace "downward-api-1255" to be "Succeeded or Failed"
Dec  2 10:05:27.379: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.158324ms
Dec  2 10:05:29.387: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022946771s
Dec  2 10:05:31.402: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037620865s
Dec  2 10:05:33.412: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047528016s
Dec  2 10:05:35.424: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059364475s
Dec  2 10:05:37.436: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.071993201s
STEP: Saw pod success
Dec  2 10:05:37.437: INFO: Pod "downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f" satisfied condition "Succeeded or Failed"
Dec  2 10:05:37.448: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f container dapi-container: <nil>
STEP: delete the pod
Dec  2 10:05:37.501: INFO: Waiting for pod downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f to disappear
Dec  2 10:05:37.512: INFO: Pod downward-api-a4846da8-19d4-4757-8f2c-a552b4ebe33f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1255" for this suite.

• [SLOW TEST:11.591 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2002,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:38.016: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:05:38.698565      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-2666", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:05:38.913: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa" in namespace "downward-api-2666" to be "Succeeded or Failed"
Dec  2 10:05:38.923: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.581347ms
Dec  2 10:05:40.933: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020020912s
Dec  2 10:05:42.944: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031043179s
Dec  2 10:05:44.954: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041503679s
Dec  2 10:05:46.964: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050969513s
STEP: Saw pod success
Dec  2 10:05:46.964: INFO: Pod "downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa" satisfied condition "Succeeded or Failed"
Dec  2 10:05:46.970: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa container client-container: <nil>
STEP: delete the pod
Dec  2 10:05:47.025: INFO: Waiting for pod downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa to disappear
Dec  2 10:05:47.034: INFO: Pod downwardapi-volume-01204703-ad11-40a7-aac1-896198261baa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:47.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2666" for this suite.

• [SLOW TEST:9.645 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2021,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:47.669: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 10:05:48.628089      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-4642", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-59e2472c-56b3-4446-a478-554764ec4418
STEP: Creating a pod to test consume secrets
Dec  2 10:05:48.837: INFO: Waiting up to 5m0s for pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e" in namespace "secrets-4642" to be "Succeeded or Failed"
Dec  2 10:05:48.853: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.828199ms
Dec  2 10:05:50.864: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027104167s
Dec  2 10:05:52.878: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040871979s
Dec  2 10:05:54.893: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055448106s
Dec  2 10:05:56.905: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068182486s
STEP: Saw pod success
Dec  2 10:05:56.906: INFO: Pod "pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e" satisfied condition "Succeeded or Failed"
Dec  2 10:05:56.912: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:05:56.974: INFO: Waiting for pod pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e to disappear
Dec  2 10:05:56.979: INFO: Pod pod-secrets-563552ef-5a9e-4975-bfde-82505ccf8a3e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:05:56.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4642" for this suite.

• [SLOW TEST:9.986 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2024,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:05:57.654: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 10:05:58.196009      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-8876", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:06:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8876" for this suite.

• [SLOW TEST:18.339 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":130,"skipped":2025,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:06:15.993: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:06:16.485958      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9335", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:06:17.277659      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9335", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:06:17.777881      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9335-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:06:18.647: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 10:06:20.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:06:22.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572379, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:06:26.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:06:26.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9335" for this suite.
STEP: Destroying namespace "webhook-9335-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":131,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:06:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sysctl
W1202 10:06:28.234471      23 warnings.go:70] No static IP address has been configured for the namespace "sysctl-2159", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-2159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:06:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2159" for this suite.

• [SLOW TEST:9.249 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":132,"skipped":2044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:06:36.993: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:06:37.537644      23 warnings.go:70] No static IP address has been configured for the namespace "services-2522", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2522
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2522
I1202 10:06:38.639562      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2522, replica count: 2
I1202 10:06:41.691494      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:06:44.692277      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:06:44.692: INFO: Creating new exec pod
Dec  2 10:06:51.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-2522 exec execpodxdz9l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  2 10:06:53.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  2 10:06:53.026: INFO: stdout: "externalname-service-vsfgh"
Dec  2 10:06:53.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-2522 exec execpodxdz9l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.230.89 80'
Dec  2 10:06:53.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.230.89 80\nConnection to 10.100.230.89 80 port [tcp/http] succeeded!\n"
Dec  2 10:06:53.217: INFO: stdout: "externalname-service-mvk6h"
Dec  2 10:06:53.217: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:06:53.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2522" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:17.586 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":133,"skipped":2073,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:06:54.579: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 10:06:55.125454      23 warnings.go:70] No static IP address has been configured for the namespace "gc-2414", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  2 10:06:56.441: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1202 10:06:56.441677      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:06:56.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2414" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":134,"skipped":2076,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:06:57.533: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replication-controller
W1202 10:06:58.003972      23 warnings.go:70] No static IP address has been configured for the namespace "replication-controller-7220", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:07:09.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7220" for this suite.

• [SLOW TEST:12.636 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":135,"skipped":2088,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:07:10.173: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename containers
W1202 10:07:10.887092      23 warnings.go:70] No static IP address has been configured for the namespace "containers-9369", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Dec  2 10:07:11.099: INFO: Waiting up to 5m0s for pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398" in namespace "containers-9369" to be "Succeeded or Failed"
Dec  2 10:07:11.107: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398": Phase="Pending", Reason="", readiness=false. Elapsed: 8.177479ms
Dec  2 10:07:13.115: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015323945s
Dec  2 10:07:15.123: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023534977s
Dec  2 10:07:17.130: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031206669s
Dec  2 10:07:19.142: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042868911s
STEP: Saw pod success
Dec  2 10:07:19.142: INFO: Pod "client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398" satisfied condition "Succeeded or Failed"
Dec  2 10:07:19.147: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:07:19.199: INFO: Waiting for pod client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398 to disappear
Dec  2 10:07:19.205: INFO: Pod client-containers-5bffb7e7-1952-40f7-8a8f-46cb97779398 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:07:19.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9369" for this suite.

• [SLOW TEST:9.567 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2103,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:07:19.740: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:07:20.304684      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-4376", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  2 10:07:20.511: INFO: Waiting up to 5m0s for pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e" in namespace "downward-api-4376" to be "Succeeded or Failed"
Dec  2 10:07:20.517: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.934162ms
Dec  2 10:07:22.529: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017814396s
Dec  2 10:07:24.543: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032016058s
Dec  2 10:07:26.554: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042934631s
Dec  2 10:07:28.570: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058361806s
Dec  2 10:07:30.583: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.072035319s
STEP: Saw pod success
Dec  2 10:07:30.584: INFO: Pod "downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e" satisfied condition "Succeeded or Failed"
Dec  2 10:07:30.593: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e container dapi-container: <nil>
STEP: delete the pod
Dec  2 10:07:30.646: INFO: Waiting for pod downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e to disappear
Dec  2 10:07:30.654: INFO: Pod downward-api-9140eaec-cbe9-4ec5-9945-dca1b1e8fd4e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:07:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4376" for this suite.

• [SLOW TEST:11.835 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:07:31.576: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 10:07:32.078998      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-8489", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Dec  2 10:07:32.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 create -f -'
Dec  2 10:07:32.601: INFO: stderr: ""
Dec  2 10:07:32.601: INFO: stdout: "pod/pause created\n"
Dec  2 10:07:32.601: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  2 10:07:32.601: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8489" to be "running and ready"
Dec  2 10:07:32.617: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.574478ms
Dec  2 10:07:34.629: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02818589s
Dec  2 10:07:36.641: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.039533233s
Dec  2 10:07:36.641: INFO: Pod "pause" satisfied condition "running and ready"
Dec  2 10:07:36.641: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  2 10:07:36.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 label pods pause testing-label=testing-label-value'
Dec  2 10:07:36.739: INFO: stderr: ""
Dec  2 10:07:36.739: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  2 10:07:36.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 get pod pause -L testing-label'
Dec  2 10:07:36.833: INFO: stderr: ""
Dec  2 10:07:36.833: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  2 10:07:36.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 label pods pause testing-label-'
Dec  2 10:07:36.942: INFO: stderr: ""
Dec  2 10:07:36.942: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  2 10:07:36.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 get pod pause -L testing-label'
Dec  2 10:07:37.052: INFO: stderr: ""
Dec  2 10:07:37.052: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Dec  2 10:07:37.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 delete --grace-period=0 --force -f -'
Dec  2 10:07:37.172: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 10:07:37.172: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  2 10:07:37.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 get rc,svc -l name=pause --no-headers'
Dec  2 10:07:37.289: INFO: stderr: "No resources found in kubectl-8489 namespace.\n"
Dec  2 10:07:37.289: INFO: stdout: ""
Dec  2 10:07:37.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8489 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  2 10:07:37.383: INFO: stderr: ""
Dec  2 10:07:37.383: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:07:37.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8489" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":138,"skipped":2128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:07:37.879: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replication-controller
W1202 10:07:38.444834      23 warnings.go:70] No static IP address has been configured for the namespace "replication-controller-8417", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  2 10:07:38.654: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  2 10:07:43.664: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:07:43.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8417" for this suite.

• [SLOW TEST:6.408 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":139,"skipped":2209,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:07:44.287: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-lifecycle-hook
W1202 10:07:44.836549      23 warnings.go:70] No static IP address has been configured for the namespace "container-lifecycle-hook-963", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  2 10:07:45.038: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:47.047: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:49.049: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:51.047: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  2 10:07:51.083: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:53.094: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:55.095: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:07:57.096: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec  2 10:07:57.126: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  2 10:07:57.133: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  2 10:07:59.135: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  2 10:07:59.192: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  2 10:08:01.134: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  2 10:08:01.145: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:08:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-963" for this suite.

• [SLOW TEST:17.397 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2216,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:08:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:08:02.138858      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-381", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:08:02.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc" in namespace "downward-api-381" to be "Succeeded or Failed"
Dec  2 10:08:02.368: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.118644ms
Dec  2 10:08:04.380: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02477693s
Dec  2 10:08:06.391: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036094645s
Dec  2 10:08:08.401: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046235562s
Dec  2 10:08:10.411: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.055936176s
Dec  2 10:08:12.424: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.069170401s
STEP: Saw pod success
Dec  2 10:08:12.424: INFO: Pod "downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc" satisfied condition "Succeeded or Failed"
Dec  2 10:08:12.431: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc container client-container: <nil>
STEP: delete the pod
Dec  2 10:08:12.481: INFO: Waiting for pod downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc to disappear
Dec  2 10:08:12.489: INFO: Pod downwardapi-volume-7f13ec11-10d5-4977-957f-cc4ec724e6bc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:08:12.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-381" for this suite.

• [SLOW TEST:11.302 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:08:12.996: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:08:13.619577      23 warnings.go:70] No static IP address has been configured for the namespace "projected-42", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-42
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-a1f6b1ae-a6cc-452a-beb7-9fc457cd96d3
STEP: Creating a pod to test consume configMaps
Dec  2 10:08:13.849: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff" in namespace "projected-42" to be "Succeeded or Failed"
Dec  2 10:08:13.855: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964558ms
Dec  2 10:08:15.870: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02066513s
Dec  2 10:08:17.882: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033495072s
Dec  2 10:08:19.895: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045941091s
Dec  2 10:08:21.906: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.057447504s
STEP: Saw pod success
Dec  2 10:08:21.907: INFO: Pod "pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff" satisfied condition "Succeeded or Failed"
Dec  2 10:08:21.916: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:08:21.973: INFO: Waiting for pod pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff to disappear
Dec  2 10:08:21.984: INFO: Pod pod-projected-configmaps-f5a38b15-ca11-4195-a123-ab24792ca0ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:08:21.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-42" for this suite.

• [SLOW TEST:9.463 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2253,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:08:22.459: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 10:08:22.975374      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-9563", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Dec  2 10:08:23.184: INFO: Waiting up to 5m0s for pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd" in namespace "var-expansion-9563" to be "Succeeded or Failed"
Dec  2 10:08:23.194: INFO: Pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.208677ms
Dec  2 10:08:25.205: INFO: Pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020852293s
Dec  2 10:08:27.218: INFO: Pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033964056s
Dec  2 10:08:29.228: INFO: Pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043428481s
STEP: Saw pod success
Dec  2 10:08:29.228: INFO: Pod "var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd" satisfied condition "Succeeded or Failed"
Dec  2 10:08:29.237: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd container dapi-container: <nil>
STEP: delete the pod
Dec  2 10:08:29.286: INFO: Waiting for pod var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd to disappear
Dec  2 10:08:29.291: INFO: Pod var-expansion-029ccb7f-a08a-45e9-a776-ded67e83c5bd no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:08:29.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9563" for this suite.

• [SLOW TEST:7.305 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2260,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:08:29.764: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 10:08:30.291787      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-7375", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
W1202 10:08:39.065382      23 warnings.go:70] No static IP address has been configured for the service "test-service-lb", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:08:44.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7375" for this suite.

• [SLOW TEST:14.881 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":144,"skipped":2263,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:08:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:08:45.174713      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1180", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:08:45.795175      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1180", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:08:46.283297      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1180-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:08:46.736: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 10:08:48.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:08:50.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572527, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:08:54.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:08:54.400: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6521-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:09:02.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1180" for this suite.
STEP: Destroying namespace "webhook-1180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.637 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":145,"skipped":2269,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:09:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 10:09:04.823506      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-6424", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  2 10:09:11.612: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6424 pod-service-account-4da67ac4-a1f8-4f20-82b3-34af350f52e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  2 10:09:11.814: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6424 pod-service-account-4da67ac4-a1f8-4f20-82b3-34af350f52e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  2 10:09:12.005: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6424 pod-service-account-4da67ac4-a1f8-4f20-82b3-34af350f52e4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:09:12.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6424" for this suite.

• [SLOW TEST:8.523 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":146,"skipped":2276,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:09:12.808: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:09:13.337886      23 warnings.go:70] No static IP address has been configured for the namespace "projected-3260", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-c8070f19-d568-459b-84ea-6a28113f35d1
STEP: Creating a pod to test consume secrets
Dec  2 10:09:13.559: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d" in namespace "projected-3260" to be "Succeeded or Failed"
Dec  2 10:09:13.568: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.764803ms
Dec  2 10:09:15.582: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02241014s
Dec  2 10:09:17.594: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034478151s
Dec  2 10:09:19.609: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049526496s
Dec  2 10:09:21.620: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060741841s
Dec  2 10:09:23.633: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.074027378s
STEP: Saw pod success
Dec  2 10:09:23.633: INFO: Pod "pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d" satisfied condition "Succeeded or Failed"
Dec  2 10:09:23.639: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:09:23.686: INFO: Waiting for pod pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d to disappear
Dec  2 10:09:23.694: INFO: Pod pod-projected-secrets-58e66532-e5e0-4038-924f-8bfafbf1aa2d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:09:23.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3260" for this suite.

• [SLOW TEST:11.338 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2293,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:09:24.147: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 10:09:25.007668      23 warnings.go:70] No static IP address has been configured for the namespace "pods-8366", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:09:25.210: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  2 10:09:25.239: INFO: The status of Pod pod-exec-websocket-9730483e-d777-45be-ba21-6db52d23be11 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:09:27.252: INFO: The status of Pod pod-exec-websocket-9730483e-d777-45be-ba21-6db52d23be11 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:09:29.248: INFO: The status of Pod pod-exec-websocket-9730483e-d777-45be-ba21-6db52d23be11 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:09:31.249: INFO: The status of Pod pod-exec-websocket-9730483e-d777-45be-ba21-6db52d23be11 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:09:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8366" for this suite.

• [SLOW TEST:7.660 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2308,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:09:31.807: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 10:09:32.334626      23 warnings.go:70] No static IP address has been configured for the namespace "gc-9460", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1202 10:10:12.600319      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  2 10:10:12.600: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec  2 10:10:12.601: INFO: Deleting pod "simpletest.rc-2gh4g" in namespace "gc-9460"
Dec  2 10:10:12.628: INFO: Deleting pod "simpletest.rc-4wxbs" in namespace "gc-9460"
Dec  2 10:10:12.651: INFO: Deleting pod "simpletest.rc-5mxsv" in namespace "gc-9460"
Dec  2 10:10:12.678: INFO: Deleting pod "simpletest.rc-8m7rk" in namespace "gc-9460"
Dec  2 10:10:12.715: INFO: Deleting pod "simpletest.rc-8sj24" in namespace "gc-9460"
Dec  2 10:10:12.747: INFO: Deleting pod "simpletest.rc-8vjrg" in namespace "gc-9460"
Dec  2 10:10:12.783: INFO: Deleting pod "simpletest.rc-h9t6z" in namespace "gc-9460"
Dec  2 10:10:12.823: INFO: Deleting pod "simpletest.rc-qgs68" in namespace "gc-9460"
Dec  2 10:10:12.854: INFO: Deleting pod "simpletest.rc-tpjpv" in namespace "gc-9460"
Dec  2 10:10:12.890: INFO: Deleting pod "simpletest.rc-vrtxp" in namespace "gc-9460"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:10:12.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9460" for this suite.

• [SLOW TEST:41.606 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":149,"skipped":2310,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:10:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:10:13.936112      23 warnings.go:70] No static IP address has been configured for the namespace "services-9931", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9931
STEP: creating service affinity-clusterip-transition in namespace services-9931
STEP: creating replication controller affinity-clusterip-transition in namespace services-9931
I1202 10:10:14.717667      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9931, replica count: 3
I1202 10:10:17.769270      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:10:20.769613      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:10:23.770368      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:10:26.771192      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:10:29.773563      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:10:32.774450      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:10:32.792: INFO: Creating new exec pod
Dec  2 10:10:41.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec  2 10:10:42.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec  2 10:10:42.061: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:10:42.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.206.133 80'
Dec  2 10:10:42.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.206.133 80\nConnection to 10.100.206.133 80 port [tcp/http] succeeded!\n"
Dec  2 10:10:42.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:10:42.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.206.133:80/ ; done'
Dec  2 10:10:43.134: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n"
Dec  2 10:10:43.134: INFO: stdout: "\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd"
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.134: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:10:43.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.206.133:80/ ; done'
Dec  2 10:10:43.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n"
Dec  2 10:10:43.964: INFO: stdout: "\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w"
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.964: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:10:43.965: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:10:43.965: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:13.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.206.133:80/ ; done'
Dec  2 10:11:14.264: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n"
Dec  2 10:11:14.264: INFO: stdout: "\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-7km5k\naffinity-clusterip-transition-hgrxd\naffinity-clusterip-transition-4w89w"
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-7km5k
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-hgrxd
Dec  2 10:11:14.264: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:43.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-9931 exec execpod-affinityskd84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.206.133:80/ ; done'
Dec  2 10:11:44.261: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.206.133:80/\n"
Dec  2 10:11:44.261: INFO: stdout: "\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w\naffinity-clusterip-transition-4w89w"
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Received response from host: affinity-clusterip-transition-4w89w
Dec  2 10:11:44.261: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9931, will wait for the garbage collector to delete the pods
Dec  2 10:11:44.363: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.285016ms
Dec  2 10:11:44.463: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.695403ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:11:56.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9931" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:103.546 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":150,"skipped":2312,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:11:56.961: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:11:57.476824      23 warnings.go:70] No static IP address has been configured for the namespace "projected-8758", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-1ec48ed8-0438-44f5-b6e7-57347c99002f
STEP: Creating a pod to test consume secrets
Dec  2 10:11:57.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31" in namespace "projected-8758" to be "Succeeded or Failed"
Dec  2 10:11:57.686: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31": Phase="Pending", Reason="", readiness=false. Elapsed: 11.228424ms
Dec  2 10:11:59.703: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029095353s
Dec  2 10:12:01.718: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044056404s
Dec  2 10:12:03.728: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054069408s
Dec  2 10:12:05.742: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068004854s
STEP: Saw pod success
Dec  2 10:12:05.743: INFO: Pod "pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31" satisfied condition "Succeeded or Failed"
Dec  2 10:12:05.752: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:12:05.826: INFO: Waiting for pod pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31 to disappear
Dec  2 10:12:05.836: INFO: Pod pod-projected-secrets-133f1a6e-7f8b-4149-bb9c-6118c4f82f31 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:12:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8758" for this suite.

• [SLOW TEST:9.402 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:12:06.364: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 10:12:06.826585      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-5362", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:13:07.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5362" for this suite.

• [SLOW TEST:61.241 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":152,"skipped":2427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:13:07.606: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 10:13:08.131788      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-6979", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 10:13:08.852987      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-6979", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  2 10:13:08.922: INFO: Number of nodes with available pods: 0
Dec  2 10:13:08.922: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:13:09.946: INFO: Number of nodes with available pods: 0
Dec  2 10:13:09.946: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:13:10.947: INFO: Number of nodes with available pods: 0
Dec  2 10:13:10.948: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:13:11.943: INFO: Number of nodes with available pods: 0
Dec  2 10:13:11.943: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:13:12.959: INFO: Number of nodes with available pods: 0
Dec  2 10:13:12.959: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 10:13:13.948: INFO: Number of nodes with available pods: 1
Dec  2 10:13:13.948: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:14.950: INFO: Number of nodes with available pods: 1
Dec  2 10:13:14.950: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:15.952: INFO: Number of nodes with available pods: 1
Dec  2 10:13:15.953: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:16.950: INFO: Number of nodes with available pods: 1
Dec  2 10:13:16.950: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:17.947: INFO: Number of nodes with available pods: 1
Dec  2 10:13:17.947: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:18.950: INFO: Number of nodes with available pods: 1
Dec  2 10:13:18.950: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:19.943: INFO: Number of nodes with available pods: 1
Dec  2 10:13:19.943: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:20.959: INFO: Number of nodes with available pods: 1
Dec  2 10:13:20.959: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:21.942: INFO: Number of nodes with available pods: 1
Dec  2 10:13:21.942: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:22.959: INFO: Number of nodes with available pods: 1
Dec  2 10:13:22.960: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:23.939: INFO: Number of nodes with available pods: 1
Dec  2 10:13:23.940: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:24.952: INFO: Number of nodes with available pods: 1
Dec  2 10:13:24.952: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:25.946: INFO: Number of nodes with available pods: 1
Dec  2 10:13:25.946: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:26.952: INFO: Number of nodes with available pods: 1
Dec  2 10:13:26.952: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:27.949: INFO: Number of nodes with available pods: 1
Dec  2 10:13:27.949: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:28.944: INFO: Number of nodes with available pods: 1
Dec  2 10:13:28.945: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:29.951: INFO: Number of nodes with available pods: 1
Dec  2 10:13:29.951: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:30.940: INFO: Number of nodes with available pods: 1
Dec  2 10:13:30.941: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:31.950: INFO: Number of nodes with available pods: 1
Dec  2 10:13:31.950: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:32.950: INFO: Number of nodes with available pods: 1
Dec  2 10:13:32.950: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:33.943: INFO: Number of nodes with available pods: 1
Dec  2 10:13:33.943: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:34.953: INFO: Number of nodes with available pods: 1
Dec  2 10:13:34.953: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:35.947: INFO: Number of nodes with available pods: 1
Dec  2 10:13:35.947: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:36.944: INFO: Number of nodes with available pods: 2
Dec  2 10:13:36.944: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:37.949: INFO: Number of nodes with available pods: 2
Dec  2 10:13:37.949: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:38.946: INFO: Number of nodes with available pods: 2
Dec  2 10:13:38.946: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:39.941: INFO: Number of nodes with available pods: 2
Dec  2 10:13:39.941: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:40.948: INFO: Number of nodes with available pods: 2
Dec  2 10:13:40.949: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:41.952: INFO: Number of nodes with available pods: 2
Dec  2 10:13:41.953: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:42.946: INFO: Number of nodes with available pods: 2
Dec  2 10:13:42.946: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:13:43.949: INFO: Number of nodes with available pods: 3
Dec  2 10:13:43.949: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Dec  2 10:13:44.040: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52186229"},"items":null}

Dec  2 10:13:44.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52186230"},"items":[{"metadata":{"name":"daemon-set-ks86r","generateName":"daemon-set-","namespace":"daemonsets-6979","uid":"a143a949-1749-4f7c-aa7f-b5dc2011c4e1","resourceVersion":"52186229","creationTimestamp":"2022-12-02T10:13:09Z","deletionTimestamp":"2022-12-02T10:14:14Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a6684ef-4071-43f5-895b-2de8919b355a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a6684ef-4071-43f5-895b-2de8919b355a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6c87v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6c87v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa","securityContext":{},"imagePullSecrets":[{"name":"kube-plus-pull-secret"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"}],"hostIP":"11.0.95.5","podIP":"11.34.25.3","podIPs":[{"ip":"11.34.25.3"}],"startTime":"2022-12-02T10:13:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-02T10:13:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754","containerID":"docker://8d68f43d638eea6d9e6fdca66ce455f7f779848347b6dade07e25fdf5bac9a75","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t4bv8","generateName":"daemon-set-","namespace":"daemonsets-6979","uid":"df294ae7-5f86-4684-ac6f-7347bb7d0c3a","resourceVersion":"52186226","creationTimestamp":"2022-12-02T10:13:09Z","deletionTimestamp":"2022-12-02T10:14:14Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a6684ef-4071-43f5-895b-2de8919b355a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a6684ef-4071-43f5-895b-2de8919b355a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4zfn7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4zfn7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"5e47d0ea-e90b-466b-b6de-2748d512ebf3","securityContext":{},"imagePullSecrets":[{"name":"kube-plus-pull-secret"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["5e47d0ea-e90b-466b-b6de-2748d512ebf3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"}],"hostIP":"11.0.95.7","podIP":"11.34.25.4","podIPs":[{"ip":"11.34.25.4"}],"startTime":"2022-12-02T10:13:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-02T10:13:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754","containerID":"docker://baec8f3d936d30756838cff98df0261869d885d9e00f06e9f26193455ccff85f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vsxpn","generateName":"daemon-set-","namespace":"daemonsets-6979","uid":"3e53e548-4cf1-4505-a5b6-ed8860aeaf06","resourceVersion":"52186228","creationTimestamp":"2022-12-02T10:13:09Z","deletionTimestamp":"2022-12-02T10:14:14Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a6684ef-4071-43f5-895b-2de8919b355a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a6684ef-4071-43f5-895b-2de8919b355a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-02T10:13:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g8xgg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g8xgg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"a11041ca-3d0d-4c61-a7c9-2e5b598977b5","securityContext":{},"imagePullSecrets":[{"name":"kube-plus-pull-secret"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["a11041ca-3d0d-4c61-a7c9-2e5b598977b5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:43Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:43Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-02T10:13:09Z"}],"hostIP":"11.0.95.6","podIP":"11.34.25.2","podIPs":[{"ip":"11.34.25.2"}],"startTime":"2022-12-02T10:13:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-02T10:13:42Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754","containerID":"docker://5239b45620adf1d0a585d66d79b3b0e9fc2892563fa1625391ee8e8e909b4efc","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:13:44.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6979" for this suite.

• [SLOW TEST:36.971 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":153,"skipped":2484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:13:44.578: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption
W1202 10:13:45.134511      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-6170", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  2 10:13:45.333: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 10:14:45.526: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:14:45.547: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption-path
W1202 10:14:46.059186      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-path-5509", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec  2 10:14:52.314: INFO: found a healthy node: 5e47d0ea-e90b-466b-b6de-2748d512ebf3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:15:24.473: INFO: pods created so far: [1 1 1]
Dec  2 10:15:24.473: INFO: length of pods created so far: 3
Dec  2 10:15:28.498: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:15:35.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5509" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:15:36.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6170" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:112.103 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":154,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:15:36.682: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:15:37.182473      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-5991", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:15:37.891337      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-5991", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:15:38.440492      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-5991-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:15:38.831: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 10:15:40.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572939, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572939, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572939, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805572939, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:15:44.340: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:15:44.349: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7906-crds.webhook.example.com via the AdmissionRegistration API
Dec  2 10:15:49.927: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:15:52.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5991" for this suite.
STEP: Destroying namespace "webhook-5991-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":155,"skipped":2544,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:15:54.398: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 10:15:55.004541      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-2916", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Dec  2 10:15:55.226: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:15:57.238: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:15:59.253: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  2 10:16:00.305: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:16:01.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2916" for this suite.

• [SLOW TEST:7.475 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":156,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:16:01.874: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:16:02.377766      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-9743", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  2 10:16:02.582: INFO: Waiting up to 5m0s for pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20" in namespace "downward-api-9743" to be "Succeeded or Failed"
Dec  2 10:16:02.588: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.394905ms
Dec  2 10:16:04.601: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019315746s
Dec  2 10:16:06.611: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029531848s
Dec  2 10:16:08.623: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041358956s
Dec  2 10:16:10.634: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052755873s
Dec  2 10:16:12.645: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.063167603s
STEP: Saw pod success
Dec  2 10:16:12.645: INFO: Pod "downward-api-844c82be-b1c4-4a33-8710-f97458127b20" satisfied condition "Succeeded or Failed"
Dec  2 10:16:12.651: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downward-api-844c82be-b1c4-4a33-8710-f97458127b20 container dapi-container: <nil>
STEP: delete the pod
Dec  2 10:16:12.719: INFO: Waiting for pod downward-api-844c82be-b1c4-4a33-8710-f97458127b20 to disappear
Dec  2 10:16:12.724: INFO: Pod downward-api-844c82be-b1c4-4a33-8710-f97458127b20 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:16:12.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9743" for this suite.

• [SLOW TEST:11.361 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2596,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:16:13.236: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:16:13.665652      23 warnings.go:70] No static IP address has been configured for the namespace "projected-930", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-930
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d8be033c-2d04-4748-825b-315cc3587282
STEP: Creating the pod
Dec  2 10:16:13.906: INFO: The status of Pod pod-projected-configmaps-5fccabb3-4055-4ded-9599-8e5f6259f4b6 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:16:15.920: INFO: The status of Pod pod-projected-configmaps-5fccabb3-4055-4ded-9599-8e5f6259f4b6 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:16:17.919: INFO: The status of Pod pod-projected-configmaps-5fccabb3-4055-4ded-9599-8e5f6259f4b6 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:16:19.920: INFO: The status of Pod pod-projected-configmaps-5fccabb3-4055-4ded-9599-8e5f6259f4b6 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-d8be033c-2d04-4748-825b-315cc3587282
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:17:38.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-930" for this suite.

• [SLOW TEST:86.821 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2599,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:17:40.057: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 10:17:40.692441      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-5763", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  2 10:17:40.886: INFO: Waiting up to 5m0s for pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec" in namespace "emptydir-5763" to be "Succeeded or Failed"
Dec  2 10:17:40.892: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788701ms
Dec  2 10:17:42.905: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018523027s
Dec  2 10:17:44.918: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031419078s
Dec  2 10:17:46.927: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040239674s
Dec  2 10:17:48.938: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051348607s
Dec  2 10:17:50.949: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.061955638s
STEP: Saw pod success
Dec  2 10:17:50.949: INFO: Pod "pod-736213fd-01bd-4cef-afef-f076cbd543ec" satisfied condition "Succeeded or Failed"
Dec  2 10:17:50.961: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-736213fd-01bd-4cef-afef-f076cbd543ec container test-container: <nil>
STEP: delete the pod
Dec  2 10:17:51.003: INFO: Waiting for pod pod-736213fd-01bd-4cef-afef-f076cbd543ec to disappear
Dec  2 10:17:51.011: INFO: Pod pod-736213fd-01bd-4cef-afef-f076cbd543ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:17:51.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5763" for this suite.

• [SLOW TEST:11.517 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:17:51.575: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 10:17:52.196542      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-9803", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:17:52.414: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:17:54.428: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:17:56.428: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:17:58.426: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:00.424: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:02.426: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:04.426: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:06.427: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:08.428: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:10.423: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:12.425: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = false)
Dec  2 10:18:14.426: INFO: The status of Pod test-webserver-5e7776a6-27ac-4a14-8492-c8ead7e0c3ba is Running (Ready = true)
Dec  2 10:18:14.435: INFO: Container started at 2022-12-02 10:17:55 +0000 UTC, pod became ready at 2022-12-02 10:18:13 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:18:14.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9803" for this suite.

• [SLOW TEST:23.344 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2640,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:18:14.920: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:18:15.423276      23 warnings.go:70] No static IP address has been configured for the namespace "services-4377", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4377
Dec  2 10:18:15.633: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:18:17.646: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec  2 10:18:17.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec  2 10:18:18.950: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec  2 10:18:18.950: INFO: stdout: "iptables"
Dec  2 10:18:18.950: INFO: proxyMode: iptables
Dec  2 10:18:18.984: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec  2 10:18:18.989: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4377
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4377
I1202 10:18:19.564698      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4377, replica count: 3
I1202 10:18:22.616198      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:25.617150      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:28.618325      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:31.618588      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:34.620004      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:37.620787      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:40.621736      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:43.621968      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:46.622897      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:49.623432      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:52.624761      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:55.625585      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:18:58.626541      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:18:58.642: INFO: Creating new exec pod
Dec  2 10:19:03.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec  2 10:19:03.894: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec  2 10:19:03.894: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:19:03.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.197.75 80'
Dec  2 10:19:04.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.197.75 80\nConnection to 10.100.197.75 80 port [tcp/http] succeeded!\n"
Dec  2 10:19:04.092: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:19:04.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:19:04.392: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:19:04.392: INFO: stdout: "\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5"
Dec  2 10:19:04.392: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:04.392: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:04.392: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:04.406: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:19:34.748: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:19:34.748: INFO: stdout: "\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-gwpd5"
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:19:34.748: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:04.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:20:04.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:20:04.739: INFO: stdout: "\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-gwpd5"
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:04.739: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:20:34.743: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:20:34.743: INFO: stdout: "\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-d2kt5"
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:20:34.743: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:21:04.738: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:21:04.738: INFO: stdout: "\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-gwpd5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-lgr75\naffinity-clusterip-timeout-lgr75"
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-gwpd5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Received response from host: affinity-clusterip-timeout-lgr75
Dec  2 10:21:04.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.197.75:80/ ; done'
Dec  2 10:21:05.046: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:21:05.046: INFO: stdout: "\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5\naffinity-clusterip-timeout-d2kt5"
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Received response from host: affinity-clusterip-timeout-d2kt5
Dec  2 10:21:05.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.197.75:80/'
Dec  2 10:21:05.245: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:21:05.245: INFO: stdout: "affinity-clusterip-timeout-d2kt5"
Dec  2 10:21:25.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.197.75:80/'
Dec  2 10:21:25.466: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:21:25.466: INFO: stdout: "affinity-clusterip-timeout-d2kt5"
Dec  2 10:21:45.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4377 exec execpod-affinity7r64z -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.197.75:80/'
Dec  2 10:21:45.655: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.197.75:80/\n"
Dec  2 10:21:45.655: INFO: stdout: "affinity-clusterip-timeout-gwpd5"
Dec  2 10:21:45.655: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4377, will wait for the garbage collector to delete the pods
Dec  2 10:21:45.762: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 15.247588ms
Dec  2 10:21:45.862: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.752576ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:21:55.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4377" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:220.644 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":161,"skipped":2650,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:21:55.566: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:21:56.069465      23 warnings.go:70] No static IP address has been configured for the namespace "services-121", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-121
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-121
STEP: creating replication controller externalsvc in namespace services-121
I1202 10:21:57.261844      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-121, replica count: 2
I1202 10:22:00.312444      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:22:03.312703      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec  2 10:22:03.815: INFO: Creating new exec pod
Dec  2 10:22:07.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-121 exec execpodvcnwf -- /bin/sh -x -c nslookup clusterip-service.services-121.svc.cluster.local'
Dec  2 10:22:08.099: INFO: stderr: "+ nslookup clusterip-service.services-121.svc.cluster.local\n"
Dec  2 10:22:08.099: INFO: stdout: "Server:\t\t10.100.192.2\nAddress:\t10.100.192.2#53\n\nclusterip-service.services-121.svc.cluster.local\tcanonical name = externalsvc.services-121.svc.cluster.local.\nName:\texternalsvc.services-121.svc.cluster.local\nAddress: 10.100.227.3\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-121, will wait for the garbage collector to delete the pods
Dec  2 10:22:08.172: INFO: Deleting ReplicationController externalsvc took: 12.835354ms
Dec  2 10:22:08.273: INFO: Terminating ReplicationController externalsvc pods took: 101.049797ms
Dec  2 10:22:11.781: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:22:12.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:17.491 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":162,"skipped":2653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:22:13.066: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pod-network-test
W1202 10:22:13.792802      23 warnings.go:70] No static IP address has been configured for the namespace "pod-network-test-1299", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1299
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  2 10:22:13.968: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  2 10:22:14.080: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:22:16.096: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:22:18.093: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:22:20.097: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:22.093: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:24.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:26.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:28.092: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:30.093: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:32.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:34.091: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:22:36.092: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  2 10:22:36.113: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:38.124: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:40.122: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:42.128: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:44.125: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:46.125: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:22:48.139: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  2 10:22:48.154: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec  2 10:22:50.166: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec  2 10:22:52.166: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec  2 10:22:54.164: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  2 10:22:58.236: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  2 10:22:58.236: INFO: Breadth first check of 11.34.25.2 on host 11.0.95.7...
Dec  2 10:22:58.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.25.5:9080/dial?request=hostname&protocol=udp&host=11.34.25.2&port=8081&tries=1'] Namespace:pod-network-test-1299 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:22:58.245: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:22:58.347: INFO: Waiting for responses: map[]
Dec  2 10:22:58.347: INFO: reached 11.34.25.2 after 0/1 tries
Dec  2 10:22:58.347: INFO: Breadth first check of 11.34.25.3 on host 11.0.95.6...
Dec  2 10:22:58.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.25.5:9080/dial?request=hostname&protocol=udp&host=11.34.25.3&port=8081&tries=1'] Namespace:pod-network-test-1299 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:22:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:22:58.461: INFO: Waiting for responses: map[]
Dec  2 10:22:58.461: INFO: reached 11.34.25.3 after 0/1 tries
Dec  2 10:22:58.461: INFO: Breadth first check of 11.34.25.4 on host 11.0.95.5...
Dec  2 10:22:58.471: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.25.5:9080/dial?request=hostname&protocol=udp&host=11.34.25.4&port=8081&tries=1'] Namespace:pod-network-test-1299 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:22:58.471: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:22:58.593: INFO: Waiting for responses: map[]
Dec  2 10:22:58.593: INFO: reached 11.34.25.4 after 0/1 tries
Dec  2 10:22:58.593: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:22:58.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1299" for this suite.

• [SLOW TEST:46.187 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":2740,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:22:59.254: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 10:22:59.744550      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-4185", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Dec  2 10:22:59.945: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec  2 10:23:04.959: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Dec  2 10:23:30.986: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:23:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4185" for this suite.

• [SLOW TEST:32.397 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":164,"skipped":2742,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:23:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 10:23:32.157239      23 warnings.go:70] No static IP address has been configured for the namespace "dns-9353", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9353.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9353.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9353.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9353.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9353.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9353.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 10:23:36.879: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.890: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.900: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.912: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.952: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:36.984: INFO: Lookups using dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9353.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9353.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9353.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9353.svc.cluster.local]

Dec  2 10:23:42.076: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9353.svc.cluster.local from pod dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2: the server could not find the requested resource (get pods dns-test-6004f337-b496-4458-872d-806a8fdf6ce2)
Dec  2 10:23:42.096: INFO: Lookups using dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2 failed for: [jessie_tcp@dns-test-service-2.dns-9353.svc.cluster.local]

Dec  2 10:23:47.090: INFO: DNS probes using dns-9353/dns-test-6004f337-b496-4458-872d-806a8fdf6ce2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:23:48.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9353" for this suite.

• [SLOW TEST:16.869 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":165,"skipped":2749,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:23:48.521: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:23:49.017331      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-5439", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5439
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-7ad21332-6953-4ba1-bb45-4d08938af7e2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:23:53.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5439" for this suite.

• [SLOW TEST:5.404 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":2754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:23:53.930: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:23:54.446331      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-1079", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1079/configmap-test-9008363f-05e2-466f-89c1-2ba98ea578ad
STEP: Creating a pod to test consume configMaps
Dec  2 10:23:54.649: INFO: Waiting up to 5m0s for pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d" in namespace "configmap-1079" to be "Succeeded or Failed"
Dec  2 10:23:54.655: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.572244ms
Dec  2 10:23:56.667: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017973959s
Dec  2 10:23:58.680: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031304834s
Dec  2 10:24:00.690: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041377766s
Dec  2 10:24:02.700: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.051073014s
STEP: Saw pod success
Dec  2 10:24:02.700: INFO: Pod "pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d" satisfied condition "Succeeded or Failed"
Dec  2 10:24:02.709: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d container env-test: <nil>
STEP: delete the pod
Dec  2 10:24:02.756: INFO: Waiting for pod pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d to disappear
Dec  2 10:24:02.762: INFO: Pod pod-configmaps-e31ef8e3-dd6c-435a-8a90-8a477c4d957d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:24:02.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1079" for this suite.

• [SLOW TEST:9.316 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":2787,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:24:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pod-network-test
W1202 10:24:03.726086      23 warnings.go:70] No static IP address has been configured for the namespace "pod-network-test-201", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-201
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  2 10:24:03.925: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  2 10:24:04.014: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:24:06.027: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:24:08.036: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:10.022: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:12.023: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:14.022: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:16.024: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:18.022: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:20.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:22.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:24.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:24:26.024: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  2 10:24:26.042: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:28.054: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:30.056: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:32.051: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:34.053: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:36.056: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:38.053: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:24:40.057: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  2 10:24:40.076: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  2 10:24:44.187: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  2 10:24:44.187: INFO: Going to poll 11.34.25.2 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:24:44.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.34.25.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-201 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:24:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:24:44.310: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  2 10:24:44.310: INFO: Going to poll 11.34.25.3 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:24:44.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.34.25.3:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-201 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:24:44.319: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:24:44.439: INFO: Found all 1 expected endpoints: [netserver-1]
Dec  2 10:24:44.439: INFO: Going to poll 11.34.25.4 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  2 10:24:44.446: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://11.34.25.4:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-201 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:24:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:24:44.556: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:24:44.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-201" for this suite.

• [SLOW TEST:41.825 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":2800,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:24:45.072: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:24:45.554871      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-129", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-adddc3c5-4725-4a4a-a732-6aad8a82e09a
STEP: Creating a pod to test consume configMaps
Dec  2 10:24:45.781: INFO: Waiting up to 5m0s for pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a" in namespace "configmap-129" to be "Succeeded or Failed"
Dec  2 10:24:45.792: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.55373ms
Dec  2 10:24:47.804: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023033368s
Dec  2 10:24:49.816: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035124868s
Dec  2 10:24:51.833: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052166516s
Dec  2 10:24:53.842: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061301844s
STEP: Saw pod success
Dec  2 10:24:53.842: INFO: Pod "pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a" satisfied condition "Succeeded or Failed"
Dec  2 10:24:53.855: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:24:53.894: INFO: Waiting for pod pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a to disappear
Dec  2 10:24:53.909: INFO: Pod pod-configmaps-6488c53d-5719-4cab-87c4-d147b910623a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:24:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-129" for this suite.

• [SLOW TEST:9.356 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":2804,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:24:54.428: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:24:54.953255      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-3140", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:24:55.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62" in namespace "downward-api-3140" to be "Succeeded or Failed"
Dec  2 10:24:55.154: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62": Phase="Pending", Reason="", readiness=false. Elapsed: 7.84765ms
Dec  2 10:24:57.165: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018559935s
Dec  2 10:24:59.191: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045029872s
Dec  2 10:25:01.205: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058430218s
Dec  2 10:25:03.218: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.071487118s
STEP: Saw pod success
Dec  2 10:25:03.219: INFO: Pod "downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62" satisfied condition "Succeeded or Failed"
Dec  2 10:25:03.225: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62 container client-container: <nil>
STEP: delete the pod
Dec  2 10:25:03.277: INFO: Waiting for pod downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62 to disappear
Dec  2 10:25:03.283: INFO: Pod downwardapi-volume-7a8a6c30-735c-42d4-8416-91c173996a62 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:25:03.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3140" for this suite.

• [SLOW TEST:9.530 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":2824,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:25:03.959: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 10:25:04.449746      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-4875", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4875
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec  2 10:25:04.635: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:25:21.154: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:26:05.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4875" for this suite.

• [SLOW TEST:62.184 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":171,"skipped":2842,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:26:06.143: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 10:26:06.605824      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-570", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  2 10:26:06.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-570 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Dec  2 10:26:06.881: INFO: stderr: ""
Dec  2 10:26:06.881: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Dec  2 10:26:06.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-570 delete pods e2e-test-httpd-pod'
Dec  2 10:26:12.847: INFO: stderr: ""
Dec  2 10:26:12.847: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:26:12.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-570" for this suite.

• [SLOW TEST:7.153 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":172,"skipped":2852,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:26:13.297: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:26:13.742867      23 warnings.go:70] No static IP address has been configured for the namespace "services-6482", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:26:13.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6482" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":173,"skipped":2866,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:26:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:26:14.909742      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-1982", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  2 10:26:15.157: INFO: The status of Pod annotationupdate50bc6230-f963-4d7d-aa2b-31227b6715bd is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:26:17.166: INFO: The status of Pod annotationupdate50bc6230-f963-4d7d-aa2b-31227b6715bd is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:26:19.169: INFO: The status of Pod annotationupdate50bc6230-f963-4d7d-aa2b-31227b6715bd is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:26:21.168: INFO: The status of Pod annotationupdate50bc6230-f963-4d7d-aa2b-31227b6715bd is Running (Ready = true)
Dec  2 10:26:21.732: INFO: Successfully updated pod "annotationupdate50bc6230-f963-4d7d-aa2b-31227b6715bd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:26:23.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1982" for this suite.

• [SLOW TEST:9.820 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:26:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:26:24.801557      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3292", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:26:25.412129      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3292", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:26:25.861828      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3292-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:26:26.344: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  2 10:26:28.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:26:30.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573587, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:26:33.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:26:33.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3292" for this suite.
STEP: Destroying namespace "webhook-3292-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.238 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":175,"skipped":2908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:26:35.530: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 10:26:36.062957      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-575", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:26:42.292: INFO: Deleting pod "var-expansion-00a45c11-ad5b-4e56-83a4-7bfcb72472bb" in namespace "var-expansion-575"
Dec  2 10:26:42.309: INFO: Wait up to 5m0s for pod "var-expansion-00a45c11-ad5b-4e56-83a4-7bfcb72472bb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-575" for this suite.

• [SLOW TEST:25.250 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":176,"skipped":2947,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:00.781: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replication-controller
W1202 10:27:01.229240      23 warnings.go:70] No static IP address has been configured for the namespace "replication-controller-7724", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Dec  2 10:27:01.440: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:27:03.456: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:27:05.454: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:27:07.454: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:08.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7724" for this suite.

• [SLOW TEST:8.488 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":177,"skipped":2952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:09.269: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 10:27:09.919209      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-8660", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:27:10.114: INFO: Got root ca configmap in namespace "svcaccounts-8660"
Dec  2 10:27:10.125: INFO: Deleted root ca configmap in namespace "svcaccounts-8660"
STEP: waiting for a new root ca configmap created
Dec  2 10:27:10.635: INFO: Recreated root ca configmap in namespace "svcaccounts-8660"
Dec  2 10:27:10.645: INFO: Updated root ca configmap in namespace "svcaccounts-8660"
STEP: waiting for the root ca configmap reconciled
Dec  2 10:27:11.154: INFO: Reconciled root ca configmap in namespace "svcaccounts-8660"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:11.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8660" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":178,"skipped":2982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:27:12.696596      23 warnings.go:70] No static IP address has been configured for the namespace "projected-1362", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-4c651c42-62db-4812-ae51-32f4918a136e
STEP: Creating a pod to test consume secrets
Dec  2 10:27:12.900: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd" in namespace "projected-1362" to be "Succeeded or Failed"
Dec  2 10:27:12.906: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.131781ms
Dec  2 10:27:14.921: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021540513s
Dec  2 10:27:16.931: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031024441s
Dec  2 10:27:18.939: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039428616s
Dec  2 10:27:20.955: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.05505903s
STEP: Saw pod success
Dec  2 10:27:20.955: INFO: Pod "pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd" satisfied condition "Succeeded or Failed"
Dec  2 10:27:20.962: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:27:21.016: INFO: Waiting for pod pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd to disappear
Dec  2 10:27:21.024: INFO: Pod pod-projected-secrets-bedebc2f-8657-4413-8a90-0642c6b73dbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:21.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1362" for this suite.

• [SLOW TEST:9.309 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:21.547: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 10:27:22.027081      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-7948", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:29.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7948" for this suite.

• [SLOW TEST:8.458 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":180,"skipped":3059,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:30.009: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename namespaces
W1202 10:27:30.524677      23 warnings.go:70] No static IP address has been configured for the namespace "namespaces-8022", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
W1202 10:27:31.143769      23 warnings.go:70] No static IP address has been configured for the namespace "nsdeletetest-7693", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7693
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
W1202 10:27:52.444293      23 warnings.go:70] No static IP address has been configured for the namespace "nsdeletetest-2994", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2994
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:27:52.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8022" for this suite.
STEP: Destroying namespace "nsdeletetest-7693" for this suite.
Dec  2 10:27:53.092: INFO: Namespace nsdeletetest-7693 was already deleted
STEP: Destroying namespace "nsdeletetest-2994" for this suite.

• [SLOW TEST:23.626 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":181,"skipped":3063,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:27:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:27:54.062257      23 warnings.go:70] No static IP address has been configured for the namespace "services-7248", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7248
Dec  2 10:27:54.270: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:27:56.284: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec  2 10:27:56.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec  2 10:27:56.498: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec  2 10:27:56.498: INFO: stdout: "iptables"
Dec  2 10:27:56.498: INFO: proxyMode: iptables
Dec  2 10:27:56.528: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec  2 10:27:56.538: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7248
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7248
I1202 10:27:57.305385      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7248, replica count: 3
I1202 10:28:00.356268      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:03.356964      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:06.357546      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:09.358133      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:12.358859      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:15.359628      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:18.360336      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:28:21.360715      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:28:21.392: INFO: Creating new exec pod
Dec  2 10:28:26.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec  2 10:28:27.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec  2 10:28:27.800: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:28:27.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.233.6 80'
Dec  2 10:28:27.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.233.6 80\nConnection to 10.100.233.6 80 port [tcp/http] succeeded!\n"
Dec  2 10:28:27.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:28:27.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 31522'
Dec  2 10:28:28.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 31522\nConnection to 11.0.95.5 31522 port [tcp/*] succeeded!\n"
Dec  2 10:28:28.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:28:28.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 31522'
Dec  2 10:28:28.414: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 31522\nConnection to 11.0.95.6 31522 port [tcp/*] succeeded!\n"
Dec  2 10:28:28.414: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:28:28.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.95.7:31522/ ; done'
Dec  2 10:28:28.702: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n"
Dec  2 10:28:28.702: INFO: stdout: "\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt\naffinity-nodeport-timeout-jfwnt"
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.702: INFO: Received response from host: affinity-nodeport-timeout-jfwnt
Dec  2 10:28:28.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://11.0.95.7:31522/'
Dec  2 10:28:28.912: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n"
Dec  2 10:28:28.912: INFO: stdout: "affinity-nodeport-timeout-jfwnt"
Dec  2 10:28:48.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://11.0.95.7:31522/'
Dec  2 10:28:49.155: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n"
Dec  2 10:28:49.155: INFO: stdout: "affinity-nodeport-timeout-jfwnt"
Dec  2 10:29:09.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7248 exec execpod-affinitym5w85 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://11.0.95.7:31522/'
Dec  2 10:29:09.378: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://11.0.95.7:31522/\n"
Dec  2 10:29:09.378: INFO: stdout: "affinity-nodeport-timeout-mxjwm"
Dec  2 10:29:09.378: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7248, will wait for the garbage collector to delete the pods
Dec  2 10:29:09.501: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.822069ms
Dec  2 10:29:09.602: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.146326ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:29:27.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7248" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:94.549 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":182,"skipped":3063,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:29:28.184: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:29:28.619098      23 warnings.go:70] No static IP address has been configured for the namespace "projected-35", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-35
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-8cf52a96-35b0-40d0-8772-fa9857ebc177
STEP: Creating a pod to test consume configMaps
Dec  2 10:29:28.826: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b" in namespace "projected-35" to be "Succeeded or Failed"
Dec  2 10:29:28.832: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092174ms
Dec  2 10:29:30.847: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02082678s
Dec  2 10:29:32.858: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032635239s
Dec  2 10:29:34.875: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048990264s
Dec  2 10:29:36.887: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061314977s
STEP: Saw pod success
Dec  2 10:29:36.887: INFO: Pod "pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b" satisfied condition "Succeeded or Failed"
Dec  2 10:29:36.894: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  2 10:29:36.959: INFO: Waiting for pod pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b to disappear
Dec  2 10:29:36.965: INFO: Pod pod-projected-configmaps-c93efdc5-0522-488c-bc0a-e471195eb33b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:29:36.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-35" for this suite.

• [SLOW TEST:9.283 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:29:37.469: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename subpath
W1202 10:29:37.928369      23 warnings.go:70] No static IP address has been configured for the namespace "subpath-845", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-6xz9
STEP: Creating a pod to test atomic-volume-subpath
Dec  2 10:29:38.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6xz9" in namespace "subpath-845" to be "Succeeded or Failed"
Dec  2 10:29:38.169: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216323ms
Dec  2 10:29:40.180: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018510711s
Dec  2 10:29:42.196: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034387643s
Dec  2 10:29:44.212: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 6.050914154s
Dec  2 10:29:46.227: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 8.065164825s
Dec  2 10:29:48.244: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 10.082019293s
Dec  2 10:29:50.254: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 12.092134118s
Dec  2 10:29:52.267: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 14.105472296s
Dec  2 10:29:54.281: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 16.119059533s
Dec  2 10:29:56.296: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 18.134413264s
Dec  2 10:29:58.315: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 20.153638104s
Dec  2 10:30:00.325: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=true. Elapsed: 22.163373203s
Dec  2 10:30:02.335: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Running", Reason="", readiness=false. Elapsed: 24.173561128s
Dec  2 10:30:04.373: INFO: Pod "pod-subpath-test-secret-6xz9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.21175751s
STEP: Saw pod success
Dec  2 10:30:04.374: INFO: Pod "pod-subpath-test-secret-6xz9" satisfied condition "Succeeded or Failed"
Dec  2 10:30:04.380: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-subpath-test-secret-6xz9 container test-container-subpath-secret-6xz9: <nil>
STEP: delete the pod
Dec  2 10:30:04.422: INFO: Waiting for pod pod-subpath-test-secret-6xz9 to disappear
Dec  2 10:30:04.428: INFO: Pod pod-subpath-test-secret-6xz9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-6xz9
Dec  2 10:30:04.428: INFO: Deleting pod "pod-subpath-test-secret-6xz9" in namespace "subpath-845"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:04.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-845" for this suite.

• [SLOW TEST:27.513 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":184,"skipped":3111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:04.983: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename job
W1202 10:30:05.546547      23 warnings.go:70] No static IP address has been configured for the namespace "job-8872", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:33.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8872" for this suite.

• [SLOW TEST:29.266 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":185,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:34.250: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:30:34.771748      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-8424", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-8424/configmap-test-215c5c4a-1481-43ed-b7f7-1abadb2cb8f3
STEP: Creating a pod to test consume configMaps
Dec  2 10:30:34.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e" in namespace "configmap-8424" to be "Succeeded or Failed"
Dec  2 10:30:34.997: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.410344ms
Dec  2 10:30:37.012: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025701074s
Dec  2 10:30:39.026: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039640773s
Dec  2 10:30:41.037: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050348595s
Dec  2 10:30:43.052: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065186236s
Dec  2 10:30:45.070: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.083415765s
STEP: Saw pod success
Dec  2 10:30:45.070: INFO: Pod "pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e" satisfied condition "Succeeded or Failed"
Dec  2 10:30:45.077: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e container env-test: <nil>
STEP: delete the pod
Dec  2 10:30:45.124: INFO: Waiting for pod pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e to disappear
Dec  2 10:30:45.131: INFO: Pod pod-configmaps-8f4376e1-1263-4a73-8c7a-aff07048c30e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:45.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8424" for this suite.

• [SLOW TEST:11.423 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3210,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:45.673: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename custom-resource-definition
W1202 10:30:46.171189      23 warnings.go:70] No static IP address has been configured for the namespace "custom-resource-definition-1646", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1646
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:30:46.354: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:51.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1646" for this suite.

• [SLOW TEST:6.779 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":187,"skipped":3211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:52.453: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename events
W1202 10:30:52.992446      23 warnings.go:70] No static IP address has been configured for the namespace "events-6199", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec  2 10:30:53.213: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:53.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6199" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":188,"skipped":3285,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:53.797: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 10:30:54.709869      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-8067", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:30:55.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8067" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":189,"skipped":3296,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:30:55.588: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename subpath
W1202 10:30:56.141582      23 warnings.go:70] No static IP address has been configured for the namespace "subpath-8637", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-7d7d
STEP: Creating a pod to test atomic-volume-subpath
Dec  2 10:30:56.374: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7d7d" in namespace "subpath-8637" to be "Succeeded or Failed"
Dec  2 10:30:56.381: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.184526ms
Dec  2 10:30:58.396: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022116624s
Dec  2 10:31:00.410: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035635852s
Dec  2 10:31:02.422: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047597364s
Dec  2 10:31:04.437: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 8.063030524s
Dec  2 10:31:06.451: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 10.076488192s
Dec  2 10:31:08.464: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 12.089488514s
Dec  2 10:31:10.478: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 14.103751391s
Dec  2 10:31:12.490: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 16.115848825s
Dec  2 10:31:14.502: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 18.128099155s
Dec  2 10:31:16.518: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 20.144040014s
Dec  2 10:31:18.534: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 22.159696838s
Dec  2 10:31:20.550: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 24.175536658s
Dec  2 10:31:22.566: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=true. Elapsed: 26.191305321s
Dec  2 10:31:24.581: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Running", Reason="", readiness=false. Elapsed: 28.207045218s
Dec  2 10:31:26.596: INFO: Pod "pod-subpath-test-configmap-7d7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.221548089s
STEP: Saw pod success
Dec  2 10:31:26.596: INFO: Pod "pod-subpath-test-configmap-7d7d" satisfied condition "Succeeded or Failed"
Dec  2 10:31:26.604: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-subpath-test-configmap-7d7d container test-container-subpath-configmap-7d7d: <nil>
STEP: delete the pod
Dec  2 10:31:26.661: INFO: Waiting for pod pod-subpath-test-configmap-7d7d to disappear
Dec  2 10:31:26.668: INFO: Pod pod-subpath-test-configmap-7d7d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7d7d
Dec  2 10:31:26.668: INFO: Deleting pod "pod-subpath-test-configmap-7d7d" in namespace "subpath-8637"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:31:26.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8637" for this suite.

• [SLOW TEST:31.605 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":190,"skipped":3302,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:31:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:31:27.799688      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3844", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:31:28.543960      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3844", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
W1202 10:31:29.046165      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-3844-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:31:29.688: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 10:31:31.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:31:33.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573890, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:31:37.294: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
W1202 10:31:37.812663      23 warnings.go:70] No static IP address has been configured for the namespace "fail-closed-namesapce", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:31:38.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3844" for this suite.
STEP: Destroying namespace "webhook-3844-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.297 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":191,"skipped":3303,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:31:40.493: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:31:41.016558      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-1263", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:31:41.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84" in namespace "downward-api-1263" to be "Succeeded or Failed"
Dec  2 10:31:41.248: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.540532ms
Dec  2 10:31:43.267: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025624526s
Dec  2 10:31:45.284: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042555514s
Dec  2 10:31:47.296: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055048514s
Dec  2 10:31:49.307: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065991027s
STEP: Saw pod success
Dec  2 10:31:49.307: INFO: Pod "downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84" satisfied condition "Succeeded or Failed"
Dec  2 10:31:49.316: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84 container client-container: <nil>
STEP: delete the pod
Dec  2 10:31:49.368: INFO: Waiting for pod downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84 to disappear
Dec  2 10:31:49.375: INFO: Pod downwardapi-volume-e083b93e-11e7-4bcd-88d1-69eb7e102f84 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:31:49.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1263" for this suite.

• [SLOW TEST:9.409 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3314,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:31:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption
W1202 10:31:50.750758      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-3823", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec  2 10:31:53.132: INFO: running pods: 0 < 3
Dec  2 10:31:55.147: INFO: running pods: 0 < 3
Dec  2 10:31:57.145: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:31:59.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3823" for this suite.

• [SLOW TEST:9.803 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":193,"skipped":3320,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:31:59.706: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:32:00.233847      23 warnings.go:70] No static IP address has been configured for the namespace "services-5505", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5505
STEP: creating service affinity-nodeport in namespace services-5505
STEP: creating replication controller affinity-nodeport in namespace services-5505
I1202 10:32:00.982737      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5505, replica count: 3
I1202 10:32:04.033222      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:32:07.033487      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:32:10.034622      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:32:13.035090      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:32:16.036089      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:32:19.036498      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:32:19.069: INFO: Creating new exec pod
Dec  2 10:32:24.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-5505 exec execpod-affinityqtsz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec  2 10:32:24.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec  2 10:32:24.375: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:32:24.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-5505 exec execpod-affinityqtsz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.193.111 80'
Dec  2 10:32:24.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.193.111 80\nConnection to 10.100.193.111 80 port [tcp/http] succeeded!\n"
Dec  2 10:32:24.573: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:32:24.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-5505 exec execpod-affinityqtsz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 32308'
Dec  2 10:32:24.790: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 32308\nConnection to 11.0.95.7 32308 port [tcp/*] succeeded!\n"
Dec  2 10:32:24.790: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:32:24.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-5505 exec execpod-affinityqtsz5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 32308'
Dec  2 10:32:25.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 32308\nConnection to 11.0.95.5 32308 port [tcp/*] succeeded!\n"
Dec  2 10:32:25.008: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 10:32:25.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-5505 exec execpod-affinityqtsz5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://11.0.95.7:32308/ ; done'
Dec  2 10:32:25.326: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://11.0.95.7:32308/\n"
Dec  2 10:32:25.326: INFO: stdout: "\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc\naffinity-nodeport-75vrc"
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Received response from host: affinity-nodeport-75vrc
Dec  2 10:32:25.326: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5505, will wait for the garbage collector to delete the pods
Dec  2 10:32:25.426: INFO: Deleting ReplicationController affinity-nodeport took: 14.436761ms
Dec  2 10:32:25.527: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.421176ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:32:38.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5505" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:39.425 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":194,"skipped":3323,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:32:39.131: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 10:32:39.630403      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-2887", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  2 10:32:39.843: INFO: Waiting up to 5m0s for pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a" in namespace "emptydir-2887" to be "Succeeded or Failed"
Dec  2 10:32:39.854: INFO: Pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.147748ms
Dec  2 10:32:41.865: INFO: Pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022012478s
Dec  2 10:32:43.875: INFO: Pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032668284s
Dec  2 10:32:45.888: INFO: Pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04555022s
STEP: Saw pod success
Dec  2 10:32:45.889: INFO: Pod "pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a" satisfied condition "Succeeded or Failed"
Dec  2 10:32:45.895: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a container test-container: <nil>
STEP: delete the pod
Dec  2 10:32:45.936: INFO: Waiting for pod pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a to disappear
Dec  2 10:32:45.941: INFO: Pod pod-badbdee0-9d04-4fc9-afcc-8f4b17f2f18a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:32:45.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2887" for this suite.

• [SLOW TEST:7.282 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3324,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:32:46.414: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:32:47.235724      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-8458", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:32:47.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5" in namespace "downward-api-8458" to be "Succeeded or Failed"
Dec  2 10:32:47.460: INFO: Pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.770973ms
Dec  2 10:32:49.475: INFO: Pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021360484s
Dec  2 10:32:51.490: INFO: Pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036119816s
Dec  2 10:32:53.505: INFO: Pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051147687s
STEP: Saw pod success
Dec  2 10:32:53.506: INFO: Pod "downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5" satisfied condition "Succeeded or Failed"
Dec  2 10:32:53.513: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5 container client-container: <nil>
STEP: delete the pod
Dec  2 10:32:53.559: INFO: Waiting for pod downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5 to disappear
Dec  2 10:32:53.565: INFO: Pod downwardapi-volume-29d383f4-d2dd-473d-88bd-d23c512242c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:32:53.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8458" for this suite.

• [SLOW TEST:7.733 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:32:54.148: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename init-container
W1202 10:32:54.702521      23 warnings.go:70] No static IP address has been configured for the namespace "init-container-3438", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  2 10:32:54.906: INFO: PodSpec: initContainers in spec.initContainers
Dec  2 10:33:43.283: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-28680bc4-7214-49c4-b7e9-3f8d6292ad5f", GenerateName:"", Namespace:"init-container-3438", SelfLink:"", UID:"2c782c85-adef-425e-8f14-62425f903554", ResourceVersion:"52195532", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63805573975, loc:(*time.Location)(0xa04d060)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"906303031"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00b6cf830), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b6cf848), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00b6cf860), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b6cf878), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pkmtr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038f5e20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pkmtr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pkmtr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pkmtr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0038b3238), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"5e47d0ea-e90b-466b-b6de-2748d512ebf3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0031a4380), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"kube-plus-pull-secret"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038b34c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038b34e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0038b34e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0038b34ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003b18fc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573975, loc:(*time.Location)(0xa04d060)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573975, loc:(*time.Location)(0xa04d060)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573975, loc:(*time.Location)(0xa04d060)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805573975, loc:(*time.Location)(0xa04d060)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"11.0.95.7", PodIP:"11.34.25.2", PodIPs:[]v1.PodIP{v1.PodIP{IP:"11.34.25.2"}}, StartTime:(*v1.Time)(0xc00b6cf8a8), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031a4460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031a44d0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker://sha256:6b8f5a05efd5a16a4e1706a73ed4f32be739c951a26e32ebd5975f2ffe862eee", ContainerID:"docker://ce844619de07cc5f0a359c2ce9184d3cdc355401a62a65140b05c57b99968437", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038f5ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038f5e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0038b382f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:33:43.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3438" for this suite.

• [SLOW TEST:49.662 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":197,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:33:43.812: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 10:33:44.323103      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-6836", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6836
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec  2 10:33:44.505: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:34:43.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6836" for this suite.

• [SLOW TEST:60.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":198,"skipped":3390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:34:44.783: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 10:34:45.352833      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-8990", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 10:34:45.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5" in namespace "downward-api-8990" to be "Succeeded or Failed"
Dec  2 10:34:45.589: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.093019ms
Dec  2 10:34:47.599: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017012453s
Dec  2 10:34:49.609: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026796529s
Dec  2 10:34:51.624: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041974011s
Dec  2 10:34:53.641: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058771183s
STEP: Saw pod success
Dec  2 10:34:53.641: INFO: Pod "downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5" satisfied condition "Succeeded or Failed"
Dec  2 10:34:53.649: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5 container client-container: <nil>
STEP: delete the pod
Dec  2 10:34:53.718: INFO: Waiting for pod downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5 to disappear
Dec  2 10:34:53.725: INFO: Pod downwardapi-volume-68729529-557d-4494-9019-7723b48f67c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:34:53.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8990" for this suite.

• [SLOW TEST:9.552 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3432,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:34:54.335: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 10:34:54.855005      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-1718", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1718
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  2 10:34:55.069: INFO: Waiting up to 5m0s for pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8" in namespace "emptydir-1718" to be "Succeeded or Failed"
Dec  2 10:34:55.083: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.314574ms
Dec  2 10:34:57.097: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027494324s
Dec  2 10:34:59.173: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103590072s
Dec  2 10:35:01.187: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.118014461s
Dec  2 10:35:03.197: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.127992361s
STEP: Saw pod success
Dec  2 10:35:03.197: INFO: Pod "pod-0ea59dfd-a807-451c-bfaa-f2f949110db8" satisfied condition "Succeeded or Failed"
Dec  2 10:35:03.204: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-0ea59dfd-a807-451c-bfaa-f2f949110db8 container test-container: <nil>
STEP: delete the pod
Dec  2 10:35:03.252: INFO: Waiting for pod pod-0ea59dfd-a807-451c-bfaa-f2f949110db8 to disappear
Dec  2 10:35:03.258: INFO: Pod pod-0ea59dfd-a807-451c-bfaa-f2f949110db8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:35:03.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1718" for this suite.

• [SLOW TEST:9.404 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:35:03.742: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 10:35:04.396616      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-6140", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:35:04.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6140" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":201,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:35:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 10:35:05.718201      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-7927", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec  2 10:35:05.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-7927 create -f -'
Dec  2 10:35:07.533: INFO: stderr: ""
Dec  2 10:35:07.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  2 10:35:08.548: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:08.548: INFO: Found 0 / 1
Dec  2 10:35:09.543: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:09.543: INFO: Found 0 / 1
Dec  2 10:35:10.544: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:10.544: INFO: Found 0 / 1
Dec  2 10:35:11.546: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:11.547: INFO: Found 0 / 1
Dec  2 10:35:12.544: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:12.544: INFO: Found 0 / 1
Dec  2 10:35:13.545: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:13.545: INFO: Found 0 / 1
Dec  2 10:35:14.545: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:14.545: INFO: Found 0 / 1
Dec  2 10:35:15.548: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:15.548: INFO: Found 0 / 1
Dec  2 10:35:16.545: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:16.545: INFO: Found 0 / 1
Dec  2 10:35:17.542: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:17.542: INFO: Found 1 / 1
Dec  2 10:35:17.542: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  2 10:35:17.550: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:17.550: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  2 10:35:17.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-7927 patch pod agnhost-primary-g2jl2 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  2 10:35:17.654: INFO: stderr: ""
Dec  2 10:35:17.654: INFO: stdout: "pod/agnhost-primary-g2jl2 patched\n"
STEP: checking annotations
Dec  2 10:35:17.663: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  2 10:35:17.663: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:35:17.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7927" for this suite.

• [SLOW TEST:13.071 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":202,"skipped":3527,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:35:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename watch
W1202 10:35:18.824389      23 warnings.go:70] No static IP address has been configured for the namespace "watch-972", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:35:24.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-972" for this suite.

• [SLOW TEST:6.918 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":203,"skipped":3532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:35:25.189: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename taint-single-pod
W1202 10:35:25.669680      23 warnings.go:70] No static IP address has been configured for the namespace "taint-single-pod-1955", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec  2 10:35:25.854: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 10:36:26.017: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:36:26.034: INFO: Starting informer...
STEP: Starting pod...
Dec  2 10:36:26.268: INFO: Pod is running on 5e47d0ea-e90b-466b-b6de-2748d512ebf3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec  2 10:36:26.296: INFO: Pod wasn't evicted. Proceeding
Dec  2 10:36:26.296: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec  2 10:37:41.401: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:37:41.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1955" for this suite.

• [SLOW TEST:136.741 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":204,"skipped":3559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:37:41.930: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 10:37:42.481212      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-8482", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8482
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec  2 10:37:42.699: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:37:57.690: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:38:40.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8482" for this suite.

• [SLOW TEST:59.341 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":205,"skipped":3591,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:38:41.272: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 10:38:41.770132      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-9896", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9896
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-9896
Dec  2 10:38:42.449: INFO: Found 0 stateful pods, waiting for 1
Dec  2 10:38:52.458: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Dec  2 10:38:52.497: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Dec  2 10:38:52.513: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Dec  2 10:38:52.517: INFO: Observed &StatefulSet event: ADDED
Dec  2 10:38:52.517: INFO: Found Statefulset ss in namespace statefulset-9896 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  2 10:38:52.517: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Dec  2 10:38:52.517: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  2 10:38:52.551: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Dec  2 10:38:52.554: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 10:38:52.554: INFO: Deleting all statefulset in ns statefulset-9896
Dec  2 10:38:52.561: INFO: Scaling statefulset ss to 0
Dec  2 10:39:02.592: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 10:39:02.598: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:39:02.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9896" for this suite.

• [SLOW TEST:21.955 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":206,"skipped":3596,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:39:03.228: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:39:03.655232      23 warnings.go:70] No static IP address has been configured for the namespace "projected-2536", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-d34eb277-ea02-411e-b0a6-b58deaff81dc
STEP: Creating secret with name secret-projected-all-test-volume-595f4178-ad89-4388-a861-36c3637c749c
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  2 10:39:03.877: INFO: Waiting up to 5m0s for pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0" in namespace "projected-2536" to be "Succeeded or Failed"
Dec  2 10:39:03.884: INFO: Pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.261166ms
Dec  2 10:39:05.892: INFO: Pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014561012s
Dec  2 10:39:07.900: INFO: Pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022455178s
Dec  2 10:39:09.907: INFO: Pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029987499s
STEP: Saw pod success
Dec  2 10:39:09.907: INFO: Pod "projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0" satisfied condition "Succeeded or Failed"
Dec  2 10:39:09.913: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  2 10:39:09.967: INFO: Waiting for pod projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0 to disappear
Dec  2 10:39:09.973: INFO: Pod projected-volume-953ce356-f994-4d2b-98c3-8a0aa5898ea0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:39:09.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2536" for this suite.

• [SLOW TEST:7.186 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3603,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:39:10.415: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:39:11.107040      23 warnings.go:70] No static IP address has been configured for the namespace "projected-9770", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-d14f877b-cece-4c42-a30e-31726cbdf3fc
STEP: Creating a pod to test consume configMaps
Dec  2 10:39:11.337: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5" in namespace "projected-9770" to be "Succeeded or Failed"
Dec  2 10:39:11.344: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.245983ms
Dec  2 10:39:13.353: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016039056s
Dec  2 10:39:15.362: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024786082s
Dec  2 10:39:17.369: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032096363s
Dec  2 10:39:19.378: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040836855s
Dec  2 10:39:21.386: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049157068s
Dec  2 10:39:23.395: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.058235919s
STEP: Saw pod success
Dec  2 10:39:23.396: INFO: Pod "pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5" satisfied condition "Succeeded or Failed"
Dec  2 10:39:23.404: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:39:23.447: INFO: Waiting for pod pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5 to disappear
Dec  2 10:39:23.453: INFO: Pod pod-projected-configmaps-a1bfd062-cbcc-483e-b61d-f04586e8c3e5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:39:23.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9770" for this suite.

• [SLOW TEST:13.479 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":3623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:39:23.901: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:39:24.483006      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-1120", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-9a022c7e-5192-42b5-b07f-65ccca9ac790
STEP: Creating a pod to test consume configMaps
Dec  2 10:39:24.681: INFO: Waiting up to 5m0s for pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb" in namespace "configmap-1120" to be "Succeeded or Failed"
Dec  2 10:39:24.688: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68768ms
Dec  2 10:39:26.696: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014941918s
Dec  2 10:39:28.703: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022744031s
Dec  2 10:39:30.712: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031122871s
Dec  2 10:39:32.719: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038022121s
STEP: Saw pod success
Dec  2 10:39:32.719: INFO: Pod "pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb" satisfied condition "Succeeded or Failed"
Dec  2 10:39:32.724: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:39:32.764: INFO: Waiting for pod pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb to disappear
Dec  2 10:39:32.770: INFO: Pod pod-configmaps-7db0fb42-4d14-4650-b588-152557eae2fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:39:32.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1120" for this suite.

• [SLOW TEST:9.368 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3709,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:39:33.269: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 10:39:33.884194      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-2850", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2850
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:39:34.572: INFO: Found 0 stateful pods, waiting for 1
Dec  2 10:39:44.582: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Dec  2 10:39:44.617: INFO: Found 1 stateful pods, waiting for 2
Dec  2 10:39:54.629: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 10:40:04.630: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 10:40:04.630: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 10:40:04.730: INFO: Deleting all statefulset in ns statefulset-2850
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:40:04.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2850" for this suite.

• [SLOW TEST:32.013 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":210,"skipped":3724,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:40:05.283: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:40:05.779376      23 warnings.go:70] No static IP address has been configured for the namespace "projected-9153", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-508f7bcb-4918-41f7-8a52-db4bc4d8ec8c
STEP: Creating a pod to test consume configMaps
Dec  2 10:40:05.976: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050" in namespace "projected-9153" to be "Succeeded or Failed"
Dec  2 10:40:05.985: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 8.810796ms
Dec  2 10:40:07.994: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017552737s
Dec  2 10:40:10.002: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025664977s
Dec  2 10:40:12.010: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033837428s
Dec  2 10:40:14.018: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041787202s
Dec  2 10:40:16.027: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050469568s
Dec  2 10:40:18.034: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 12.05733169s
Dec  2 10:40:20.042: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 14.065373856s
Dec  2 10:40:22.049: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 16.072855673s
Dec  2 10:40:24.057: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 18.080423803s
Dec  2 10:40:26.065: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 20.088800662s
Dec  2 10:40:28.074: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 22.097689496s
Dec  2 10:40:30.083: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Pending", Reason="", readiness=false. Elapsed: 24.10639953s
Dec  2 10:40:32.091: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.114982397s
STEP: Saw pod success
Dec  2 10:40:32.091: INFO: Pod "pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050" satisfied condition "Succeeded or Failed"
Dec  2 10:40:32.097: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:40:32.157: INFO: Waiting for pod pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050 to disappear
Dec  2 10:40:32.163: INFO: Pod pod-projected-configmaps-9cb6fd78-ecf4-4e3c-b8aa-5d84b5540050 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:40:32.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9153" for this suite.

• [SLOW TEST:27.396 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3738,"failed":0}
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:40:32.680: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 10:40:33.114304      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-8214", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec  2 10:40:33.383: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.383: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.402: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.402: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.430: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.431: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.525: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:33.525: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  2 10:40:40.559: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  2 10:40:40.559: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  2 10:40:41.582: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec  2 10:40:41.595: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 0
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.597: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.610: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.610: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.696: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.697: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:41.717: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:41.717: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:41.739: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:41.739: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:46.477: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:46.477: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:46.532: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
STEP: listing Deployments
Dec  2 10:40:46.570: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec  2 10:40:46.586: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec  2 10:40:46.594: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:46.614: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:46.656: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:46.697: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:46.732: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:46.753: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:53.069: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:53.129: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:53.223: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:53.240: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  2 10:40:55.798: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec  2 10:40:55.856: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.856: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.856: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 1
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 3
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 2
Dec  2 10:40:55.857: INFO: observed Deployment test-deployment in namespace deployment-8214 with ReadyReplicas 3
STEP: deleting the Deployment
Dec  2 10:40:55.875: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.876: INFO: observed event type MODIFIED
Dec  2 10:40:55.877: INFO: observed event type MODIFIED
Dec  2 10:40:55.877: INFO: observed event type MODIFIED
Dec  2 10:40:55.877: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 10:40:55.885: INFO: Log out all the ReplicaSets if there is no deployment created
Dec  2 10:40:55.892: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-8214  c3c14be7-b096-4ede-93e5-2e858128fc73 52199048 4 2022-12-02 10:40:42 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0f9c6111-1071-4db1-96d3-b9194764e5e7 0xc00ce653b7 0xc00ce653b8}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f9c6111-1071-4db1-96d3-b9194764e5e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:40:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00ce65440 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec  2 10:40:55.902: INFO: pod: "test-deployment-56c98d85f9-7qtfk":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-7qtfk test-deployment-56c98d85f9- deployment-8214  c8460807-988a-4200-9c2b-1d508cbd1fc3 52198999 0 2022-12-02 10:40:47 +0000 UTC 2022-12-02 10:40:55 +0000 UTC 0xc00177c3a8 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 c3c14be7-b096-4ede-93e5-2e858128fc73 0xc00177c3d7 0xc00177c3d8}] []  [{kube-controller-manager Update v1 2022-12-02 10:40:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c14be7-b096-4ede-93e5-2e858128fc73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:40:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.26.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlczb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlczb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.26.5,StartTime:2022-12-02 10:40:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:40:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker://sha256:ed210e3e4a5bae1237f1bb44d72a05a2f1e5c6bfe7a7e73da179e2534269c459,ContainerID:docker://52fdc56424fc76c5cec7cadc57be494f2a1df0c23ac558c627f08abcf885c63a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.26.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  2 10:40:55.903: INFO: pod: "test-deployment-56c98d85f9-hrxzs":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-hrxzs test-deployment-56c98d85f9- deployment-8214  52579ba0-c807-4ca1-a4b5-58b979920922 52199043 0 2022-12-02 10:40:42 +0000 UTC 2022-12-02 10:40:57 +0000 UTC 0xc00177c5a8 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 c3c14be7-b096-4ede-93e5-2e858128fc73 0xc00177c5d7 0xc00177c5d8}] []  [{kube-controller-manager Update v1 2022-12-02 10:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c14be7-b096-4ede-93e5-2e858128fc73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:40:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.26.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7qhz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7qhz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.26.4,StartTime:2022-12-02 10:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker://sha256:ed210e3e4a5bae1237f1bb44d72a05a2f1e5c6bfe7a7e73da179e2534269c459,ContainerID:docker://88c8301e29b328532000fa19070b9f04b4dc24bf90e5a7c80dc595a656996e37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.26.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  2 10:40:55.903: INFO: ReplicaSet "test-deployment-6c9dc9cdf6":
&ReplicaSet{ObjectMeta:{test-deployment-6c9dc9cdf6  deployment-8214  1af00000-03eb-454d-8ef5-ef4e3681adeb 52198919 3 2022-12-02 10:40:34 +0000 UTC <nil> <nil> map[pod-template-hash:6c9dc9cdf6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0f9c6111-1071-4db1-96d3-b9194764e5e7 0xc00ce654a7 0xc00ce654a8}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f9c6111-1071-4db1-96d3-b9194764e5e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:40:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6c9dc9cdf6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6c9dc9cdf6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00ce65530 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec  2 10:40:55.911: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-8214  20aa840b-6a40-4efe-a084-e7683ab0759b 52199040 2 2022-12-02 10:40:47 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0f9c6111-1071-4db1-96d3-b9194764e5e7 0xc00ce65597 0xc00ce65598}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:40:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f9c6111-1071-4db1-96d3-b9194764e5e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:40:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00ce65620 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec  2 10:40:55.934: INFO: pod: "test-deployment-d4dfddfbf-5ct9t":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-5ct9t test-deployment-d4dfddfbf- deployment-8214  4449a700-2794-4354-8dbb-308c1d944033 52199056 0 2022-12-02 10:40:47 +0000 UTC 2022-12-02 10:40:57 +0000 UTC 0xc00238cc78 map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 20aa840b-6a40-4efe-a084-e7683ab0759b 0xc00238cca7 0xc00238cca8}] []  [{kube-controller-manager Update v1 2022-12-02 10:40:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20aa840b-6a40-4efe-a084-e7683ab0759b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:40:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.26.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k87wp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k87wp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.26.6,StartTime:2022-12-02 10:40:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:40:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://6798e1a5b15176f4dce6a27f6c0021d90a0f52c0429f6c05d59c28698b41cf8f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.26.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  2 10:40:55.935: INFO: pod: "test-deployment-d4dfddfbf-6vb4g":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-6vb4g test-deployment-d4dfddfbf- deployment-8214  dcbf4e3c-3f90-4de8-a053-4a8f79f80b6c 52199055 0 2022-12-02 10:40:54 +0000 UTC 2022-12-02 10:40:57 +0000 UTC 0xc00238cea8 map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 20aa840b-6a40-4efe-a084-e7683ab0759b 0xc00238ced7 0xc00238ced8}] []  [{kube-controller-manager Update v1 2022-12-02 10:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20aa840b-6a40-4efe-a084-e7683ab0759b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.26.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqv5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqv5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.26.7,StartTime:2022-12-02 10:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:40:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://89b84254df4b4e313360d247ba3174d754686efcc7da6f77819d78fdf09df03c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.26.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:40:55.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8214" for this suite.

• [SLOW TEST:23.700 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":212,"skipped":3738,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:40:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename containers
W1202 10:40:56.875636      23 warnings.go:70] No static IP address has been configured for the namespace "containers-1888", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Dec  2 10:40:57.071: INFO: Waiting up to 5m0s for pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385" in namespace "containers-1888" to be "Succeeded or Failed"
Dec  2 10:40:57.076: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Pending", Reason="", readiness=false. Elapsed: 5.554415ms
Dec  2 10:40:59.096: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02533624s
Dec  2 10:41:01.107: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035988249s
Dec  2 10:41:03.114: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043438631s
Dec  2 10:41:05.125: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054249619s
Dec  2 10:41:07.132: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.061100765s
STEP: Saw pod success
Dec  2 10:41:07.132: INFO: Pod "client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385" satisfied condition "Succeeded or Failed"
Dec  2 10:41:07.138: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:41:07.181: INFO: Waiting for pod client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385 to disappear
Dec  2 10:41:07.187: INFO: Pod client-containers-3f0ae6c5-e267-427f-98ba-c2dd06f28385 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:41:07.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1888" for this suite.

• [SLOW TEST:11.271 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":3741,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:41:07.652: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 10:41:08.139717      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-4823", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4823
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:41:08.346: INFO: created pod
Dec  2 10:41:08.346: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4823" to be "Succeeded or Failed"
Dec  2 10:41:08.352: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514923ms
Dec  2 10:41:10.360: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01401184s
Dec  2 10:41:12.368: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022564544s
Dec  2 10:41:14.376: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 6.030378807s
Dec  2 10:41:16.385: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 8.038992623s
Dec  2 10:41:18.392: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.045932134s
STEP: Saw pod success
Dec  2 10:41:18.392: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec  2 10:41:48.392: INFO: polling logs
Dec  2 10:41:48.411: INFO: Pod logs: 
I1202 10:41:13.337887       1 log.go:195] OK: Got token
I1202 10:41:13.338013       1 log.go:195] validating with in-cluster discovery
I1202 10:41:13.338467       1 log.go:195] OK: got issuer https://master.cfcr.internal:8443
I1202 10:41:13.338583       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://master.cfcr.internal:8443", Subject:"system:serviceaccount:svcaccounts-4823:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669978269, NotBefore:1669977669, IssuedAt:1669977669, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4823", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"98507279-b7ca-4c4f-b12f-596f92e5050d"}}}
I1202 10:41:15.087287       1 log.go:195] OK: Constructed OIDC provider for issuer https://master.cfcr.internal:8443
I1202 10:41:15.090380       1 log.go:195] OK: Validated signature on JWT
I1202 10:41:15.090520       1 log.go:195] OK: Got valid claims from token!
I1202 10:41:15.090572       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://master.cfcr.internal:8443", Subject:"system:serviceaccount:svcaccounts-4823:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1669978269, NotBefore:1669977669, IssuedAt:1669977669, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4823", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"98507279-b7ca-4c4f-b12f-596f92e5050d"}}}

Dec  2 10:41:48.411: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:41:48.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4823" for this suite.

• [SLOW TEST:41.234 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":214,"skipped":3744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:41:48.887: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:41:49.355468      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-8759", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-7f21a552-f55e-4274-bb1c-0f970da36d3c
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:41:49.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8759" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":215,"skipped":3778,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:41:50.180: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 10:41:50.653415      23 warnings.go:70] No static IP address has been configured for the namespace "dns-5252", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5252.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5252.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5252.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 10:41:56.971: INFO: DNS probes using dns-5252/dns-test-66431027-87ae-42d2-bd7f-73a11f54837e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:41:57.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5252" for this suite.

• [SLOW TEST:7.308 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":216,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:41:57.488: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 10:41:58.032333      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-2418", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2418
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:42:02.235: INFO: Deleting pod "var-expansion-ad1785d1-caf6-42cd-a991-2b0d73022bbe" in namespace "var-expansion-2418"
Dec  2 10:42:02.252: INFO: Wait up to 5m0s for pod "var-expansion-ad1785d1-caf6-42cd-a991-2b0d73022bbe" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:42:20.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2418" for this suite.

• [SLOW TEST:23.647 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":217,"skipped":3803,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:42:21.136: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context-test
W1202 10:42:21.653246      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-test-7763", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:42:21.854: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643" in namespace "security-context-test-7763" to be "Succeeded or Failed"
Dec  2 10:42:21.861: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": Phase="Pending", Reason="", readiness=false. Elapsed: 6.712953ms
Dec  2 10:42:23.870: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015604234s
Dec  2 10:42:25.878: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024209067s
Dec  2 10:42:27.887: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032593562s
Dec  2 10:42:29.895: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041007791s
Dec  2 10:42:29.895: INFO: Pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643" satisfied condition "Succeeded or Failed"
Dec  2 10:42:29.911: INFO: Got logs for pod "busybox-privileged-false-69e5bf86-0ed2-4602-aad3-84f8d0241643": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:42:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7763" for this suite.

• [SLOW TEST:9.665 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":3804,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:42:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename watch
W1202 10:42:31.252749      23 warnings.go:70] No static IP address has been configured for the namespace "watch-2804", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  2 10:42:31.453: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199840 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:42:31.454: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199840 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  2 10:42:41.469: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199917 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:42:41.470: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199917 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  2 10:42:51.485: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199970 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:42:51.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52199970 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  2 10:43:01.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52200022 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:43:01.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2804  664ca8a9-6e28-470e-ab6f-d6c47a31370d 52200022 0 2022-12-02 10:42:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-02 10:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  2 10:43:11.520: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2804  f76117b0-ad9a-4d02-b0e4-a77fa3216e47 52200075 0 2022-12-02 10:43:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-02 10:43:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:43:11.520: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2804  f76117b0-ad9a-4d02-b0e4-a77fa3216e47 52200075 0 2022-12-02 10:43:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-02 10:43:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  2 10:43:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2804  f76117b0-ad9a-4d02-b0e4-a77fa3216e47 52200121 0 2022-12-02 10:43:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-02 10:43:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 10:43:21.536: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2804  f76117b0-ad9a-4d02-b0e4-a77fa3216e47 52200121 0 2022-12-02 10:43:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-02 10:43:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:43:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2804" for this suite.

• [SLOW TEST:61.246 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":219,"skipped":3816,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:43:32.048: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption
W1202 10:43:32.534448      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-9461", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:43:32.712: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename disruption-2
W1202 10:43:33.178465      23 warnings.go:70] No static IP address has been configured for the namespace "disruption-2-127", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9461
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:43:39.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-127" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:43:39.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9461" for this suite.

• [SLOW TEST:8.398 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":220,"skipped":3829,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:43:40.446: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:43:40.879332      23 warnings.go:70] No static IP address has been configured for the namespace "services-6256", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-6256
STEP: creating replication controller nodeport-test in namespace services-6256
I1202 10:43:41.566572      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6256, replica count: 2
I1202 10:43:44.617402      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 10:43:47.618572      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 10:43:47.618: INFO: Creating new exec pod
Dec  2 10:43:52.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  2 10:43:54.012: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:54.012: INFO: stdout: ""
Dec  2 10:43:55.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  2 10:43:55.283: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:55.283: INFO: stdout: ""
Dec  2 10:43:56.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  2 10:43:56.250: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:56.250: INFO: stdout: ""
Dec  2 10:43:57.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  2 10:43:57.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:57.232: INFO: stdout: ""
Dec  2 10:43:58.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  2 10:43:58.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:58.216: INFO: stdout: "nodeport-test-6vdrh"
Dec  2 10:43:58.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:43:58.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:58.403: INFO: stdout: ""
Dec  2 10:43:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:43:59.643: INFO: stderr: "+ nc -v -t -w 2 10.100.250.198 80\n+ echo hostName\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:43:59.643: INFO: stdout: ""
Dec  2 10:44:00.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:00.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:00.618: INFO: stdout: ""
Dec  2 10:44:01.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:01.654: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:01.654: INFO: stdout: ""
Dec  2 10:44:02.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:02.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:02.619: INFO: stdout: ""
Dec  2 10:44:03.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:03.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:03.620: INFO: stdout: ""
Dec  2 10:44:04.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:04.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:04.642: INFO: stdout: ""
Dec  2 10:44:05.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:05.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:05.658: INFO: stdout: ""
Dec  2 10:44:06.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:06.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:06.656: INFO: stdout: ""
Dec  2 10:44:07.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.250.198 80'
Dec  2 10:44:07.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.250.198 80\nConnection to 10.100.250.198 80 port [tcp/http] succeeded!\n"
Dec  2 10:44:07.701: INFO: stdout: "nodeport-test-6vdrh"
Dec  2 10:44:07.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 31850'
Dec  2 10:44:07.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 31850\nConnection to 11.0.95.7 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:07.973: INFO: stdout: ""
Dec  2 10:44:08.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 31850'
Dec  2 10:44:09.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 31850\nConnection to 11.0.95.7 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:09.220: INFO: stdout: ""
Dec  2 10:44:09.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 31850'
Dec  2 10:44:10.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 31850\nConnection to 11.0.95.7 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:10.165: INFO: stdout: ""
Dec  2 10:44:10.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 31850'
Dec  2 10:44:11.174: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 31850\nConnection to 11.0.95.7 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:11.174: INFO: stdout: ""
Dec  2 10:44:11.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.7 31850'
Dec  2 10:44:12.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.7 31850\nConnection to 11.0.95.7 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:12.176: INFO: stdout: "nodeport-test-6vdrh"
Dec  2 10:44:12.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6256 exec execpodspdxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 31850'
Dec  2 10:44:12.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 31850\nConnection to 11.0.95.6 31850 port [tcp/*] succeeded!\n"
Dec  2 10:44:12.376: INFO: stdout: "nodeport-test-jnksv"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:44:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6256" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:32.374 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":221,"skipped":3833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:44:12.821: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubelet-test
W1202 10:44:13.288818      23 warnings.go:70] No static IP address has been configured for the namespace "kubelet-test-1541", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:44:13.496: INFO: The status of Pod busybox-scheduling-068a5931-a6e9-4ce7-a9af-6da1f878c4c2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:44:15.503: INFO: The status of Pod busybox-scheduling-068a5931-a6e9-4ce7-a9af-6da1f878c4c2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:44:17.504: INFO: The status of Pod busybox-scheduling-068a5931-a6e9-4ce7-a9af-6da1f878c4c2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:44:19.503: INFO: The status of Pod busybox-scheduling-068a5931-a6e9-4ce7-a9af-6da1f878c4c2 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:44:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1541" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":3875,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:44:19.999: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename svcaccounts
W1202 10:44:20.486427      23 warnings.go:70] No static IP address has been configured for the namespace "svcaccounts-4249", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:44:20.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4249" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":223,"skipped":3881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:44:21.256: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 10:44:21.698912      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-4620", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4620
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:44:21.891: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec  2 10:44:37.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 create -f -'
Dec  2 10:44:40.582: INFO: stderr: ""
Dec  2 10:44:40.582: INFO: stdout: "e2e-test-crd-publish-openapi-121-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  2 10:44:40.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 delete e2e-test-crd-publish-openapi-121-crds test-foo'
Dec  2 10:44:40.692: INFO: stderr: ""
Dec  2 10:44:40.692: INFO: stdout: "e2e-test-crd-publish-openapi-121-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec  2 10:44:40.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 apply -f -'
Dec  2 10:44:42.578: INFO: stderr: ""
Dec  2 10:44:42.578: INFO: stdout: "e2e-test-crd-publish-openapi-121-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  2 10:44:42.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 delete e2e-test-crd-publish-openapi-121-crds test-foo'
Dec  2 10:44:42.667: INFO: stderr: ""
Dec  2 10:44:42.667: INFO: stdout: "e2e-test-crd-publish-openapi-121-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec  2 10:44:42.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 create -f -'
Dec  2 10:44:44.306: INFO: rc: 1
Dec  2 10:44:44.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 apply -f -'
Dec  2 10:44:44.636: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec  2 10:44:44.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 create -f -'
Dec  2 10:44:45.250: INFO: rc: 1
Dec  2 10:44:45.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 --namespace=crd-publish-openapi-4620 apply -f -'
Dec  2 10:44:45.558: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec  2 10:44:45.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 explain e2e-test-crd-publish-openapi-121-crds'
Dec  2 10:44:45.895: INFO: stderr: ""
Dec  2 10:44:45.895: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-121-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec  2 10:44:45.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 explain e2e-test-crd-publish-openapi-121-crds.metadata'
Dec  2 10:44:46.265: INFO: stderr: ""
Dec  2 10:44:46.265: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-121-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec  2 10:44:46.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 explain e2e-test-crd-publish-openapi-121-crds.spec'
Dec  2 10:44:46.596: INFO: stderr: ""
Dec  2 10:44:46.596: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-121-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec  2 10:44:46.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 explain e2e-test-crd-publish-openapi-121-crds.spec.bars'
Dec  2 10:44:46.911: INFO: stderr: ""
Dec  2 10:44:46.911: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-121-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec  2 10:44:46.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4620 explain e2e-test-crd-publish-openapi-121-crds.spec.bars2'
Dec  2 10:44:47.295: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:44:57.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4620" for this suite.

• [SLOW TEST:37.222 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":224,"skipped":3976,"failed":0}
SS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:44:58.479: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context
W1202 10:44:58.937987      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-2229", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-2229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec  2 10:44:59.217: INFO: Waiting up to 5m0s for pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9" in namespace "security-context-2229" to be "Succeeded or Failed"
Dec  2 10:44:59.257: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.403928ms
Dec  2 10:45:01.270: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052818022s
Dec  2 10:45:03.278: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061340395s
Dec  2 10:45:05.305: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088248802s
Dec  2 10:45:07.318: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100459633s
Dec  2 10:45:09.331: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113805842s
Dec  2 10:45:11.346: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.128477804s
STEP: Saw pod success
Dec  2 10:45:11.346: INFO: Pod "security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9" satisfied condition "Succeeded or Failed"
Dec  2 10:45:11.355: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9 container test-container: <nil>
STEP: delete the pod
Dec  2 10:45:11.447: INFO: Waiting for pod security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9 to disappear
Dec  2 10:45:11.464: INFO: Pod security-context-1f169c6a-b02b-4897-b590-2e2a82afb1a9 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:45:11.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2229" for this suite.

• [SLOW TEST:13.447 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":225,"skipped":3978,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:45:11.926: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename endpointslicemirroring
W1202 10:45:12.579362      23 warnings.go:70] No static IP address has been configured for the namespace "endpointslicemirroring-2724", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-2724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Dec  2 10:45:13.356: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Dec  2 10:45:15.391: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Dec  2 10:45:17.427: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:45:19.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2724" for this suite.

• [SLOW TEST:8.012 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":226,"skipped":3993,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:45:19.951: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 10:45:20.585394      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-3579", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:45:20.790: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  2 10:45:20.810: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  2 10:45:25.820: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  2 10:45:47.838: INFO: Creating deployment "test-rolling-update-deployment"
Dec  2 10:45:47.850: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  2 10:45:47.865: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  2 10:45:49.884: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  2 10:45:49.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:45:51.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:45:53.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:45:55.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:45:57.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:45:59.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:46:01.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574748, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-54955647cf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:46:03.902: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 10:46:03.930: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3579  505c565a-72d4-495d-8db8-9662ed1763ef 52201433 1 2022-12-02 10:45:48 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-12-02 10:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:46:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b962f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-02 10:45:48 +0000 UTC,LastTransitionTime:2022-12-02 10:45:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-54955647cf" has successfully progressed.,LastUpdateTime:2022-12-02 10:46:03 +0000 UTC,LastTransitionTime:2022-12-02 10:45:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  2 10:46:03.940: INFO: New ReplicaSet "test-rolling-update-deployment-54955647cf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-54955647cf  deployment-3579  b3497c11-50fd-4d81-99ed-25c75e614e08 52201420 1 2022-12-02 10:45:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:54955647cf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 505c565a-72d4-495d-8db8-9662ed1763ef 0xc006b52377 0xc006b52378}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"505c565a-72d4-495d-8db8-9662ed1763ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:46:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 54955647cf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:54955647cf] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b52428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:46:03.940: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  2 10:46:03.940: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3579  7d384945-2960-415a-8000-4dadd1688da0 52201432 2 2022-12-02 10:45:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 505c565a-72d4-495d-8db8-9662ed1763ef 0xc006b52247 0xc006b52248}] []  [{e2e.test Update apps/v1 2022-12-02 10:45:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:46:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"505c565a-72d4-495d-8db8-9662ed1763ef\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:46:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006b52308 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:46:03.948: INFO: Pod "test-rolling-update-deployment-54955647cf-8898r" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-54955647cf-8898r test-rolling-update-deployment-54955647cf- deployment-3579  814e1999-d858-4108-afa4-20c0937a50ac 52201419 0 2022-12-02 10:45:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:54955647cf] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-54955647cf b3497c11-50fd-4d81-99ed-25c75e614e08 0xc006b20d97 0xc006b20d98}] []  [{kube-controller-manager Update v1 2022-12-02 10:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3497c11-50fd-4d81-99ed-25c75e614e08\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:46:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfhxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfhxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:45:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.3,StartTime:2022-12-02 10:45:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:46:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:docker://sha256:a05bd3a9140b72c5a17eb6881a75c5003b270c0b3e32a995fb10ec96004279d2,ContainerID:docker://5e322d72f811a61e12bcdb3c80f05aaaf4c579d0c6fa7034438af68f149e3dfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:46:03.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3579" for this suite.

• [SLOW TEST:44.502 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":227,"skipped":3993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:46:04.451: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pod-network-test
W1202 10:46:04.960506      23 warnings.go:70] No static IP address has been configured for the namespace "pod-network-test-1612", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1612
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  2 10:46:05.144: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  2 10:46:05.253: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:46:07.263: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:46:09.264: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:46:11.269: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:13.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:15.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:17.267: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:19.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:21.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:23.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:25.269: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  2 10:46:27.265: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  2 10:46:27.288: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:29.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:31.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:33.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:35.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:37.300: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:39.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:41.301: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec  2 10:46:43.302: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  2 10:46:43.318: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  2 10:46:47.382: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  2 10:46:47.382: INFO: Breadth first check of 11.34.26.2 on host 11.0.95.7...
Dec  2 10:46:47.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.26.5:9080/dial?request=hostname&protocol=http&host=11.34.26.2&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:46:47.389: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:46:47.517: INFO: Waiting for responses: map[]
Dec  2 10:46:47.517: INFO: reached 11.34.26.2 after 0/1 tries
Dec  2 10:46:47.517: INFO: Breadth first check of 11.34.26.3 on host 11.0.95.6...
Dec  2 10:46:47.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.26.5:9080/dial?request=hostname&protocol=http&host=11.34.26.3&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:46:47.525: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:46:47.642: INFO: Waiting for responses: map[]
Dec  2 10:46:47.643: INFO: reached 11.34.26.3 after 0/1 tries
Dec  2 10:46:47.643: INFO: Breadth first check of 11.34.26.4 on host 11.0.95.5...
Dec  2 10:46:47.652: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://11.34.26.5:9080/dial?request=hostname&protocol=http&host=11.34.26.4&port=8083&tries=1'] Namespace:pod-network-test-1612 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:46:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:46:47.764: INFO: Waiting for responses: map[]
Dec  2 10:46:47.764: INFO: reached 11.34.26.4 after 0/1 tries
Dec  2 10:46:47.764: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:46:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1612" for this suite.

• [SLOW TEST:43.822 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4032,"failed":0}
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:46:48.274: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 10:46:48.749981      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-5023", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Dec  2 10:46:48.990: INFO: Creating simple deployment test-deployment-c2dkn
Dec  2 10:46:49.018: INFO: deployment "test-deployment-c2dkn" doesn't have the required revision set
Dec  2 10:46:51.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574809, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574809, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574810, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574809, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-c2dkn-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Dec  2 10:46:53.078: INFO: Deployment test-deployment-c2dkn has Conditions: [{Available True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2dkn-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Dec  2 10:46:53.102: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574813, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574813, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574813, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805574809, loc:(*time.Location)(0xa04d060)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-c2dkn-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Dec  2 10:46:53.107: INFO: Observed &Deployment event: ADDED
Dec  2 10:46:53.107: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2dkn-794dd694d8"}
Dec  2 10:46:53.108: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.108: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2dkn-794dd694d8"}
Dec  2 10:46:53.108: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  2 10:46:53.108: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.109: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  2 10:46:53.109: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:50 +0000 UTC 2022-12-02 10:46:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-c2dkn-794dd694d8" is progressing.}
Dec  2 10:46:53.110: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.110: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  2 10:46:53.110: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2dkn-794dd694d8" has successfully progressed.}
Dec  2 10:46:53.111: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.111: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  2 10:46:53.111: INFO: Observed Deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2dkn-794dd694d8" has successfully progressed.}
Dec  2 10:46:53.111: INFO: Found Deployment test-deployment-c2dkn in namespace deployment-5023 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  2 10:46:53.111: INFO: Deployment test-deployment-c2dkn has an updated status
STEP: patching the Statefulset Status
Dec  2 10:46:53.112: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  2 10:46:53.126: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Dec  2 10:46:53.130: INFO: Observed &Deployment event: ADDED
Dec  2 10:46:53.130: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2dkn-794dd694d8"}
Dec  2 10:46:53.130: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.130: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-c2dkn-794dd694d8"}
Dec  2 10:46:53.130: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  2 10:46:53.131: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.131: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-02 10:46:49 +0000 UTC 2022-12-02 10:46:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  2 10:46:53.132: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:50 +0000 UTC 2022-12-02 10:46:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-c2dkn-794dd694d8" is progressing.}
Dec  2 10:46:53.133: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.133: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  2 10:46:53.133: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2dkn-794dd694d8" has successfully progressed.}
Dec  2 10:46:53.134: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.134: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  2 10:46:53.134: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-02 10:46:53 +0000 UTC 2022-12-02 10:46:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-c2dkn-794dd694d8" has successfully progressed.}
Dec  2 10:46:53.134: INFO: Observed deployment test-deployment-c2dkn in namespace deployment-5023 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  2 10:46:53.135: INFO: Observed &Deployment event: MODIFIED
Dec  2 10:46:53.135: INFO: Found deployment test-deployment-c2dkn in namespace deployment-5023 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec  2 10:46:53.135: INFO: Deployment test-deployment-c2dkn has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 10:46:53.144: INFO: Deployment "test-deployment-c2dkn":
&Deployment{ObjectMeta:{test-deployment-c2dkn  deployment-5023  af7c5f98-b995-4f00-a294-f2f3536232bf 52201834 1 2022-12-02 10:46:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-12-02 10:46:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-12-02 10:46:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-12-02 10:46:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f3d968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-c2dkn-794dd694d8",LastUpdateTime:2022-12-02 10:46:54 +0000 UTC,LastTransitionTime:2022-12-02 10:46:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  2 10:46:53.150: INFO: New ReplicaSet "test-deployment-c2dkn-794dd694d8" of Deployment "test-deployment-c2dkn":
&ReplicaSet{ObjectMeta:{test-deployment-c2dkn-794dd694d8  deployment-5023  55f64f8d-2d82-4339-b702-9a48ecc4dbbb 52201830 1 2022-12-02 10:46:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-c2dkn af7c5f98-b995-4f00-a294-f2f3536232bf 0xc00707c0b7 0xc00707c0b8}] []  [{kube-controller-manager Update apps/v1 2022-12-02 10:46:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af7c5f98-b995-4f00-a294-f2f3536232bf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 10:46:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00707c1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  2 10:46:53.161: INFO: Pod "test-deployment-c2dkn-794dd694d8-s5fb4" is available:
&Pod{ObjectMeta:{test-deployment-c2dkn-794dd694d8-s5fb4 test-deployment-c2dkn-794dd694d8- deployment-5023  f9ba7476-8947-4036-8745-f75ae102faf2 52201829 0 2022-12-02 10:46:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-c2dkn-794dd694d8 55f64f8d-2d82-4339-b702-9a48ecc4dbbb 0xc00707c9f7 0xc00707c9f8}] []  [{kube-controller-manager Update v1 2022-12-02 10:46:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55f64f8d-2d82-4339-b702-9a48ecc4dbbb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 10:46:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s562g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s562g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 10:46:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.2,StartTime:2022-12-02 10:46:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 10:46:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://c4605c7c9711359e2e51f1832c3384347fcc15e71179f6b026a59a0c90f4792f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:46:53.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5023" for this suite.

• [SLOW TEST:5.325 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":229,"skipped":4032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:46:53.606: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:46:54.036236      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-1236", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-40d39626-d1fd-486e-8d0e-982036b1c8e0
STEP: Creating a pod to test consume configMaps
Dec  2 10:46:54.259: INFO: Waiting up to 5m0s for pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203" in namespace "configmap-1236" to be "Succeeded or Failed"
Dec  2 10:46:54.265: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221362ms
Dec  2 10:46:56.277: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018296565s
Dec  2 10:46:58.291: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032542965s
Dec  2 10:47:00.303: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044168686s
Dec  2 10:47:02.313: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054276033s
STEP: Saw pod success
Dec  2 10:47:02.313: INFO: Pod "pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203" satisfied condition "Succeeded or Failed"
Dec  2 10:47:02.322: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  2 10:47:02.388: INFO: Waiting for pod pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203 to disappear
Dec  2 10:47:02.394: INFO: Pod pod-configmaps-4548fa56-980b-471e-b3b0-b9c8ebd33203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:47:02.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1236" for this suite.

• [SLOW TEST:9.362 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4056,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:47:02.968: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 10:47:03.808698      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-9497", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Dec  2 10:47:03.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec  2 10:47:04.098: INFO: stderr: ""
Dec  2 10:47:04.098: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Dec  2 10:47:04.098: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec  2 10:47:04.098: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9497" to be "running and ready, or succeeded"
Dec  2 10:47:04.105: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114514ms
Dec  2 10:47:06.121: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023126309s
Dec  2 10:47:08.136: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038075205s
Dec  2 10:47:10.145: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.046203116s
Dec  2 10:47:10.145: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec  2 10:47:10.145: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec  2 10:47:10.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator'
Dec  2 10:47:10.252: INFO: stderr: ""
Dec  2 10:47:10.252: INFO: stdout: "I1202 10:47:09.758292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bcsh 331\nI1202 10:47:09.958462       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/h56 287\nI1202 10:47:10.159037       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/2xn4 212\nI1202 10:47:10.358402       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m72r 410\nI1202 10:47:10.558572       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/j7rk 351\nI1202 10:47:10.758952       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/7g9 357\nI1202 10:47:10.958356       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/grh 287\nI1202 10:47:11.158831       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mj8 307\n"
STEP: limiting log lines
Dec  2 10:47:10.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator --tail=1'
Dec  2 10:47:10.351: INFO: stderr: ""
Dec  2 10:47:10.351: INFO: stdout: "I1202 10:47:11.158831       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mj8 307\n"
Dec  2 10:47:10.351: INFO: got output "I1202 10:47:11.158831       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mj8 307\n"
STEP: limiting log bytes
Dec  2 10:47:10.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator --limit-bytes=1'
Dec  2 10:47:10.455: INFO: stderr: ""
Dec  2 10:47:10.455: INFO: stdout: "I"
Dec  2 10:47:10.455: INFO: got output "I"
STEP: exposing timestamps
Dec  2 10:47:10.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator --tail=1 --timestamps'
Dec  2 10:47:10.570: INFO: stderr: ""
Dec  2 10:47:10.570: INFO: stdout: "2022-12-02T10:47:11.358948616Z I1202 10:47:11.358690       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/874 288\n"
Dec  2 10:47:10.570: INFO: got output "2022-12-02T10:47:11.358948616Z I1202 10:47:11.358690       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/874 288\n"
STEP: restricting to a time range
Dec  2 10:47:13.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator --since=1s'
Dec  2 10:47:13.182: INFO: stderr: ""
Dec  2 10:47:13.182: INFO: stdout: "I1202 10:47:13.159095       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/6lhc 564\nI1202 10:47:13.358411       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/2zgq 290\nI1202 10:47:13.558784       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/97w 368\nI1202 10:47:13.759182       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/fsn4 569\nI1202 10:47:13.958470       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/fltn 491\n"
Dec  2 10:47:13.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 logs logs-generator logs-generator --since=24h'
Dec  2 10:47:13.299: INFO: stderr: ""
Dec  2 10:47:13.299: INFO: stdout: "I1202 10:47:09.758292       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/bcsh 331\nI1202 10:47:09.958462       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/h56 287\nI1202 10:47:10.159037       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/2xn4 212\nI1202 10:47:10.358402       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m72r 410\nI1202 10:47:10.558572       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/j7rk 351\nI1202 10:47:10.758952       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/7g9 357\nI1202 10:47:10.958356       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/grh 287\nI1202 10:47:11.158831       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/mj8 307\nI1202 10:47:11.358690       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/874 288\nI1202 10:47:11.558953       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wm52 397\nI1202 10:47:11.759356       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/swb 251\nI1202 10:47:11.958774       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/7kj 418\nI1202 10:47:12.159212       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/pgp 253\nI1202 10:47:12.358553       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/lzj 407\nI1202 10:47:12.558928       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/wrcq 299\nI1202 10:47:12.759309       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/7ldl 360\nI1202 10:47:12.958695       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/7nb 307\nI1202 10:47:13.159095       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/6lhc 564\nI1202 10:47:13.358411       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/2zgq 290\nI1202 10:47:13.558784       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/97w 368\nI1202 10:47:13.759182       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/fsn4 569\nI1202 10:47:13.958470       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/fltn 491\nI1202 10:47:14.158870       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/7br9 270\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Dec  2 10:47:13.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-9497 delete pod logs-generator'
Dec  2 10:47:15.526: INFO: stderr: ""
Dec  2 10:47:15.526: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:47:15.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9497" for this suite.

• [SLOW TEST:13.029 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":231,"skipped":4073,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:47:15.997: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename cronjob
W1202 10:47:16.529881      23 warnings.go:70] No static IP address has been configured for the namespace "cronjob-525", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:52:16.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-525" for this suite.

• [SLOW TEST:301.264 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":232,"skipped":4075,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:52:17.262: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 10:52:17.769274      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-7580", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-598da235-0caf-4612-8e17-86f25b122b64
STEP: Creating a pod to test consume configMaps
Dec  2 10:52:17.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb" in namespace "configmap-7580" to be "Succeeded or Failed"
Dec  2 10:52:17.974: INFO: Pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.805793ms
Dec  2 10:52:19.985: INFO: Pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018490804s
Dec  2 10:52:21.993: INFO: Pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026496292s
Dec  2 10:52:24.000: INFO: Pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03443096s
STEP: Saw pod success
Dec  2 10:52:24.001: INFO: Pod "pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb" satisfied condition "Succeeded or Failed"
Dec  2 10:52:24.010: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:52:24.070: INFO: Waiting for pod pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb to disappear
Dec  2 10:52:24.076: INFO: Pod pod-configmaps-afd77d92-c598-435d-8a0c-539abfa88fbb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:52:24.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7580" for this suite.

• [SLOW TEST:7.416 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4090,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:52:24.678: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 10:52:25.583778      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7294", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 10:52:26.276843      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7294", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 10:52:26.861734      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7294-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 10:52:27.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  2 10:52:29.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 10:52:31.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575148, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 10:52:34.772: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:52:34.781: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:52:43.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7294" for this suite.
STEP: Destroying namespace "webhook-7294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:20.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":234,"skipped":4097,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:52:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubelet-test
W1202 10:52:45.179984      23 warnings.go:70] No static IP address has been configured for the namespace "kubelet-test-2988", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:52:45.412: INFO: The status of Pod busybox-host-aliases0166d632-c667-4e05-a34b-1b7f2acb326f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:52:47.422: INFO: The status of Pod busybox-host-aliases0166d632-c667-4e05-a34b-1b7f2acb326f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:52:49.424: INFO: The status of Pod busybox-host-aliases0166d632-c667-4e05-a34b-1b7f2acb326f is Pending, waiting for it to be Running (with Ready = true)
Dec  2 10:52:51.423: INFO: The status of Pod busybox-host-aliases0166d632-c667-4e05-a34b-1b7f2acb326f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:52:51.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2988" for this suite.

• [SLOW TEST:7.243 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4099,"failed":0}
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:52:51.943: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 10:52:52.518903      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-8819", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5805c3a7-5197-4130-b6e6-adf4ded3d1b2
STEP: Creating a pod to test consume secrets
Dec  2 10:52:52.731: INFO: Waiting up to 5m0s for pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df" in namespace "secrets-8819" to be "Succeeded or Failed"
Dec  2 10:52:52.737: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.218036ms
Dec  2 10:52:54.747: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015976104s
Dec  2 10:52:56.755: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023797588s
Dec  2 10:52:58.762: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031367023s
Dec  2 10:53:00.776: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04528053s
STEP: Saw pod success
Dec  2 10:53:00.776: INFO: Pod "pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df" satisfied condition "Succeeded or Failed"
Dec  2 10:53:00.784: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 10:53:00.836: INFO: Waiting for pod pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df to disappear
Dec  2 10:53:00.842: INFO: Pod pod-secrets-59cad1a0-97ee-44bf-925c-82dc2a23e5df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:53:00.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8819" for this suite.

• [SLOW TEST:9.385 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4099,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:53:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 10:53:01.849428      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-8922", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 10:53:02.530720      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-8922", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 10:53:02.571: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  2 10:53:02.592: INFO: Number of nodes with available pods: 0
Dec  2 10:53:02.593: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  2 10:53:02.642: INFO: Number of nodes with available pods: 0
Dec  2 10:53:02.642: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:03.652: INFO: Number of nodes with available pods: 0
Dec  2 10:53:03.652: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:04.651: INFO: Number of nodes with available pods: 0
Dec  2 10:53:04.651: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:05.657: INFO: Number of nodes with available pods: 0
Dec  2 10:53:05.657: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:06.654: INFO: Number of nodes with available pods: 0
Dec  2 10:53:06.654: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:07.650: INFO: Number of nodes with available pods: 0
Dec  2 10:53:07.650: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:08.653: INFO: Number of nodes with available pods: 0
Dec  2 10:53:08.654: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:09.652: INFO: Number of nodes with available pods: 0
Dec  2 10:53:09.652: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:10.651: INFO: Number of nodes with available pods: 0
Dec  2 10:53:10.651: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:11.650: INFO: Number of nodes with available pods: 0
Dec  2 10:53:11.650: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:12.653: INFO: Number of nodes with available pods: 0
Dec  2 10:53:12.653: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:13.653: INFO: Number of nodes with available pods: 0
Dec  2 10:53:13.653: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:14.654: INFO: Number of nodes with available pods: 0
Dec  2 10:53:14.655: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:15.651: INFO: Number of nodes with available pods: 1
Dec  2 10:53:15.651: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  2 10:53:15.700: INFO: Number of nodes with available pods: 1
Dec  2 10:53:15.700: INFO: Number of running nodes: 0, number of available pods: 1
Dec  2 10:53:16.711: INFO: Number of nodes with available pods: 0
Dec  2 10:53:16.711: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  2 10:53:16.739: INFO: Number of nodes with available pods: 0
Dec  2 10:53:16.739: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:17.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:17.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:18.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:18.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:19.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:19.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:20.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:20.750: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:21.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:21.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:22.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:22.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:23.747: INFO: Number of nodes with available pods: 0
Dec  2 10:53:23.747: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:24.747: INFO: Number of nodes with available pods: 0
Dec  2 10:53:24.747: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:25.748: INFO: Number of nodes with available pods: 0
Dec  2 10:53:25.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:26.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:26.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:27.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:27.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:28.747: INFO: Number of nodes with available pods: 0
Dec  2 10:53:28.747: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:29.748: INFO: Number of nodes with available pods: 0
Dec  2 10:53:29.748: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:30.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:30.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:31.750: INFO: Number of nodes with available pods: 0
Dec  2 10:53:31.750: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:32.750: INFO: Number of nodes with available pods: 0
Dec  2 10:53:32.750: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:33.746: INFO: Number of nodes with available pods: 0
Dec  2 10:53:33.746: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:34.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:34.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:35.750: INFO: Number of nodes with available pods: 0
Dec  2 10:53:35.750: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:36.748: INFO: Number of nodes with available pods: 0
Dec  2 10:53:36.748: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:37.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:37.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:38.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:38.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:39.746: INFO: Number of nodes with available pods: 0
Dec  2 10:53:39.746: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:40.750: INFO: Number of nodes with available pods: 0
Dec  2 10:53:40.750: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:41.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:41.749: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:42.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:42.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:43.751: INFO: Number of nodes with available pods: 0
Dec  2 10:53:43.751: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:44.748: INFO: Number of nodes with available pods: 0
Dec  2 10:53:44.748: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 10:53:45.749: INFO: Number of nodes with available pods: 1
Dec  2 10:53:45.749: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8922, will wait for the garbage collector to delete the pods
Dec  2 10:53:45.840: INFO: Deleting DaemonSet.extensions daemon-set took: 14.252976ms
Dec  2 10:53:45.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.65716ms
Dec  2 10:53:49.749: INFO: Number of nodes with available pods: 0
Dec  2 10:53:49.749: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 10:53:49.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52204518"},"items":null}

Dec  2 10:53:49.765: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52204518"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:53:49.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8922" for this suite.

• [SLOW TEST:49.085 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":237,"skipped":4108,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:53:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 10:53:50.888897      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-256", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:54:02.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-256" for this suite.

• [SLOW TEST:12.306 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":238,"skipped":4109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:54:02.729: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 10:54:03.251807      23 warnings.go:70] No static IP address has been configured for the namespace "services-6711", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:54:03.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6711" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":239,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:54:04.118: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename limitrange
W1202 10:54:04.572478      23 warnings.go:70] No static IP address has been configured for the namespace "limitrange-663", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec  2 10:54:04.765: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec  2 10:54:04.782: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  2 10:54:04.782: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec  2 10:54:04.816: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  2 10:54:04.816: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec  2 10:54:04.849: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec  2 10:54:04.849: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec  2 10:54:11.936: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:54:11.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-663" for this suite.

• [SLOW TEST:8.353 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":240,"skipped":4153,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:54:12.471: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 10:54:12.965467      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-4612", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4612
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec  2 10:54:19.183: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4612 PodName:pod-sharedvolume-73102a53-7784-4020-969d-63a8a454ef57 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 10:54:19.183: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 10:54:19.289: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:54:19.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4612" for this suite.

• [SLOW TEST:7.359 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":241,"skipped":4158,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:54:19.832: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 10:54:20.283352      23 warnings.go:70] No static IP address has been configured for the namespace "projected-4042", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-1d8a1842-9853-40fc-b70d-a651365f977f
STEP: Creating a pod to test consume configMaps
Dec  2 10:54:20.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf" in namespace "projected-4042" to be "Succeeded or Failed"
Dec  2 10:54:20.510: INFO: Pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.977281ms
Dec  2 10:54:22.524: INFO: Pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023819173s
Dec  2 10:54:24.538: INFO: Pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038183466s
Dec  2 10:54:26.550: INFO: Pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049693818s
STEP: Saw pod success
Dec  2 10:54:26.550: INFO: Pod "pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf" satisfied condition "Succeeded or Failed"
Dec  2 10:54:26.558: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf container agnhost-container: <nil>
STEP: delete the pod
Dec  2 10:54:26.603: INFO: Waiting for pod pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf to disappear
Dec  2 10:54:26.612: INFO: Pod pod-projected-configmaps-95f16099-644c-4b82-ac25-3fad53e923bf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:54:26.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4042" for this suite.

• [SLOW TEST:7.283 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:54:27.122: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir-wrapper
W1202 10:54:27.626520      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-wrapper-856", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  2 10:54:28.349: INFO: Pod name wrapped-volume-race-34b757ea-b88d-47ea-8293-fd1b41b49f29: Found 0 pods out of 5
Dec  2 10:54:33.366: INFO: Pod name wrapped-volume-race-34b757ea-b88d-47ea-8293-fd1b41b49f29: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-34b757ea-b88d-47ea-8293-fd1b41b49f29 in namespace emptydir-wrapper-856, will wait for the garbage collector to delete the pods
Dec  2 10:55:29.502: INFO: Deleting ReplicationController wrapped-volume-race-34b757ea-b88d-47ea-8293-fd1b41b49f29 took: 19.832421ms
Dec  2 10:55:29.603: INFO: Terminating ReplicationController wrapped-volume-race-34b757ea-b88d-47ea-8293-fd1b41b49f29 pods took: 100.368944ms
STEP: Creating RC which spawns configmap-volume pods
Dec  2 10:55:51.852: INFO: Pod name wrapped-volume-race-e72fa7d0-d5e9-47ce-aff9-790f4314e594: Found 0 pods out of 5
Dec  2 10:55:56.876: INFO: Pod name wrapped-volume-race-e72fa7d0-d5e9-47ce-aff9-790f4314e594: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e72fa7d0-d5e9-47ce-aff9-790f4314e594 in namespace emptydir-wrapper-856, will wait for the garbage collector to delete the pods
Dec  2 10:56:09.012: INFO: Deleting ReplicationController wrapped-volume-race-e72fa7d0-d5e9-47ce-aff9-790f4314e594 took: 20.713146ms
Dec  2 10:56:09.113: INFO: Terminating ReplicationController wrapped-volume-race-e72fa7d0-d5e9-47ce-aff9-790f4314e594 pods took: 101.027559ms
STEP: Creating RC which spawns configmap-volume pods
Dec  2 10:56:12.945: INFO: Pod name wrapped-volume-race-ea289a4b-ee3e-4059-91bc-8f58302cd0e1: Found 0 pods out of 5
Dec  2 10:56:17.968: INFO: Pod name wrapped-volume-race-ea289a4b-ee3e-4059-91bc-8f58302cd0e1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ea289a4b-ee3e-4059-91bc-8f58302cd0e1 in namespace emptydir-wrapper-856, will wait for the garbage collector to delete the pods
Dec  2 10:56:30.109: INFO: Deleting ReplicationController wrapped-volume-race-ea289a4b-ee3e-4059-91bc-8f58302cd0e1 took: 14.969328ms
Dec  2 10:56:30.209: INFO: Terminating ReplicationController wrapped-volume-race-ea289a4b-ee3e-4059-91bc-8f58302cd0e1 pods took: 100.520252ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:56:35.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-856" for this suite.

• [SLOW TEST:128.700 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":243,"skipped":4238,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:56:35.822: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename cronjob
W1202 10:56:36.336309      23 warnings.go:70] No static IP address has been configured for the namespace "cronjob-8376", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 10:58:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8376" for this suite.

• [SLOW TEST:85.284 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":244,"skipped":4239,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 10:58:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 10:58:01.610865      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-3252", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-6f140439-91a5-4988-8c50-bc5dd251725f in namespace container-probe-3252
Dec  2 10:58:05.871: INFO: Started pod test-webserver-6f140439-91a5-4988-8c50-bc5dd251725f in namespace container-probe-3252
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 10:58:05.881: INFO: Initial restart count of pod test-webserver-6f140439-91a5-4988-8c50-bc5dd251725f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:02:07.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3252" for this suite.

• [SLOW TEST:246.789 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:02:07.899: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:02:08.376438      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2504", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:02:09.000248      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2504", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:02:09.460730      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2504-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:02:10.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 11:02:12.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575730, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575730, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805575730, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:02:15.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec  2 11:02:15.563: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:02:15.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2504" for this suite.
STEP: Destroying namespace "webhook-2504-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.190 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":246,"skipped":4282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:02:17.093: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
W1202 11:02:17.581081      23 warnings.go:70] No static IP address has been configured for the namespace "e2e-kubelet-etc-hosts-7876", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Dec  2 11:02:17.790: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:02:19.801: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:02:21.801: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:02:23.800: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Dec  2 11:02:23.833: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:02:25.841: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  2 11:02:25.852: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:25.979: INFO: Exec stderr: ""
Dec  2 11:02:25.979: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:25.979: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.122: INFO: Exec stderr: ""
Dec  2 11:02:26.122: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.122: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.242: INFO: Exec stderr: ""
Dec  2 11:02:26.242: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.242: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.361: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  2 11:02:26.361: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.361: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.475: INFO: Exec stderr: ""
Dec  2 11:02:26.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.475: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.587: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  2 11:02:26.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.587: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.715: INFO: Exec stderr: ""
Dec  2 11:02:26.715: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.836: INFO: Exec stderr: ""
Dec  2 11:02:26.836: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.836: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:26.961: INFO: Exec stderr: ""
Dec  2 11:02:26.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7876 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:02:26.961: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:02:27.084: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:02:27.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7876" for this suite.

• [SLOW TEST:10.571 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:02:27.664: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context-test
W1202 11:02:28.288550      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-test-1661", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:02:28.495: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526" in namespace "security-context-test-1661" to be "Succeeded or Failed"
Dec  2 11:02:28.503: INFO: Pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526": Phase="Pending", Reason="", readiness=false. Elapsed: 7.767716ms
Dec  2 11:02:30.513: INFO: Pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017642537s
Dec  2 11:02:32.524: INFO: Pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028899542s
Dec  2 11:02:34.536: INFO: Pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040513112s
Dec  2 11:02:34.536: INFO: Pod "alpine-nnp-false-97874a3c-bd79-44e8-a8dd-18ada94de526" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:02:34.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1661" for this suite.

• [SLOW TEST:7.352 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4365,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:02:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename namespaces
W1202 11:02:35.557358      23 warnings.go:70] No static IP address has been configured for the namespace "namespaces-5801", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
W1202 11:02:36.197135      23 warnings.go:70] No static IP address has been configured for the namespace "nspatchtest-6d18bc89-5a9c-4dea-82c8-5ddca31f93aa-3459", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-6d18bc89-5a9c-4dea-82c8-5ddca31f93aa-3459
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
W1202 11:02:36.877181      23 warnings.go:70] No static IP address has been configured for the namespace "nspatchtest-6d18bc89-5a9c-4dea-82c8-5ddca31f93aa-3459", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:02:36.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5801" for this suite.
STEP: Destroying namespace "nspatchtest-6d18bc89-5a9c-4dea-82c8-5ddca31f93aa-3459" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":249,"skipped":4367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:02:37.907: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 11:02:38.505975      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-3679", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Dec  2 11:04:39.283: INFO: Successfully updated pod "var-expansion-6c2216d4-8e41-4963-bea7-a40b9c5fadb2"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec  2 11:05:07.303: INFO: Deleting pod "var-expansion-6c2216d4-8e41-4963-bea7-a40b9c5fadb2" in namespace "var-expansion-3679"
Dec  2 11:05:07.319: INFO: Wait up to 5m0s for pod "var-expansion-6c2216d4-8e41-4963-bea7-a40b9c5fadb2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:05:41.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3679" for this suite.

• [SLOW TEST:183.964 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":250,"skipped":4403,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:05:41.872: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replication-controller
W1202 11:05:42.387162      23 warnings.go:70] No static IP address has been configured for the namespace "replication-controller-2652", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:05:42.581: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  2 11:05:44.652: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:05:45.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2652" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":251,"skipped":4407,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:05:46.219: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-runtime
W1202 11:05:46.695193      23 warnings.go:70] No static IP address has been configured for the namespace "container-runtime-4457", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:06:24.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4457" for this suite.

• [SLOW TEST:39.038 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4428,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:06:25.257: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 11:06:25.703310      23 warnings.go:70] No static IP address has been configured for the namespace "pods-6095", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:06:25.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6095" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":253,"skipped":4442,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:06:26.549: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 11:06:27.148398      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-454", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 11:06:27.848894      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-454", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:06:27.888: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  2 11:06:27.928: INFO: Number of nodes with available pods: 0
Dec  2 11:06:27.929: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:28.957: INFO: Number of nodes with available pods: 0
Dec  2 11:06:28.957: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:29.956: INFO: Number of nodes with available pods: 0
Dec  2 11:06:29.956: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:30.957: INFO: Number of nodes with available pods: 0
Dec  2 11:06:30.957: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:31.953: INFO: Number of nodes with available pods: 0
Dec  2 11:06:31.953: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:32.959: INFO: Number of nodes with available pods: 0
Dec  2 11:06:32.959: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:33.955: INFO: Number of nodes with available pods: 0
Dec  2 11:06:33.955: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:06:35.075: INFO: Number of nodes with available pods: 1
Dec  2 11:06:35.075: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:35.960: INFO: Number of nodes with available pods: 1
Dec  2 11:06:35.960: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:36.960: INFO: Number of nodes with available pods: 1
Dec  2 11:06:36.960: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:37.955: INFO: Number of nodes with available pods: 1
Dec  2 11:06:37.955: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:38.955: INFO: Number of nodes with available pods: 1
Dec  2 11:06:38.955: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:39.959: INFO: Number of nodes with available pods: 1
Dec  2 11:06:39.959: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:40.948: INFO: Number of nodes with available pods: 2
Dec  2 11:06:40.948: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:41.958: INFO: Number of nodes with available pods: 2
Dec  2 11:06:41.958: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:42.961: INFO: Number of nodes with available pods: 2
Dec  2 11:06:42.961: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:43.954: INFO: Number of nodes with available pods: 2
Dec  2 11:06:43.954: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:44.953: INFO: Number of nodes with available pods: 2
Dec  2 11:06:44.953: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:45.957: INFO: Number of nodes with available pods: 2
Dec  2 11:06:45.957: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:46.954: INFO: Number of nodes with available pods: 2
Dec  2 11:06:46.954: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:47.957: INFO: Number of nodes with available pods: 2
Dec  2 11:06:47.957: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:48.958: INFO: Number of nodes with available pods: 2
Dec  2 11:06:48.958: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:49.953: INFO: Number of nodes with available pods: 2
Dec  2 11:06:49.953: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:06:50.956: INFO: Number of nodes with available pods: 3
Dec  2 11:06:50.956: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  2 11:06:51.031: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:51.031: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:51.031: INFO: Wrong image for pod: daemon-set-tk2g8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:52.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:52.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:53.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:53.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:54.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:54.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:55.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:55.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:56.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:56.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:57.056: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:57.056: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:58.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:58.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:59.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:06:59.052: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:00.074: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:00.075: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:01.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:01.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:02.055: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:02.055: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:03.049: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:03.049: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:04.056: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:04.056: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:05.047: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:05.047: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:06.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:06.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:07.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:07.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:08.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:08.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:09.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:09.052: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:09.052: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:10.049: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:10.049: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:10.049: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:11.057: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:11.057: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:11.058: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:12.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:12.053: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:12.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:13.050: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:13.050: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:13.050: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:14.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:14.054: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:14.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:15.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:15.052: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:15.052: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:16.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:16.054: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:16.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:17.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:17.051: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:17.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:18.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:18.053: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:18.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:19.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:19.054: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:19.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:20.047: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:20.047: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:20.047: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:21.048: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:21.048: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:21.048: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:22.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:22.051: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:22.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:23.050: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:23.050: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:23.050: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:24.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:24.052: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:24.052: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:25.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:25.054: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:25.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:26.052: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:26.052: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:26.052: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:27.050: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:27.050: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:27.050: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:28.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:28.056: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:28.056: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:29.057: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:29.057: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:29.057: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:30.050: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:30.050: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:30.050: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:31.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:31.054: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:31.054: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:32.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:32.051: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:32.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:33.050: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:33.051: INFO: Pod daemon-set-9f2jl is not available
Dec  2 11:07:33.051: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:34.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:34.053: INFO: Wrong image for pod: daemon-set-sktbk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:35.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:36.051: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:36.051: INFO: Pod daemon-set-zcfhc is not available
Dec  2 11:07:37.054: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:37.054: INFO: Pod daemon-set-zcfhc is not available
Dec  2 11:07:38.048: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:38.048: INFO: Pod daemon-set-zcfhc is not available
Dec  2 11:07:39.053: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:39.053: INFO: Pod daemon-set-zcfhc is not available
Dec  2 11:07:40.047: INFO: Wrong image for pod: daemon-set-2lhdg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  2 11:07:40.048: INFO: Pod daemon-set-zcfhc is not available
Dec  2 11:07:53.053: INFO: Pod daemon-set-gxqj9 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  2 11:07:53.089: INFO: Number of nodes with available pods: 2
Dec  2 11:07:53.089: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:54.106: INFO: Number of nodes with available pods: 2
Dec  2 11:07:54.106: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:55.120: INFO: Number of nodes with available pods: 2
Dec  2 11:07:55.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:56.120: INFO: Number of nodes with available pods: 2
Dec  2 11:07:56.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:57.115: INFO: Number of nodes with available pods: 2
Dec  2 11:07:57.115: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:58.118: INFO: Number of nodes with available pods: 2
Dec  2 11:07:58.118: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:07:59.116: INFO: Number of nodes with available pods: 2
Dec  2 11:07:59.116: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:00.171: INFO: Number of nodes with available pods: 2
Dec  2 11:08:00.171: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:01.120: INFO: Number of nodes with available pods: 2
Dec  2 11:08:01.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:02.119: INFO: Number of nodes with available pods: 2
Dec  2 11:08:02.119: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:03.111: INFO: Number of nodes with available pods: 2
Dec  2 11:08:03.111: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:04.120: INFO: Number of nodes with available pods: 2
Dec  2 11:08:04.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:05.120: INFO: Number of nodes with available pods: 2
Dec  2 11:08:05.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:06.115: INFO: Number of nodes with available pods: 2
Dec  2 11:08:06.115: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:07.120: INFO: Number of nodes with available pods: 2
Dec  2 11:08:07.120: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:08.119: INFO: Number of nodes with available pods: 2
Dec  2 11:08:08.119: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:09.114: INFO: Number of nodes with available pods: 2
Dec  2 11:08:09.114: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:10.118: INFO: Number of nodes with available pods: 2
Dec  2 11:08:10.118: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:11.112: INFO: Number of nodes with available pods: 2
Dec  2 11:08:11.112: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:12.118: INFO: Number of nodes with available pods: 2
Dec  2 11:08:12.118: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:13.115: INFO: Number of nodes with available pods: 2
Dec  2 11:08:13.116: INFO: Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa is running more than one daemon pod
Dec  2 11:08:14.119: INFO: Number of nodes with available pods: 3
Dec  2 11:08:14.119: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-454, will wait for the garbage collector to delete the pods
Dec  2 11:08:14.238: INFO: Deleting DaemonSet.extensions daemon-set took: 13.795103ms
Dec  2 11:08:14.338: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.26252ms
Dec  2 11:08:17.851: INFO: Number of nodes with available pods: 0
Dec  2 11:08:17.851: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 11:08:17.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52210941"},"items":null}

Dec  2 11:08:17.869: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52210941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:08:17.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-454" for this suite.

• [SLOW TEST:111.837 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":254,"skipped":4452,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:08:18.387: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename namespaces
W1202 11:08:18.892867      23 warnings.go:70] No static IP address has been configured for the namespace "namespaces-3062", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
W1202 11:08:19.614103      23 warnings.go:70] No static IP address has been configured for the namespace "nsdeletetest-636", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-636
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
W1202 11:08:29.209554      23 warnings.go:70] No static IP address has been configured for the namespace "nsdeletetest-1615", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1615
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:08:29.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3062" for this suite.
STEP: Destroying namespace "nsdeletetest-636" for this suite.
Dec  2 11:08:29.939: INFO: Namespace nsdeletetest-636 was already deleted
STEP: Destroying namespace "nsdeletetest-1615" for this suite.

• [SLOW TEST:12.550 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":255,"skipped":4472,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:08:30.937: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context-test
W1202 11:08:31.463039      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-test-7784", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:08:31.656: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602" in namespace "security-context-test-7784" to be "Succeeded or Failed"
Dec  2 11:08:31.664: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106576ms
Dec  2 11:08:33.676: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019835414s
Dec  2 11:08:35.690: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03373128s
Dec  2 11:08:37.702: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045575607s
Dec  2 11:08:39.709: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.053199743s
Dec  2 11:08:39.709: INFO: Pod "busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:08:39.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7784" for this suite.

• [SLOW TEST:9.297 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:08:40.234: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-pred
W1202 11:08:40.699371      23 warnings.go:70] No static IP address has been configured for the namespace "sched-pred-7044", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  2 11:08:40.887: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  2 11:08:40.920: INFO: Waiting for terminating namespaces to be deleted...
Dec  2 11:08:40.931: INFO: 
Logging pods the apiserver thinks is on node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 before test
Dec  2 11:08:41.030: INFO: coredns-6c4c59f5b4-fgkn9 from kube-system started at 2022-12-02 10:36:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.030: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:08:41.030: INFO: promtail-pfssc from logging started at 2022-12-02 10:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.030: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.030: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:08:41.030: INFO: node-exporter-wh467 from monitoring started at 2022-12-01 00:27:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:08:41.031: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:08:41.031: INFO: fluent-bit-95snl from pks-system started at 2022-12-02 10:36:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:08:41.031: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:08:41.031: INFO: node-exporter-ngrrc from pks-system started at 2022-12-02 10:36:34 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:08:41.031: INFO: telegraf-g9948 from pks-system started at 2022-12-02 10:36:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:08:41.031: INFO: busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602 from security-context-test-7784 started at 2022-12-02 11:08:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container busybox-user-65534-5813fe09-4b8a-4988-8f73-5c764e880602 ready: false, restart count 0
Dec  2 11:08:41.031: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25 from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:08:41.031: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:08:41.031: INFO: restic-pm729 from velero started at 2022-12-02 10:36:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.031: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:08:41.031: INFO: 
Logging pods the apiserver thinks is on node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 before test
Dec  2 11:08:41.126: INFO: argo-server-78c9599f9-phpdk from argo started at 2022-12-01 00:07:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.126: INFO: 	Container argo-server ready: false, restart count 0
Dec  2 11:08:41.126: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.126: INFO: cert-manager-6bdfc96d57-j8xtt from cert-manager started at 2022-12-02 09:50:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.126: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:08:41.129: INFO: code-server-68cdb99985-76lzq from code-server started at 2022-12-01 00:08:06 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container code-server ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-jobservice-67df6d9455-6d4w5 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-trivy-0 from harbor-restored-1 started at 2022-12-01 00:07:25 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-exporter-6fcd6687f5-tg4sq from harbor-restored-100 started at 2022-12-01 00:07:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-jobservice-67df6d9455-nrds9 from harbor-restored-100 started at 2022-12-01 00:07:44 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-registry-566bccf488-cqlc2 from harbor-restored-100 started at 2022-12-01 00:08:12 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.129: INFO: harbor-trivy-0 from harbor-restored-100 started at 2022-12-01 00:10:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.129: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.129: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.131: INFO: harbor-core-66c67cf548-htx9b from harbor-restored-2 started at 2022-12-01 00:08:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.131: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.131: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.131: INFO: harbor-jobservice-67df6d9455-qnhcr from harbor-restored-2 started at 2022-12-01 00:07:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.131: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.131: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.131: INFO: harbor-portal-58c8bb4658-b9ccp from harbor-restored-2 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.131: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.131: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.131: INFO: harbor-registry-566bccf488-2294p from harbor-restored-2 started at 2022-12-01 00:07:48 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.131: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.131: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.131: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.131: INFO: harbor-core-66c67cf548-2srb6 from harbor-restored-600 started at 2022-12-01 00:07:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-exporter-6fcd6687f5-zttc4 from harbor-restored-600 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-portal-58c8bb4658-bf9pv from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-registry-566bccf488-4r7nw from harbor-restored-600 started at 2022-12-01 00:07:31 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-trivy-0 from harbor-restored-600 started at 2022-12-01 00:11:19 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-exporter-6fcd6687f5-g9cgh from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-jobservice-67df6d9455-cr5z2 from harbor-restored-700 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-portal-58c8bb4658-mqpqt from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-trivy-0 from harbor-restored-700 started at 2022-12-01 00:36:30 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-exporter-6fcd6687f5-lph6q from harbor-restored-701 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.134: INFO: harbor-jobservice-67df6d9455-4rj79 from harbor-restored-701 started at 2022-12-01 00:08:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.134: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.136: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.136: INFO: harbor-portal-58c8bb4658-kgnn2 from harbor-restored-701 started at 2022-12-01 00:07:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.136: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.136: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.136: INFO: harbor-trivy-0 from harbor-restored-701 started at 2022-12-01 00:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.136: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.136: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.136: INFO: harbor-jobservice-67df6d9455-mmml7 from harbor-restored-702 started at 2022-12-01 00:07:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.136: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-registry-566bccf488-dw7qz from harbor-restored-702 started at 2022-12-01 00:08:11 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.137: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-core-66c67cf548-4g4zb from harbor-restored-703 started at 2022-12-01 00:08:15 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.137: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-exporter-6fcd6687f5-wrxhf from harbor-restored-703 started at 2022-12-01 00:08:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.137: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-jobservice-67df6d9455-gzs5l from harbor-restored-703 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.137: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-redis-0 from harbor-restored-703 started at 2022-12-01 00:07:50 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.137: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.137: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.137: INFO: harbor-registry-566bccf488-dmrc5 from harbor-restored-703 started at 2022-12-01 00:08:59 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.139: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.139: INFO: harbor-trivy-0 from harbor-restored-703 started at 2022-12-01 00:10:12 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.139: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: harbor-core-66c67cf548-qxk8k from harbor-restored-705 started at 2022-12-01 00:08:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.139: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: harbor-exporter-6fcd6687f5-2vsd9 from harbor-restored-705 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.139: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: harbor-registry-566bccf488-ng244 from harbor-restored-705 started at 2022-12-01 00:09:00 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.139: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.139: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.139: INFO: harbor-trivy-0 from harbor-restored-705 started at 2022-12-01 00:36:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.141: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.141: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.141: INFO: harbor-database-0 from harbor-restored-706 started at 2022-12-01 00:07:40 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.141: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.141: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.141: INFO: harbor-exporter-6fcd6687f5-fqhxs from harbor-restored-706 started at 2022-12-01 00:07:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.141: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.141: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-portal-58c8bb4658-s8d6h from harbor-restored-706 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.142: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-redis-0 from harbor-restored-706 started at 2022-12-01 00:10:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.142: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-registry-566bccf488-fsj6q from harbor-restored-706 started at 2022-12-01 00:08:19 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.142: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-trivy-0 from harbor-restored-706 started at 2022-12-01 00:10:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.142: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-core-66c67cf548-jv9kv from harbor-restored-710 started at 2022-12-01 00:07:49 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.142: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.142: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.142: INFO: harbor-jobservice-67df6d9455-l79z9 from harbor-restored-710 started at 2022-12-01 00:07:22 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.143: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.143: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.143: INFO: harbor-trivy-0 from harbor-restored-710 started at 2022-12-01 00:08:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.143: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.143: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.144: INFO: harbor-database-0 from harbor started at 2022-12-01 00:06:05 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.144: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.144: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.144: INFO: harbor-exporter-6fcd6687f5-m98lw from harbor started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.144: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.144: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.144: INFO: harbor-jobservice-67df6d9455-bp2t7 from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.144: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.144: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.144: INFO: harbor-registry-566bccf488-bcjg8 from harbor started at 2022-12-01 00:08:08 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.144: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.144: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.144: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.144: INFO: harbor-trivy-0 from harbor started at 2022-12-01 00:08:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.144: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.145: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.145: INFO: istiod-57c4757749-hs8dc from istio-system started at 2022-12-01 00:48:17 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.145: INFO: 	Container discovery ready: true, restart count 0
Dec  2 11:08:41.145: INFO: kiali-operator-9d8cbb844-rgrkz from istio-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.145: INFO: 	Container operator ready: true, restart count 0
Dec  2 11:08:41.145: INFO: coredns-6c4c59f5b4-djxn2 from kube-system started at 2022-12-01 00:32:29 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.145: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:08:41.145: INFO: metrics-server-757c77bdc5-2ggxp from kube-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.145: INFO: 	Container metrics-server ready: true, restart count 0
Dec  2 11:08:41.145: INFO: logging-operator-56b5fcc8bd-xkzld from logging started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.145: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container logging-operator ready: false, restart count 0
Dec  2 11:08:41.147: INFO: promtail-7pk9g from logging started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.147: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:08:41.147: INFO: alertmanager-main-1 from monitoring started at 2022-12-01 00:09:05 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.147: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.147: INFO: alertmanager-main-2 from monitoring started at 2022-12-02 09:50:38 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.147: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:08:41.147: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.147: INFO: kube-state-metrics-85476668-r7zjx from monitoring started at 2022-12-01 00:07:34 +0000 UTC (4 container statuses recorded)
Dec  2 11:08:41.148: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.148: INFO: 	Container kube-rbac-proxy-main ready: false, restart count 0
Dec  2 11:08:41.148: INFO: 	Container kube-rbac-proxy-self ready: false, restart count 0
Dec  2 11:08:41.148: INFO: 	Container kube-state-metrics ready: false, restart count 0
Dec  2 11:08:41.148: INFO: node-exporter-wzvwt from monitoring started at 2022-11-30 23:36:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.148: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:08:41.148: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:08:41.149: INFO: prometheus-operator-6984ff874-jwfxk from monitoring started at 2022-12-02 09:50:26 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.149: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:08:41.149: INFO: 	Container prometheus-operator ready: false, restart count 0
Dec  2 11:08:41.149: INFO: event-controller-7db654d86f-4l24d from pks-system started at 2022-12-02 09:50:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container event-controller ready: true, restart count 0
Dec  2 11:08:41.149: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:08:41.149: INFO: fluent-bit-lc7xf from pks-system started at 2022-12-02 09:52:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:08:41.149: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:08:41.149: INFO: node-exporter-f8q68 from pks-system started at 2022-12-01 00:33:40 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:08:41.149: INFO: telegraf-4hnb8 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:08:41.149: INFO: validator-5675b6544f-pvrdh from pks-system started at 2022-12-02 09:50:59 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.149: INFO: 	Container validator ready: true, restart count 0
Dec  2 11:08:41.149: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-qb7jc from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.151: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:08:41.151: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:08:41.151: INFO: restic-wq8qp from velero started at 2022-11-30 23:37:03 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.151: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:08:41.151: INFO: 
Logging pods the apiserver thinks is on node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa before test
Dec  2 11:08:41.248: INFO: workflow-controller-c8c847777-chkd8 from argo started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.248: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.248: INFO: 	Container workflow-controller ready: false, restart count 0
Dec  2 11:08:41.248: INFO: cert-manager-cainjector-786868d5bb-9s22p from cert-manager started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.248: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:08:41.248: INFO: cert-manager-webhook-5fd474b69d-xrpgh from cert-manager started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.248: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:08:41.248: INFO: harbor-core-66c67cf548-r2496 from harbor-restored-1 started at 2022-12-01 00:08:14 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.248: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.248: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.248: INFO: harbor-database-0 from harbor-restored-1 started at 2022-12-01 00:07:47 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.248: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.248: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-exporter-6fcd6687f5-9g4ql from harbor-restored-1 started at 2022-12-01 00:09:23 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-portal-58c8bb4658-ng6v7 from harbor-restored-1 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-redis-0 from harbor-restored-1 started at 2022-12-01 00:09:57 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-registry-566bccf488-2twt7 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-core-66c67cf548-wklw5 from harbor-restored-100 started at 2022-12-01 00:09:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-database-0 from harbor-restored-100 started at 2022-12-01 00:08:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-portal-58c8bb4658-rxsk5 from harbor-restored-100 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-redis-0 from harbor-restored-100 started at 2022-12-01 00:08:35 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-database-0 from harbor-restored-2 started at 2022-12-01 00:09:41 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-exporter-6fcd6687f5-l5zmg from harbor-restored-2 started at 2022-12-01 00:08:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-redis-0 from harbor-restored-2 started at 2022-12-01 00:08:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-trivy-0 from harbor-restored-2 started at 2022-12-02 09:50:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-database-0 from harbor-restored-600 started at 2022-12-01 00:09:59 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-jobservice-67df6d9455-zmxln from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-redis-0 from harbor-restored-600 started at 2022-12-01 00:07:51 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-core-66c67cf548-6wt2b from harbor-restored-700 started at 2022-12-01 00:09:17 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-database-0 from harbor-restored-700 started at 2022-12-01 00:10:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-redis-0 from harbor-restored-700 started at 2022-12-01 00:08:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-registry-566bccf488-zzvj7 from harbor-restored-700 started at 2022-12-01 00:08:51 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.249: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.249: INFO: harbor-core-66c67cf548-bnjrn from harbor-restored-701 started at 2022-12-01 00:08:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.249: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-database-0 from harbor-restored-701 started at 2022-12-01 00:09:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-redis-0 from harbor-restored-701 started at 2022-12-01 00:07:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-registry-566bccf488-4scbl from harbor-restored-701 started at 2022-12-01 00:07:53 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-core-66c67cf548-f5fjz from harbor-restored-702 started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-database-0 from harbor-restored-702 started at 2022-12-01 00:10:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-exporter-6fcd6687f5-v5tz8 from harbor-restored-702 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-portal-58c8bb4658-zzshz from harbor-restored-702 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-redis-0 from harbor-restored-702 started at 2022-12-02 09:50:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-trivy-0 from harbor-restored-702 started at 2022-12-02 09:50:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-database-0 from harbor-restored-703 started at 2022-12-01 00:09:09 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-portal-58c8bb4658-7gmp5 from harbor-restored-703 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-database-0 from harbor-restored-705 started at 2022-12-01 00:10:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-jobservice-67df6d9455-4lg98 from harbor-restored-705 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-portal-58c8bb4658-9tfsd from harbor-restored-705 started at 2022-12-01 00:09:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-redis-0 from harbor-restored-705 started at 2022-11-30 19:09:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-core-66c67cf548-wnj7c from harbor-restored-706 started at 2022-12-01 00:08:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-jobservice-67df6d9455-4tzqq from harbor-restored-706 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.250: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.250: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:08:41.250: INFO: harbor-database-0 from harbor-restored-710 started at 2022-12-01 00:08:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container database ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-exporter-6fcd6687f5-zzfwt from harbor-restored-710 started at 2022-12-01 00:08:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-portal-58c8bb4658-htz6g from harbor-restored-710 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-redis-0 from harbor-restored-710 started at 2022-12-01 00:09:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-registry-566bccf488-bzh9n from harbor-restored-710 started at 2022-12-01 00:08:28 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-core-66c67cf548-8bjrq from harbor started at 2022-12-01 00:09:34 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container core ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-portal-58c8bb4658-6x22v from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:08:41.251: INFO: harbor-redis-0 from harbor started at 2022-12-01 00:08:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:08:41.251: INFO: ingress-gateway-management-c6c49cf-2gtg5 from ingress-gateway-management started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: true, restart count 0
Dec  2 11:08:41.251: INFO: istio-operator-67b8b6c9dd-bqlxh from istio-operator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-operator ready: true, restart count 0
Dec  2 11:08:41.251: INFO: keycloak-0 from keycloak started at 2022-11-30 18:59:19 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container keycloak ready: false, restart count 0
Dec  2 11:08:41.251: INFO: keycloak-operator-76c854b9f4-ts46s from keycloak started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container keycloak-operator ready: true, restart count 0
Dec  2 11:08:41.251: INFO: keycloak-postgresql-746d7cc6dd-rzdpf from keycloak started at 2022-11-30 19:10:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container keycloak-postgresql ready: true, restart count 0
Dec  2 11:08:41.251: INFO: docs-67854bcfd4-kfwsh from kube-plus-docs started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container docs ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: coredns-6c4c59f5b4-knwtr from kube-system started at 2022-12-01 00:36:09 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:08:41.251: INFO: replicator-kubernetes-replicator-6774dd4f4d-5t7wv from kubernetes-replicator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container kubernetes-replicator ready: true, restart count 0
Dec  2 11:08:41.251: INFO: loki-0 from logging started at 2022-12-02 09:50:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container loki ready: false, restart count 0
Dec  2 11:08:41.251: INFO: promtail-854st from logging started at 2022-12-01 00:09:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:08:41.251: INFO: alertmanager-main-0 from monitoring started at 2022-12-01 00:07:33 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:08:41.251: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.251: INFO: blackbox-exporter-685b55655c-bdb7h from monitoring started at 2022-12-02 09:50:26 +0000 UTC (4 container statuses recorded)
Dec  2 11:08:41.251: INFO: 	Container blackbox-exporter ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container module-configmap-reloader ready: false, restart count 0
Dec  2 11:08:41.252: INFO: grafana-69d978c595-lqn8f from monitoring started at 2022-12-01 00:09:02 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container grafana ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container grafana-sc-dashboard ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: node-exporter-zn785 from monitoring started at 2022-11-30 18:58:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:08:41.252: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:08:41.252: INFO: prometheus-k8s-0 from monitoring started at 2022-12-02 09:50:58 +0000 UTC (3 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container prometheus ready: false, restart count 0
Dec  2 11:08:41.252: INFO: fluent-bit-w54dc from pks-system started at 2022-12-02 09:52:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:08:41.252: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:08:41.252: INFO: metric-controller-7b78cd6c98-g275p from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container metric-controller ready: true, restart count 0
Dec  2 11:08:41.252: INFO: node-exporter-77jkw from pks-system started at 2022-12-01 00:33:16 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:08:41.252: INFO: observability-manager-fc646c85f-mf72v from pks-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container observability-manager ready: true, restart count 0
Dec  2 11:08:41.252: INFO: sink-controller-78ccbc6f4d-gjph5 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container sink-controller ready: true, restart count 0
Dec  2 11:08:41.252: INFO: telegraf-4w56v from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:08:41.252: INFO: pomerium-58fb8f9f6-g77zd from pomerium started at 2022-12-01 00:56:56 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:08:41.252: INFO: pomerium-66c956fc58-df627 from pomerium started at 2022-12-01 01:00:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:08:41.252: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:08:41.252: INFO: registry-creds-registry-creds-controller-c6879445d-mhvtd from registry-creds started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container controller ready: true, restart count 0
Dec  2 11:08:41.252: INFO: sealed-secrets-controller-856cd994fb-xzcjz from sealed-secrets started at 2022-11-30 18:58:55 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Dec  2 11:08:41.252: INFO: sonobuoy from sonobuoy started at 2022-12-02 09:03:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  2 11:08:41.252: INFO: sonobuoy-e2e-job-4a9382b33c5d4cba from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container e2e ready: true, restart count 0
Dec  2 11:08:41.252: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:08:41.252: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-wtc8k from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:08:41.252: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:08:41.252: INFO: restic-55bfd from velero started at 2022-11-30 18:58:22 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:08:41.252: INFO: velero-69f6b65985-bqq8v from velero started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:08:41.252: INFO: 	Container velero ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-01fa5438-214b-4c9b-9710-2975d52bc187 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-01fa5438-214b-4c9b-9710-2975d52bc187 off the node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-01fa5438-214b-4c9b-9710-2975d52bc187
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:08:53.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7044" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:13.640 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":257,"skipped":4516,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:08:53.875: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:08:54.741059      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6207", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:08:55.453109      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6207", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:08:55.907389      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6207-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:08:56.360: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 11:08:58.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:09:00.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576136, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:09:03.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec  2 11:09:09.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=webhook-6207 attach --namespace=webhook-6207 to-be-attached-pod -i -c=container1'
Dec  2 11:09:11.050: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:09:11.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6207" for this suite.
STEP: Destroying namespace "webhook-6207-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":258,"skipped":4516,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:09:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 11:09:12.999930      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-6743", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Dec  2 11:09:13.214: INFO: Waiting up to 5m0s for pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602" in namespace "var-expansion-6743" to be "Succeeded or Failed"
Dec  2 11:09:13.220: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517939ms
Dec  2 11:09:15.231: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016463142s
Dec  2 11:09:17.240: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026165517s
Dec  2 11:09:19.252: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037697006s
Dec  2 11:09:21.263: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.049263091s
STEP: Saw pod success
Dec  2 11:09:21.263: INFO: Pod "var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602" satisfied condition "Succeeded or Failed"
Dec  2 11:09:21.272: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602 container dapi-container: <nil>
STEP: delete the pod
Dec  2 11:09:21.334: INFO: Waiting for pod var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602 to disappear
Dec  2 11:09:21.342: INFO: Pod var-expansion-b1a39226-e324-40d6-9162-ae08f4e7d602 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:09:21.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6743" for this suite.

• [SLOW TEST:9.340 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":259,"skipped":4526,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:09:21.863: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 11:09:22.335947      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-7604", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:09:22.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-7604 version'
Dec  2 11:09:22.611: INFO: stderr: ""
Dec  2 11:09:22.611: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.15\", GitCommit:\"1d79bc3bcccfba7466c44cc2055d6e7442e140ea\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T12:18:10Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.15+vmware.1\", GitCommit:\"a12824c6657d8e54ec59545bcafd5517bb9db9fd\", GitTreeState:\"clean\", BuildDate:\"2022-09-26T13:06:18Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:09:22.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7604" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":260,"skipped":4529,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:09:23.042: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 11:09:23.478553      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-7912", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7912
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:09:23.676: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  2 11:09:40.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-7912 --namespace=crd-publish-openapi-7912 create -f -'
Dec  2 11:09:43.397: INFO: stderr: ""
Dec  2 11:09:43.397: INFO: stdout: "e2e-test-crd-publish-openapi-4826-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  2 11:09:43.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-7912 --namespace=crd-publish-openapi-7912 delete e2e-test-crd-publish-openapi-4826-crds test-cr'
Dec  2 11:09:43.489: INFO: stderr: ""
Dec  2 11:09:43.489: INFO: stdout: "e2e-test-crd-publish-openapi-4826-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec  2 11:09:43.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-7912 --namespace=crd-publish-openapi-7912 apply -f -'
Dec  2 11:09:43.809: INFO: stderr: ""
Dec  2 11:09:43.809: INFO: stdout: "e2e-test-crd-publish-openapi-4826-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  2 11:09:43.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-7912 --namespace=crd-publish-openapi-7912 delete e2e-test-crd-publish-openapi-4826-crds test-cr'
Dec  2 11:09:43.890: INFO: stderr: ""
Dec  2 11:09:43.890: INFO: stdout: "e2e-test-crd-publish-openapi-4826-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  2 11:09:43.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-7912 explain e2e-test-crd-publish-openapi-4826-crds'
Dec  2 11:09:45.304: INFO: stderr: ""
Dec  2 11:09:45.304: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4826-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:09:55.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7912" for this suite.

• [SLOW TEST:33.313 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":261,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:09:56.356: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename watch
W1202 11:09:56.801213      23 warnings.go:70] No static IP address has been configured for the namespace "watch-9217", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  2 11:09:57.054: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9217  4811a361-20b6-479b-ab50-498cd5869d31 52211829 0 2022-12-02 11:09:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-12-02 11:09:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 11:09:57.055: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9217  4811a361-20b6-479b-ab50-498cd5869d31 52211830 0 2022-12-02 11:09:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-12-02 11:09:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:09:57.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9217" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":262,"skipped":4570,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:09:58.078: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 11:09:58.537570      23 warnings.go:70] No static IP address has been configured for the namespace "gc-6805", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:09:58.853: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"59df7b60-b12f-41a4-86ce-c40a537fc091", Controller:(*bool)(0xc00596713a), BlockOwnerDeletion:(*bool)(0xc00596713b)}}
Dec  2 11:09:58.865: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9a05fa46-75d3-4096-a2da-2c8a09df68b7", Controller:(*bool)(0xc005857382), BlockOwnerDeletion:(*bool)(0xc005857383)}}
Dec  2 11:09:58.892: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0f556ff1-3343-43f8-b01f-10df7b3f91f9", Controller:(*bool)(0xc005857762), BlockOwnerDeletion:(*bool)(0xc005857763)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6805" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":263,"skipped":4575,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:04.536: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 11:10:05.079370      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-9827", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9827
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  2 11:10:05.278: INFO: Waiting up to 5m0s for pod "pod-1adac193-e2b8-4e40-b553-53871198e10c" in namespace "emptydir-9827" to be "Succeeded or Failed"
Dec  2 11:10:05.287: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.953508ms
Dec  2 11:10:07.298: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01955852s
Dec  2 11:10:09.310: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031390214s
Dec  2 11:10:11.323: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044408325s
Dec  2 11:10:13.335: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056789603s
Dec  2 11:10:15.347: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.068347593s
Dec  2 11:10:17.360: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.081697105s
Dec  2 11:10:19.371: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.092755829s
Dec  2 11:10:21.384: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.105940523s
Dec  2 11:10:23.399: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.120364774s
Dec  2 11:10:25.413: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.135192776s
STEP: Saw pod success
Dec  2 11:10:25.414: INFO: Pod "pod-1adac193-e2b8-4e40-b553-53871198e10c" satisfied condition "Succeeded or Failed"
Dec  2 11:10:25.423: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-1adac193-e2b8-4e40-b553-53871198e10c container test-container: <nil>
STEP: delete the pod
Dec  2 11:10:25.478: INFO: Waiting for pod pod-1adac193-e2b8-4e40-b553-53871198e10c to disappear
Dec  2 11:10:25.487: INFO: Pod pod-1adac193-e2b8-4e40-b553-53871198e10c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:25.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9827" for this suite.

• [SLOW TEST:21.616 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4582,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:26.156: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 11:10:26.675039      23 warnings.go:70] No static IP address has been configured for the namespace "gc-5544", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1202 11:10:37.052760      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  2 11:10:37.052: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec  2 11:10:37.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cpkf" in namespace "gc-5544"
Dec  2 11:10:37.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dbdp" in namespace "gc-5544"
Dec  2 11:10:37.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vl4v" in namespace "gc-5544"
Dec  2 11:10:37.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7944" in namespace "gc-5544"
Dec  2 11:10:37.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkfzm" in namespace "gc-5544"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:37.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5544" for this suite.

• [SLOW TEST:11.543 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":265,"skipped":4584,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:37.698: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 11:10:38.149087      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-464", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:38.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-464" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":266,"skipped":4592,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:38.919: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename runtimeclass
W1202 11:10:39.339893      23 warnings.go:70] No static IP address has been configured for the namespace "runtimeclass-2049", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-2049
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec  2 11:10:39.569: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec  2 11:10:39.622: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:39.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2049" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":267,"skipped":4605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:40.223: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 11:10:40.770143      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-4624", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6912f6fa-7d56-4398-9a8c-3637e822bbae
STEP: Creating a pod to test consume configMaps
Dec  2 11:10:40.990: INFO: Waiting up to 5m0s for pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825" in namespace "configmap-4624" to be "Succeeded or Failed"
Dec  2 11:10:40.996: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642443ms
Dec  2 11:10:43.012: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022209451s
Dec  2 11:10:45.042: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051978661s
Dec  2 11:10:47.056: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065617029s
Dec  2 11:10:49.064: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074028601s
Dec  2 11:10:51.079: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088915199s
Dec  2 11:10:53.090: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.100231879s
STEP: Saw pod success
Dec  2 11:10:53.090: INFO: Pod "pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825" satisfied condition "Succeeded or Failed"
Dec  2 11:10:53.099: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 11:10:53.151: INFO: Waiting for pod pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825 to disappear
Dec  2 11:10:53.160: INFO: Pod pod-configmaps-706c7820-10be-47e2-b2c9-926506d2a825 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:10:53.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4624" for this suite.

• [SLOW TEST:13.443 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":4636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:10:53.666: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-runtime
W1202 11:10:54.136564      23 warnings.go:70] No static IP address has been configured for the namespace "container-runtime-2841", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  2 11:11:02.452: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:11:02.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2841" for this suite.

• [SLOW TEST:9.285 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":4676,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:11:02.952: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename containers
W1202 11:11:03.409104      23 warnings.go:70] No static IP address has been configured for the namespace "containers-3320", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:11:11.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3320" for this suite.

• [SLOW TEST:9.154 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4679,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:11:12.106: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:11:12.626903      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1167", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:11:13.292645      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1167", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
W1202 11:11:13.770479      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-1167-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:11:14.010: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 11:11:16.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:11:18.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576274, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:11:21.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:11:21.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1167" for this suite.
STEP: Destroying namespace "webhook-1167-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.310 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":271,"skipped":4688,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:11:23.417: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 11:11:23.920456      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-6790", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  2 11:11:24.128: INFO: Waiting up to 5m0s for pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2" in namespace "emptydir-6790" to be "Succeeded or Failed"
Dec  2 11:11:24.135: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235216ms
Dec  2 11:11:26.145: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017360834s
Dec  2 11:11:28.160: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031825952s
Dec  2 11:11:30.170: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042060753s
Dec  2 11:11:32.183: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055057191s
STEP: Saw pod success
Dec  2 11:11:32.183: INFO: Pod "pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2" satisfied condition "Succeeded or Failed"
Dec  2 11:11:32.193: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2 container test-container: <nil>
STEP: delete the pod
Dec  2 11:11:32.242: INFO: Waiting for pod pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2 to disappear
Dec  2 11:11:32.250: INFO: Pod pod-f17da9b1-21a8-4218-9de5-d5cfbf1bf4a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:11:32.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6790" for this suite.

• [SLOW TEST:9.357 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:11:32.774: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:11:33.222973      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6638", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:11:33.980150      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6638", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:11:34.412505      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-6638-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:11:34.768: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  2 11:11:36.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:11:38.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576294, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:11:42.296: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:11:42.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6638" for this suite.
STEP: Destroying namespace "webhook-6638-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.401 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":273,"skipped":4728,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:11:44.176: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 11:11:44.653133      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-178", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-178
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-178
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-178
Dec  2 11:11:45.368: INFO: Found 0 stateful pods, waiting for 1
Dec  2 11:11:55.378: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  2 11:11:55.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 11:11:55.606: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 11:11:55.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 11:11:55.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 11:11:55.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  2 11:12:05.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 11:12:05.631: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 11:12:05.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999573s
Dec  2 11:12:06.681: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989353238s
Dec  2 11:12:07.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980327129s
Dec  2 11:12:08.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970469774s
Dec  2 11:12:09.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961720584s
Dec  2 11:12:10.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.949861751s
Dec  2 11:12:11.737: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.937983138s
Dec  2 11:12:12.749: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.924438871s
Dec  2 11:12:13.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912329152s
Dec  2 11:12:14.773: INFO: Verifying statefulset ss doesn't scale past 1 for another 900.397225ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-178
Dec  2 11:12:15.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 11:12:15.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 11:12:15.987: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 11:12:15.987: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 11:12:15.998: INFO: Found 1 stateful pods, waiting for 3
Dec  2 11:12:26.011: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:26.011: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:26.011: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:12:36.015: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:36.016: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:36.016: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=false
Dec  2 11:12:46.008: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:46.008: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:12:46.008: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  2 11:12:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 11:12:46.226: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 11:12:46.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 11:12:46.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 11:12:46.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 11:12:46.416: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 11:12:46.416: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 11:12:46.416: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 11:12:46.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  2 11:12:46.942: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  2 11:12:46.942: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  2 11:12:46.942: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  2 11:12:46.942: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 11:12:46.949: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  2 11:12:56.976: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 11:12:56.976: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 11:12:56.976: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  2 11:12:57.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999416s
Dec  2 11:12:58.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986976206s
Dec  2 11:12:59.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978446921s
Dec  2 11:13:00.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96394176s
Dec  2 11:13:01.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.927686436s
Dec  2 11:13:02.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.915641323s
Dec  2 11:13:03.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.905825687s
Dec  2 11:13:04.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.895002452s
Dec  2 11:13:05.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.887018627s
Dec  2 11:13:06.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 878.37152ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-178
Dec  2 11:13:07.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 11:13:07.340: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 11:13:07.340: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 11:13:07.340: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 11:13:07.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 11:13:07.533: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 11:13:07.533: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 11:13:07.533: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 11:13:07.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=statefulset-178 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  2 11:13:07.774: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  2 11:13:07.774: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  2 11:13:07.774: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  2 11:13:07.774: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 11:13:17.809: INFO: Deleting all statefulset in ns statefulset-178
Dec  2 11:13:17.819: INFO: Scaling statefulset ss to 0
Dec  2 11:13:17.844: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 11:13:17.852: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:13:17.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-178" for this suite.

• [SLOW TEST:94.162 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":274,"skipped":4728,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:13:18.338: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 11:13:18.773928      23 warnings.go:70] No static IP address has been configured for the namespace "pods-8198", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:13:18.967: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  2 11:13:18.993: INFO: The status of Pod pod-logs-websocket-b9026287-3c5b-4f30-9dbc-c89c3c6818f0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:13:21.006: INFO: The status of Pod pod-logs-websocket-b9026287-3c5b-4f30-9dbc-c89c3c6818f0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:13:23.002: INFO: The status of Pod pod-logs-websocket-b9026287-3c5b-4f30-9dbc-c89c3c6818f0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:13:25.007: INFO: The status of Pod pod-logs-websocket-b9026287-3c5b-4f30-9dbc-c89c3c6818f0 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:13:27.004: INFO: The status of Pod pod-logs-websocket-b9026287-3c5b-4f30-9dbc-c89c3c6818f0 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:13:27.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8198" for this suite.

• [SLOW TEST:9.181 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4729,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:13:27.519: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename cronjob
W1202 11:13:27.948261      23 warnings.go:70] No static IP address has been configured for the namespace "cronjob-7623", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
Dec  2 11:15:00.187: INFO: Warning: Found 0 jobs in namespace cronjob-7623
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:15:02.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7623" for this suite.

• [SLOW TEST:95.231 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":276,"skipped":4736,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:15:02.751: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename custom-resource-definition
W1202 11:15:03.286570      23 warnings.go:70] No static IP address has been configured for the namespace "custom-resource-definition-8040", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8040
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:15:03.486: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:16:05.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8040" for this suite.

• [SLOW TEST:63.201 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":277,"skipped":4736,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:16:05.953: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:16:06.488984      23 warnings.go:70] No static IP address has been configured for the namespace "projected-1540", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0b7b3074-a5c2-4569-a701-61c5d73cdd20
STEP: Creating a pod to test consume secrets
Dec  2 11:16:06.728: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398" in namespace "projected-1540" to be "Succeeded or Failed"
Dec  2 11:16:06.738: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398": Phase="Pending", Reason="", readiness=false. Elapsed: 9.648617ms
Dec  2 11:16:08.751: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022821818s
Dec  2 11:16:10.759: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030503423s
Dec  2 11:16:12.773: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045143341s
Dec  2 11:16:14.785: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.057044697s
STEP: Saw pod success
Dec  2 11:16:14.792: INFO: Pod "pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398" satisfied condition "Succeeded or Failed"
Dec  2 11:16:14.804: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  2 11:16:14.862: INFO: Waiting for pod pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398 to disappear
Dec  2 11:16:14.872: INFO: Pod pod-projected-secrets-337339cb-7992-4d72-a620-40c837350398 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:16:14.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1540" for this suite.

• [SLOW TEST:9.725 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":4738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:16:15.681: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 11:16:16.158394      23 warnings.go:70] No static IP address has been configured for the namespace "services-4597", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4597
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4597
STEP: creating replication controller externalsvc in namespace services-4597
I1202 11:16:17.343311      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4597, replica count: 2
I1202 11:16:20.395288      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:16:23.395731      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec  2 11:16:23.944: INFO: Creating new exec pod
Dec  2 11:16:27.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-4597 exec execpodfwsb4 -- /bin/sh -x -c nslookup nodeport-service.services-4597.svc.cluster.local'
Dec  2 11:16:28.219: INFO: stderr: "+ nslookup nodeport-service.services-4597.svc.cluster.local\n"
Dec  2 11:16:28.219: INFO: stdout: "Server:\t\t10.100.192.2\nAddress:\t10.100.192.2#53\n\nnodeport-service.services-4597.svc.cluster.local\tcanonical name = externalsvc.services-4597.svc.cluster.local.\nName:\texternalsvc.services-4597.svc.cluster.local\nAddress: 10.100.253.78\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4597, will wait for the garbage collector to delete the pods
Dec  2 11:16:28.296: INFO: Deleting ReplicationController externalsvc took: 14.322328ms
Dec  2 11:16:28.397: INFO: Terminating ReplicationController externalsvc pods took: 101.026573ms
Dec  2 11:16:31.809: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:16:32.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4597" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:17.163 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":279,"skipped":4760,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:16:32.844: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 11:16:33.358625      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-1264", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6b4b3e5e-46b9-408b-93c6-0a405e5b1437
STEP: Creating a pod to test consume configMaps
Dec  2 11:16:33.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91" in namespace "configmap-1264" to be "Succeeded or Failed"
Dec  2 11:16:33.576: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Pending", Reason="", readiness=false. Elapsed: 9.756985ms
Dec  2 11:16:35.589: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023322323s
Dec  2 11:16:37.603: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036435533s
Dec  2 11:16:39.611: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044960891s
Dec  2 11:16:41.627: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061064974s
Dec  2 11:16:43.639: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.073290682s
STEP: Saw pod success
Dec  2 11:16:43.640: INFO: Pod "pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91" satisfied condition "Succeeded or Failed"
Dec  2 11:16:43.646: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 11:16:43.702: INFO: Waiting for pod pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91 to disappear
Dec  2 11:16:43.708: INFO: Pod pod-configmaps-fe6d39ce-efa5-47bb-be84-3f16ae14dd91 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:16:43.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1264" for this suite.

• [SLOW TEST:11.354 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":4770,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:16:44.205: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:16:44.655351      23 warnings.go:70] No static IP address has been configured for the namespace "projected-4092", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:16:44.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05" in namespace "projected-4092" to be "Succeeded or Failed"
Dec  2 11:16:44.865: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05": Phase="Pending", Reason="", readiness=false. Elapsed: 14.172792ms
Dec  2 11:16:46.878: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026793473s
Dec  2 11:16:48.889: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038083915s
Dec  2 11:16:50.899: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048257881s
Dec  2 11:16:52.909: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.058619042s
STEP: Saw pod success
Dec  2 11:16:52.909: INFO: Pod "downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05" satisfied condition "Succeeded or Failed"
Dec  2 11:16:52.918: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05 container client-container: <nil>
STEP: delete the pod
Dec  2 11:16:52.960: INFO: Waiting for pod downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05 to disappear
Dec  2 11:16:52.969: INFO: Pod downwardapi-volume-9a05182f-8929-41fb-9c15-c4e0ac259a05 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:16:52.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4092" for this suite.

• [SLOW TEST:9.314 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":4778,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:16:53.519: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 11:16:54.017273      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-5704", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec  2 11:16:58.235: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5704 PodName:var-expansion-8b906c7d-2329-41af-8dd7-98a5958b9c40 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:16:58.235: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: test for file in mounted path
Dec  2 11:16:58.356: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5704 PodName:var-expansion-8b906c7d-2329-41af-8dd7-98a5958b9c40 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:16:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: updating the annotation value
Dec  2 11:16:58.973: INFO: Successfully updated pod "var-expansion-8b906c7d-2329-41af-8dd7-98a5958b9c40"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec  2 11:16:58.981: INFO: Deleting pod "var-expansion-8b906c7d-2329-41af-8dd7-98a5958b9c40" in namespace "var-expansion-5704"
Dec  2 11:16:58.996: INFO: Wait up to 5m0s for pod "var-expansion-8b906c7d-2329-41af-8dd7-98a5958b9c40" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:17:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5704" for this suite.

• [SLOW TEST:40.055 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":282,"skipped":4790,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:17:33.576: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename resourcequota
W1202 11:17:34.133656      23 warnings.go:70] No static IP address has been configured for the namespace "resourcequota-101", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:17:45.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-101" for this suite.

• [SLOW TEST:12.392 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":283,"skipped":4797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:17:45.968: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 11:17:46.488525      23 warnings.go:70] No static IP address has been configured for the namespace "dns-9575", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9575 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9575;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9575 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9575;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9575.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9575.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9575.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9575.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9575.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9575.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9575.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9575.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9575.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 190.215.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.215.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.215.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.215.190_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9575 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9575;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9575 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9575;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9575.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9575.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9575.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9575.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9575.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9575.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9575.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9575.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9575.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9575.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 190.215.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.215.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.215.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.215.190_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 11:17:53.709: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.719: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.726: INFO: Unable to read wheezy_udp@dns-test-service.dns-9575 from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9575 from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.765: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.834: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.845: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.853: INFO: Unable to read jessie_udp@dns-test-service.dns-9575 from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.871: INFO: Unable to read jessie_udp@dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.886: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:53.951: INFO: Lookups using dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9575 wheezy_tcp@dns-test-service.dns-9575 wheezy_udp@dns-test-service.dns-9575.svc wheezy_tcp@dns-test-service.dns-9575.svc wheezy_udp@_http._tcp.dns-test-service.dns-9575.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9575.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9575 jessie_udp@dns-test-service.dns-9575.svc jessie_udp@_http._tcp.dns-test-service.dns-9575.svc jessie_tcp@_http._tcp.dns-test-service.dns-9575.svc]

Dec  2 11:17:58.965: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:59.003: INFO: Unable to read wheezy_udp@dns-test-service.dns-9575.svc from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:59.088: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:59.107: INFO: Unable to read jessie_udp@dns-test-service.dns-9575 from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:17:59.196: INFO: Lookups using dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501 failed for: [wheezy_udp@dns-test-service wheezy_udp@dns-test-service.dns-9575.svc jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-9575]

Dec  2 11:18:04.103: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:18:04.237: INFO: Lookups using dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501 failed for: [jessie_udp@dns-test-service]

Dec  2 11:18:09.082: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501: the server could not find the requested resource (get pods dns-test-441af3ad-a91c-4140-95cb-ef9655468501)
Dec  2 11:18:09.209: INFO: Lookups using dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501 failed for: [jessie_udp@dns-test-service]

Dec  2 11:18:14.214: INFO: DNS probes using dns-9575/dns-test-441af3ad-a91c-4140-95cb-ef9655468501 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:18:15.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9575" for this suite.

• [SLOW TEST:29.903 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":284,"skipped":4853,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:18:15.871: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 11:18:16.345502      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-5014", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:18:16.559: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e" in namespace "downward-api-5014" to be "Succeeded or Failed"
Dec  2 11:18:16.570: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.643032ms
Dec  2 11:18:18.580: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02071105s
Dec  2 11:18:20.587: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02817026s
Dec  2 11:18:22.598: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038231296s
Dec  2 11:18:24.607: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.047781792s
STEP: Saw pod success
Dec  2 11:18:24.607: INFO: Pod "downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e" satisfied condition "Succeeded or Failed"
Dec  2 11:18:24.613: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e container client-container: <nil>
STEP: delete the pod
Dec  2 11:18:24.671: INFO: Waiting for pod downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e to disappear
Dec  2 11:18:24.677: INFO: Pod downwardapi-volume-6837b584-e21e-403c-9b89-4a214f87f70e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:18:24.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5014" for this suite.

• [SLOW TEST:9.339 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":4855,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:18:25.212: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption
W1202 11:18:25.671770      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-1352", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  2 11:18:25.902: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  2 11:19:26.094: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:19:26.113: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-preemption-path
W1202 11:19:26.876606      23 warnings.go:70] No static IP address has been configured for the namespace "sched-preemption-path-370", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:19:27.095: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec  2 11:19:27.104: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:19:27.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-370" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:19:27.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1352" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:63.008 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":286,"skipped":4865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:19:28.221: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename podtemplate
W1202 11:19:28.686253      23 warnings.go:70] No static IP address has been configured for the namespace "podtemplate-894", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Dec  2 11:19:28.903: INFO: created test-podtemplate-1
Dec  2 11:19:28.914: INFO: created test-podtemplate-2
Dec  2 11:19:28.925: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec  2 11:19:28.933: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec  2 11:19:28.973: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:19:28.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-894" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":287,"skipped":4889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:19:29.443: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 11:19:29.895351      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-4096", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4096
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:19:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  2 11:19:46.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4096 --namespace=crd-publish-openapi-4096 create -f -'
Dec  2 11:19:49.349: INFO: stderr: ""
Dec  2 11:19:49.349: INFO: stdout: "e2e-test-crd-publish-openapi-437-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  2 11:19:49.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4096 --namespace=crd-publish-openapi-4096 delete e2e-test-crd-publish-openapi-437-crds test-cr'
Dec  2 11:19:49.448: INFO: stderr: ""
Dec  2 11:19:49.448: INFO: stdout: "e2e-test-crd-publish-openapi-437-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec  2 11:19:49.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4096 --namespace=crd-publish-openapi-4096 apply -f -'
Dec  2 11:19:50.809: INFO: stderr: ""
Dec  2 11:19:50.809: INFO: stdout: "e2e-test-crd-publish-openapi-437-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  2 11:19:50.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4096 --namespace=crd-publish-openapi-4096 delete e2e-test-crd-publish-openapi-437-crds test-cr'
Dec  2 11:19:50.910: INFO: stderr: ""
Dec  2 11:19:50.910: INFO: stdout: "e2e-test-crd-publish-openapi-437-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec  2 11:19:50.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=crd-publish-openapi-4096 explain e2e-test-crd-publish-openapi-437-crds'
Dec  2 11:19:51.238: INFO: stderr: ""
Dec  2 11:19:51.238: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-437-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:20:01.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4096" for this suite.

• [SLOW TEST:32.642 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":288,"skipped":4919,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:20:02.085: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename configmap
W1202 11:20:02.597609      23 warnings.go:70] No static IP address has been configured for the namespace "configmap-6990", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-b44ec6fd-8966-45e6-9704-d1eb7cf85229
STEP: Creating a pod to test consume configMaps
Dec  2 11:20:02.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910" in namespace "configmap-6990" to be "Succeeded or Failed"
Dec  2 11:20:02.846: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 6.444606ms
Dec  2 11:20:04.853: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013721815s
Dec  2 11:20:06.864: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024517227s
Dec  2 11:20:08.874: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035232084s
Dec  2 11:20:10.883: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04400038s
Dec  2 11:20:12.892: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052507852s
Dec  2 11:20:14.900: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 12.060963335s
Dec  2 11:20:16.907: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Pending", Reason="", readiness=false. Elapsed: 14.067765626s
Dec  2 11:20:18.917: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.077602831s
STEP: Saw pod success
Dec  2 11:20:18.917: INFO: Pod "pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910" satisfied condition "Succeeded or Failed"
Dec  2 11:20:18.926: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 11:20:18.979: INFO: Waiting for pod pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910 to disappear
Dec  2 11:20:18.985: INFO: Pod pod-configmaps-29304e2e-e178-4bd2-a7ae-0bc98afe5910 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:20:18.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6990" for this suite.

• [SLOW TEST:17.456 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":4929,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:20:19.544: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename gc
W1202 11:20:20.053575      23 warnings.go:70] No static IP address has been configured for the namespace "gc-4199", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  2 11:20:30.309: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1202 11:20:30.309221      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:20:30.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4199" for this suite.

• [SLOW TEST:11.254 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":290,"skipped":4931,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:20:30.798: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename endpointslice
W1202 11:20:31.342891      23 warnings.go:70] No static IP address has been configured for the namespace "endpointslice-1562", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:20:31.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1562" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":291,"skipped":4944,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:20:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 11:20:32.808692      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-571", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-7ac39ccf-c482-47af-908c-1ee3bda7de0c in namespace container-probe-571
Dec  2 11:20:39.024: INFO: Started pod busybox-7ac39ccf-c482-47af-908c-1ee3bda7de0c in namespace container-probe-571
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 11:20:39.032: INFO: Initial restart count of pod busybox-7ac39ccf-c482-47af-908c-1ee3bda7de0c is 0
Dec  2 11:21:25.245: INFO: Restart count of pod container-probe-571/busybox-7ac39ccf-c482-47af-908c-1ee3bda7de0c is now 1 (46.213437859s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:21:25.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-571" for this suite.

• [SLOW TEST:53.676 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":4957,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:21:25.792: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename aggregator
W1202 11:21:26.253136      23 warnings.go:70] No static IP address has been configured for the namespace "aggregator-9004", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec  2 11:21:26.440: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Dec  2 11:21:27.013: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  2 11:21:29.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576886, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:21:31.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576887, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805576886, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:21:33.810: INFO: Waited 236.478873ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Dec  2 11:21:34.847: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:21:35.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9004" for this suite.

• [SLOW TEST:10.397 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":293,"skipped":4975,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:21:36.194: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-watch
W1202 11:21:36.671897      23 warnings.go:70] No static IP address has been configured for the namespace "crd-watch-7221", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-7221
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:21:36.852: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Creating first CR 
Dec  2 11:21:44.441: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:21:44Z]] name:name1 resourceVersion:52217654 uid:16a7586a-21ca-4687-9539-176b2dccc4cb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec  2 11:21:54.456: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:21:54Z]] name:name2 resourceVersion:52217717 uid:07f938c3-cf83-4484-86dc-6b14e4bed7aa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec  2 11:22:04.467: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:22:04Z]] name:name1 resourceVersion:52217777 uid:16a7586a-21ca-4687-9539-176b2dccc4cb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec  2 11:22:14.480: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:22:14Z]] name:name2 resourceVersion:52217835 uid:07f938c3-cf83-4484-86dc-6b14e4bed7aa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec  2 11:22:24.497: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:22:04Z]] name:name1 resourceVersion:52217894 uid:16a7586a-21ca-4687-9539-176b2dccc4cb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec  2 11:22:34.517: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-02T11:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-02T11:22:14Z]] name:name2 resourceVersion:52217953 uid:07f938c3-cf83-4484-86dc-6b14e4bed7aa] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:22:45.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7221" for this suite.

• [SLOW TEST:69.314 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":294,"skipped":4996,"failed":0}
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:22:45.509: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename replicaset
W1202 11:22:46.115970      23 warnings.go:70] No static IP address has been configured for the namespace "replicaset-794", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Dec  2 11:22:46.375: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  2 11:22:51.382: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Dec  2 11:22:51.390: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Dec  2 11:22:51.407: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Dec  2 11:22:51.411: INFO: Observed &ReplicaSet event: ADDED
Dec  2 11:22:51.411: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.412: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.412: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.412: INFO: Found replicaset test-rs in namespace replicaset-794 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  2 11:22:51.412: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Dec  2 11:22:51.412: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  2 11:22:51.422: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Dec  2 11:22:51.425: INFO: Observed &ReplicaSet event: ADDED
Dec  2 11:22:51.425: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.426: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.426: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.426: INFO: Observed replicaset test-rs in namespace replicaset-794 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  2 11:22:51.426: INFO: Observed &ReplicaSet event: MODIFIED
Dec  2 11:22:51.426: INFO: Found replicaset test-rs in namespace replicaset-794 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec  2 11:22:51.426: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:22:51.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-794" for this suite.

• [SLOW TEST:6.390 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":295,"skipped":4996,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:22:51.899: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:22:52.368561      23 warnings.go:70] No static IP address has been configured for the namespace "projected-3710", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:22:52.565: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f" in namespace "projected-3710" to be "Succeeded or Failed"
Dec  2 11:22:52.571: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848679ms
Dec  2 11:22:54.582: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016948885s
Dec  2 11:22:56.590: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024618155s
Dec  2 11:22:58.597: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032200186s
Dec  2 11:23:00.605: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039584639s
STEP: Saw pod success
Dec  2 11:23:00.605: INFO: Pod "downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f" satisfied condition "Succeeded or Failed"
Dec  2 11:23:00.614: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f container client-container: <nil>
STEP: delete the pod
Dec  2 11:23:00.674: INFO: Waiting for pod downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f to disappear
Dec  2 11:23:00.681: INFO: Pod downwardapi-volume-6d4d2828-f287-43e6-9e21-4c3a439e1c3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:00.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3710" for this suite.

• [SLOW TEST:9.334 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5012,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:01.234: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 11:23:01.735442      23 warnings.go:70] No static IP address has been configured for the namespace "dns-6061", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec  2 11:23:01.939: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6061  076f7312-074f-47da-9686-a36a9f438e54 52218195 0 2022-12-02 11:23:01 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-12-02 11:23:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kbr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kbr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:23:01.949: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:23:03.957: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:23:05.956: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:23:07.957: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec  2 11:23:07.957: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6061 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:23:07.957: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Verifying customized DNS server is configured on pod...
Dec  2 11:23:08.072: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6061 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  2 11:23:08.072: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
Dec  2 11:23:08.188: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:08.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6061" for this suite.

• [SLOW TEST:7.430 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":297,"skipped":5017,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:08.664: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 11:23:09.244964      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-5858", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Dec  2 11:23:09.440: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec  2 11:23:09.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:12.904: INFO: stderr: ""
Dec  2 11:23:12.904: INFO: stdout: "service/agnhost-replica created\n"
Dec  2 11:23:12.904: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec  2 11:23:12.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:13.776: INFO: stderr: ""
Dec  2 11:23:13.776: INFO: stdout: "service/agnhost-primary created\n"
Dec  2 11:23:13.777: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  2 11:23:13.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:16.448: INFO: stderr: ""
Dec  2 11:23:16.448: INFO: stdout: "service/frontend created\n"
Dec  2 11:23:16.448: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec  2 11:23:16.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:16.808: INFO: stderr: ""
Dec  2 11:23:16.808: INFO: stdout: "deployment.apps/frontend created\n"
Dec  2 11:23:16.808: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  2 11:23:16.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:17.116: INFO: stderr: ""
Dec  2 11:23:17.116: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec  2 11:23:17.116: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  2 11:23:17.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 create -f -'
Dec  2 11:23:17.475: INFO: stderr: ""
Dec  2 11:23:17.475: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec  2 11:23:17.475: INFO: Waiting for all frontend pods to be Running.
Dec  2 11:23:37.527: INFO: Waiting for frontend to serve content.
Dec  2 11:23:37.555: INFO: Trying to add a new entry to the guestbook.
Dec  2 11:23:37.584: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  2 11:23:37.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:38.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:38.170: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec  2 11:23:38.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:38.764: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:38.764: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec  2 11:23:38.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:39.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:39.337: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  2 11:23:39.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:39.441: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:39.441: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  2 11:23:39.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:39.541: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:39.541: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec  2 11:23:39.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Dec  2 11:23:39.626: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  2 11:23:39.626: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5858" for this suite.

• [SLOW TEST:31.438 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":298,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:40.103: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename ingress
W1202 11:23:40.531476      23 warnings.go:70] No static IP address has been configured for the namespace "ingress-3791", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-3791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  2 11:23:40.757: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  2 11:23:40.765: INFO: starting watch
STEP: patching
STEP: updating
Dec  2 11:23:40.791: INFO: waiting for watch events with expected annotations
Dec  2 11:23:40.791: INFO: missing expected annotations, waiting: map[string]string{"ncp/error.loadbalancer":"INVALID_INGRESS"}
Dec  2 11:23:40.791: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:40.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3791" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":299,"skipped":5039,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:41.475: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 11:23:41.906845      23 warnings.go:70] No static IP address has been configured for the namespace "pods-5159", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec  2 11:23:42.187: INFO: observed Pod pod-test in namespace pods-5159 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec  2 11:23:42.194: INFO: observed Pod pod-test in namespace pods-5159 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC  }]
Dec  2 11:23:44.632: INFO: observed Pod pod-test in namespace pods-5159 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC  }]
Dec  2 11:23:48.290: INFO: Found Pod pod-test in namespace pods-5159 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-02 11:23:42 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Dec  2 11:23:48.310: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec  2 11:23:48.369: INFO: observed event type ADDED
Dec  2 11:23:48.369: INFO: observed event type MODIFIED
Dec  2 11:23:48.369: INFO: observed event type MODIFIED
Dec  2 11:23:48.369: INFO: observed event type MODIFIED
Dec  2 11:23:48.370: INFO: observed event type MODIFIED
Dec  2 11:23:48.370: INFO: observed event type MODIFIED
Dec  2 11:23:48.370: INFO: observed event type MODIFIED
Dec  2 11:23:50.326: INFO: observed event type MODIFIED
Dec  2 11:23:51.353: INFO: observed event type MODIFIED
Dec  2 11:23:51.367: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:51.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5159" for this suite.

• [SLOW TEST:10.360 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":300,"skipped":5045,"failed":0}
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:51.835: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sysctl
W1202 11:23:52.343887      23 warnings.go:70] No static IP address has been configured for the namespace "sysctl-3634", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-3634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:23:52.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3634" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":301,"skipped":5047,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:23:53.040: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename watch
W1202 11:23:53.459124      23 warnings.go:70] No static IP address has been configured for the namespace "watch-7914", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  2 11:23:53.681: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218793 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 11:23:53.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218794 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 11:23:53.681: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218795 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  2 11:24:03.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218867 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 11:24:03.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218868 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  2 11:24:03.734: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7914  756b998c-51aa-4152-868a-c96b686e8b95 52218869 0 2022-12-02 11:23:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-02 11:23:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:24:03.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7914" for this suite.

• [SLOW TEST:11.147 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":302,"skipped":5066,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:24:04.189: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename events
W1202 11:24:04.684303      23 warnings.go:70] No static IP address has been configured for the namespace "events-6489", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Dec  2 11:24:04.881: INFO: created test-event-1
Dec  2 11:24:04.892: INFO: created test-event-2
Dec  2 11:24:04.898: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec  2 11:24:04.904: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec  2 11:24:04.952: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:24:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6489" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":303,"skipped":5083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:24:06.234: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename containers
W1202 11:24:06.683256      23 warnings.go:70] No static IP address has been configured for the namespace "containers-3300", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Dec  2 11:24:06.883: INFO: Waiting up to 5m0s for pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2" in namespace "containers-3300" to be "Succeeded or Failed"
Dec  2 11:24:06.890: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989081ms
Dec  2 11:24:08.991: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107460521s
Dec  2 11:24:11.000: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.116084756s
Dec  2 11:24:13.009: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.125196077s
Dec  2 11:24:15.016: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.132795715s
STEP: Saw pod success
Dec  2 11:24:15.016: INFO: Pod "client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2" satisfied condition "Succeeded or Failed"
Dec  2 11:24:15.023: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2 container agnhost-container: <nil>
STEP: delete the pod
Dec  2 11:24:15.066: INFO: Waiting for pod client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2 to disappear
Dec  2 11:24:15.077: INFO: Pod client-containers-cebae374-8094-4f0b-a60f-95228a5e63e2 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:24:15.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3300" for this suite.

• [SLOW TEST:9.375 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5120,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:24:15.609: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:24:16.115633      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2830", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:24:16.740457      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2830", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:24:17.173933      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-2830-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:24:17.709: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  2 11:24:19.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:24:21.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577057, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:24:25.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:24:25.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2830" for this suite.
STEP: Destroying namespace "webhook-2830-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.145 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":305,"skipped":5123,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:24:26.754: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-pred
W1202 11:24:27.212407      23 warnings.go:70] No static IP address has been configured for the namespace "sched-pred-8410", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  2 11:24:27.402: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  2 11:24:27.423: INFO: Waiting for terminating namespaces to be deleted...
Dec  2 11:24:27.436: INFO: 
Logging pods the apiserver thinks is on node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 before test
Dec  2 11:24:27.516: INFO: coredns-6c4c59f5b4-fgkn9 from kube-system started at 2022-12-02 10:36:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:24:27.516: INFO: promtail-pfssc from logging started at 2022-12-02 10:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.516: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:24:27.516: INFO: node-exporter-wh467 from monitoring started at 2022-12-01 00:27:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:24:27.516: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:24:27.516: INFO: fluent-bit-95snl from pks-system started at 2022-12-02 10:36:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:24:27.516: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:24:27.516: INFO: node-exporter-ngrrc from pks-system started at 2022-12-02 10:36:34 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:24:27.516: INFO: telegraf-g9948 from pks-system started at 2022-12-02 10:36:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:24:27.516: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25 from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:24:27.516: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:24:27.516: INFO: restic-pm729 from velero started at 2022-12-02 10:36:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.516: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:24:27.516: INFO: 
Logging pods the apiserver thinks is on node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 before test
Dec  2 11:24:27.610: INFO: argo-server-78c9599f9-phpdk from argo started at 2022-12-01 00:07:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container argo-server ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: cert-manager-6bdfc96d57-j8xtt from cert-manager started at 2022-12-02 09:50:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:24:27.610: INFO: code-server-68cdb99985-76lzq from code-server started at 2022-12-01 00:08:06 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container code-server ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-jobservice-67df6d9455-6d4w5 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-trivy-0 from harbor-restored-1 started at 2022-12-01 00:07:25 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-exporter-6fcd6687f5-tg4sq from harbor-restored-100 started at 2022-12-01 00:07:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-jobservice-67df6d9455-nrds9 from harbor-restored-100 started at 2022-12-01 00:07:44 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-registry-566bccf488-cqlc2 from harbor-restored-100 started at 2022-12-01 00:08:12 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-trivy-0 from harbor-restored-100 started at 2022-12-01 00:10:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-core-66c67cf548-htx9b from harbor-restored-2 started at 2022-12-01 00:08:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-jobservice-67df6d9455-qnhcr from harbor-restored-2 started at 2022-12-01 00:07:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-portal-58c8bb4658-b9ccp from harbor-restored-2 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-registry-566bccf488-2294p from harbor-restored-2 started at 2022-12-01 00:07:48 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-core-66c67cf548-2srb6 from harbor-restored-600 started at 2022-12-01 00:07:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-exporter-6fcd6687f5-zttc4 from harbor-restored-600 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-portal-58c8bb4658-bf9pv from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-registry-566bccf488-4r7nw from harbor-restored-600 started at 2022-12-01 00:07:31 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-trivy-0 from harbor-restored-600 started at 2022-12-01 00:11:19 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.610: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.610: INFO: harbor-exporter-6fcd6687f5-g9cgh from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-cr5z2 from harbor-restored-700 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-portal-58c8bb4658-mqpqt from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-700 started at 2022-12-01 00:36:30 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-exporter-6fcd6687f5-lph6q from harbor-restored-701 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-4rj79 from harbor-restored-701 started at 2022-12-01 00:08:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-portal-58c8bb4658-kgnn2 from harbor-restored-701 started at 2022-12-01 00:07:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-701 started at 2022-12-01 00:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-mmml7 from harbor-restored-702 started at 2022-12-01 00:07:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-registry-566bccf488-dw7qz from harbor-restored-702 started at 2022-12-01 00:08:11 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-core-66c67cf548-4g4zb from harbor-restored-703 started at 2022-12-01 00:08:15 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-exporter-6fcd6687f5-wrxhf from harbor-restored-703 started at 2022-12-01 00:08:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-gzs5l from harbor-restored-703 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-redis-0 from harbor-restored-703 started at 2022-12-01 00:07:50 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-registry-566bccf488-dmrc5 from harbor-restored-703 started at 2022-12-01 00:08:59 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-703 started at 2022-12-01 00:10:12 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-core-66c67cf548-qxk8k from harbor-restored-705 started at 2022-12-01 00:08:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-exporter-6fcd6687f5-2vsd9 from harbor-restored-705 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-registry-566bccf488-ng244 from harbor-restored-705 started at 2022-12-01 00:09:00 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-705 started at 2022-12-01 00:36:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-database-0 from harbor-restored-706 started at 2022-12-01 00:07:40 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-exporter-6fcd6687f5-fqhxs from harbor-restored-706 started at 2022-12-01 00:07:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-portal-58c8bb4658-s8d6h from harbor-restored-706 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-redis-0 from harbor-restored-706 started at 2022-12-01 00:10:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-registry-566bccf488-fsj6q from harbor-restored-706 started at 2022-12-01 00:08:19 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-706 started at 2022-12-01 00:10:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-core-66c67cf548-jv9kv from harbor-restored-710 started at 2022-12-01 00:07:49 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-l79z9 from harbor-restored-710 started at 2022-12-01 00:07:22 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor-restored-710 started at 2022-12-01 00:08:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-database-0 from harbor started at 2022-12-01 00:06:05 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-exporter-6fcd6687f5-m98lw from harbor started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-jobservice-67df6d9455-bp2t7 from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-registry-566bccf488-bcjg8 from harbor started at 2022-12-01 00:08:08 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.611: INFO: harbor-trivy-0 from harbor started at 2022-12-01 00:08:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: istiod-57c4757749-hs8dc from istio-system started at 2022-12-01 00:48:17 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container discovery ready: true, restart count 0
Dec  2 11:24:27.611: INFO: kiali-operator-9d8cbb844-rgrkz from istio-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container operator ready: true, restart count 0
Dec  2 11:24:27.611: INFO: coredns-6c4c59f5b4-djxn2 from kube-system started at 2022-12-01 00:32:29 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:24:27.611: INFO: metrics-server-757c77bdc5-2ggxp from kube-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container metrics-server ready: true, restart count 0
Dec  2 11:24:27.611: INFO: logging-operator-56b5fcc8bd-xkzld from logging started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container logging-operator ready: false, restart count 0
Dec  2 11:24:27.611: INFO: promtail-7pk9g from logging started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:24:27.611: INFO: alertmanager-main-1 from monitoring started at 2022-12-01 00:09:05 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: alertmanager-main-2 from monitoring started at 2022-12-02 09:50:38 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: kube-state-metrics-85476668-r7zjx from monitoring started at 2022-12-01 00:07:34 +0000 UTC (4 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container kube-rbac-proxy-main ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container kube-rbac-proxy-self ready: false, restart count 0
Dec  2 11:24:27.611: INFO: 	Container kube-state-metrics ready: false, restart count 0
Dec  2 11:24:27.611: INFO: node-exporter-wzvwt from monitoring started at 2022-11-30 23:36:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:24:27.611: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:24:27.611: INFO: prometheus-operator-6984ff874-jwfxk from monitoring started at 2022-12-02 09:50:26 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.612: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:24:27.612: INFO: 	Container prometheus-operator ready: false, restart count 0
Dec  2 11:24:27.612: INFO: event-controller-7db654d86f-4l24d from pks-system started at 2022-12-02 09:50:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container event-controller ready: true, restart count 0
Dec  2 11:24:27.612: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:24:27.612: INFO: fluent-bit-lc7xf from pks-system started at 2022-12-02 09:52:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:24:27.612: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:24:27.612: INFO: node-exporter-f8q68 from pks-system started at 2022-12-01 00:33:40 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:24:27.612: INFO: telegraf-4hnb8 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:24:27.612: INFO: validator-5675b6544f-pvrdh from pks-system started at 2022-12-02 09:50:59 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container validator ready: true, restart count 0
Dec  2 11:24:27.612: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-qb7jc from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:24:27.612: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:24:27.612: INFO: restic-wq8qp from velero started at 2022-11-30 23:37:03 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.612: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:24:27.612: INFO: 
Logging pods the apiserver thinks is on node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa before test
Dec  2 11:24:27.709: INFO: workflow-controller-c8c847777-chkd8 from argo started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.709: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.709: INFO: 	Container workflow-controller ready: false, restart count 0
Dec  2 11:24:27.710: INFO: cert-manager-cainjector-786868d5bb-9s22p from cert-manager started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.710: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:24:27.710: INFO: cert-manager-webhook-5fd474b69d-xrpgh from cert-manager started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.710: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:24:27.710: INFO: harbor-core-66c67cf548-r2496 from harbor-restored-1 started at 2022-12-01 00:08:14 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.710: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.710: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.710: INFO: harbor-database-0 from harbor-restored-1 started at 2022-12-01 00:07:47 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.710: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.710: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.710: INFO: harbor-exporter-6fcd6687f5-9g4ql from harbor-restored-1 started at 2022-12-01 00:09:23 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.710: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.710: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.711: INFO: harbor-portal-58c8bb4658-ng6v7 from harbor-restored-1 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.711: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.711: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.711: INFO: harbor-redis-0 from harbor-restored-1 started at 2022-12-01 00:09:57 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.711: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.711: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.711: INFO: harbor-registry-566bccf488-2twt7 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.711: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.711: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.711: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.711: INFO: harbor-core-66c67cf548-wklw5 from harbor-restored-100 started at 2022-12-01 00:09:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.711: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.711: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.712: INFO: harbor-database-0 from harbor-restored-100 started at 2022-12-01 00:08:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.712: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.712: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.712: INFO: harbor-portal-58c8bb4658-rxsk5 from harbor-restored-100 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.712: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.712: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.712: INFO: harbor-redis-0 from harbor-restored-100 started at 2022-12-01 00:08:35 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.712: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.712: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.713: INFO: harbor-database-0 from harbor-restored-2 started at 2022-12-01 00:09:41 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.713: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.713: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.713: INFO: harbor-exporter-6fcd6687f5-l5zmg from harbor-restored-2 started at 2022-12-01 00:08:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.713: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.713: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.713: INFO: harbor-redis-0 from harbor-restored-2 started at 2022-12-01 00:08:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.713: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-trivy-0 from harbor-restored-2 started at 2022-12-02 09:50:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-database-0 from harbor-restored-600 started at 2022-12-01 00:09:59 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-jobservice-67df6d9455-zmxln from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-redis-0 from harbor-restored-600 started at 2022-12-01 00:07:51 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-core-66c67cf548-6wt2b from harbor-restored-700 started at 2022-12-01 00:09:17 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.714: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.714: INFO: harbor-database-0 from harbor-restored-700 started at 2022-12-01 00:10:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.714: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.715: INFO: harbor-redis-0 from harbor-restored-700 started at 2022-12-01 00:08:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.715: INFO: harbor-registry-566bccf488-zzvj7 from harbor-restored-700 started at 2022-12-01 00:08:51 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.715: INFO: harbor-core-66c67cf548-bnjrn from harbor-restored-701 started at 2022-12-01 00:08:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.715: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.715: INFO: harbor-database-0 from harbor-restored-701 started at 2022-12-01 00:09:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.715: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.715: INFO: harbor-redis-0 from harbor-restored-701 started at 2022-12-01 00:07:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.715: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.716: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.716: INFO: harbor-registry-566bccf488-4scbl from harbor-restored-701 started at 2022-12-01 00:07:53 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.716: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.716: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.716: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.716: INFO: harbor-core-66c67cf548-f5fjz from harbor-restored-702 started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.716: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.716: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.716: INFO: harbor-database-0 from harbor-restored-702 started at 2022-12-01 00:10:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.716: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.716: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.716: INFO: harbor-exporter-6fcd6687f5-v5tz8 from harbor-restored-702 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.717: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.717: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.717: INFO: harbor-portal-58c8bb4658-zzshz from harbor-restored-702 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.717: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.717: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.717: INFO: harbor-redis-0 from harbor-restored-702 started at 2022-12-02 09:50:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.717: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.717: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.717: INFO: harbor-trivy-0 from harbor-restored-702 started at 2022-12-02 09:50:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.717: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.717: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:24:27.718: INFO: harbor-database-0 from harbor-restored-703 started at 2022-12-01 00:09:09 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.718: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.718: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.718: INFO: harbor-portal-58c8bb4658-7gmp5 from harbor-restored-703 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.718: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.718: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.718: INFO: harbor-database-0 from harbor-restored-705 started at 2022-12-01 00:10:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.718: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.718: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.719: INFO: harbor-jobservice-67df6d9455-4lg98 from harbor-restored-705 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.719: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.719: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.719: INFO: harbor-portal-58c8bb4658-9tfsd from harbor-restored-705 started at 2022-12-01 00:09:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.719: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.719: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.719: INFO: harbor-redis-0 from harbor-restored-705 started at 2022-11-30 19:09:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.719: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.719: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.720: INFO: harbor-core-66c67cf548-wnj7c from harbor-restored-706 started at 2022-12-01 00:08:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.720: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.720: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.720: INFO: harbor-jobservice-67df6d9455-4tzqq from harbor-restored-706 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.720: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.720: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:24:27.720: INFO: harbor-database-0 from harbor-restored-710 started at 2022-12-01 00:08:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.720: INFO: 	Container database ready: false, restart count 0
Dec  2 11:24:27.720: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.720: INFO: harbor-exporter-6fcd6687f5-zzfwt from harbor-restored-710 started at 2022-12-01 00:08:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.721: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:24:27.721: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.721: INFO: harbor-portal-58c8bb4658-htz6g from harbor-restored-710 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.721: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.721: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.721: INFO: harbor-redis-0 from harbor-restored-710 started at 2022-12-01 00:09:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.721: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.721: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.721: INFO: harbor-registry-566bccf488-bzh9n from harbor-restored-710 started at 2022-12-01 00:08:28 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.721: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.722: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:24:27.722: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:24:27.722: INFO: harbor-core-66c67cf548-8bjrq from harbor started at 2022-12-01 00:09:34 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.722: INFO: 	Container core ready: false, restart count 0
Dec  2 11:24:27.722: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.722: INFO: harbor-portal-58c8bb4658-6x22v from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.722: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.722: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:24:27.722: INFO: harbor-redis-0 from harbor started at 2022-12-01 00:08:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.722: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.723: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:24:27.723: INFO: ingress-gateway-management-c6c49cf-2gtg5 from ingress-gateway-management started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.723: INFO: 	Container istio-proxy ready: true, restart count 0
Dec  2 11:24:27.723: INFO: istio-operator-67b8b6c9dd-bqlxh from istio-operator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.723: INFO: 	Container istio-operator ready: true, restart count 0
Dec  2 11:24:27.723: INFO: keycloak-0 from keycloak started at 2022-11-30 18:59:19 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.723: INFO: 	Container keycloak ready: false, restart count 0
Dec  2 11:24:27.723: INFO: keycloak-operator-76c854b9f4-ts46s from keycloak started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.723: INFO: 	Container keycloak-operator ready: true, restart count 0
Dec  2 11:24:27.723: INFO: keycloak-postgresql-746d7cc6dd-rzdpf from keycloak started at 2022-11-30 19:10:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.723: INFO: 	Container keycloak-postgresql ready: true, restart count 0
Dec  2 11:24:27.723: INFO: docs-67854bcfd4-kfwsh from kube-plus-docs started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.724: INFO: 	Container docs ready: false, restart count 0
Dec  2 11:24:27.724: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.724: INFO: coredns-6c4c59f5b4-knwtr from kube-system started at 2022-12-01 00:36:09 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.724: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:24:27.724: INFO: replicator-kubernetes-replicator-6774dd4f4d-5t7wv from kubernetes-replicator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.724: INFO: 	Container kubernetes-replicator ready: true, restart count 0
Dec  2 11:24:27.724: INFO: loki-0 from logging started at 2022-12-02 09:50:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.724: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.724: INFO: 	Container loki ready: false, restart count 0
Dec  2 11:24:27.724: INFO: promtail-854st from logging started at 2022-12-01 00:09:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.724: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.724: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:24:27.724: INFO: alertmanager-main-0 from monitoring started at 2022-12-01 00:07:33 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.725: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.725: INFO: blackbox-exporter-685b55655c-bdb7h from monitoring started at 2022-12-02 09:50:26 +0000 UTC (4 container statuses recorded)
Dec  2 11:24:27.725: INFO: 	Container blackbox-exporter ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container module-configmap-reloader ready: false, restart count 0
Dec  2 11:24:27.725: INFO: grafana-69d978c595-lqn8f from monitoring started at 2022-12-01 00:09:02 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.725: INFO: 	Container grafana ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container grafana-sc-dashboard ready: false, restart count 0
Dec  2 11:24:27.725: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.726: INFO: node-exporter-zn785 from monitoring started at 2022-11-30 18:58:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.726: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:24:27.726: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:24:27.726: INFO: prometheus-k8s-0 from monitoring started at 2022-12-02 09:50:58 +0000 UTC (3 container statuses recorded)
Dec  2 11:24:27.726: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:24:27.726: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.726: INFO: 	Container prometheus ready: false, restart count 0
Dec  2 11:24:27.726: INFO: fluent-bit-w54dc from pks-system started at 2022-12-02 09:52:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.726: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:24:27.726: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:24:27.726: INFO: metric-controller-7b78cd6c98-g275p from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.726: INFO: 	Container metric-controller ready: true, restart count 0
Dec  2 11:24:27.727: INFO: node-exporter-77jkw from pks-system started at 2022-12-01 00:33:16 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.727: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:24:27.727: INFO: observability-manager-fc646c85f-mf72v from pks-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.727: INFO: 	Container observability-manager ready: true, restart count 0
Dec  2 11:24:27.727: INFO: sink-controller-78ccbc6f4d-gjph5 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.727: INFO: 	Container sink-controller ready: true, restart count 0
Dec  2 11:24:27.727: INFO: telegraf-4w56v from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.727: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:24:27.727: INFO: pomerium-58fb8f9f6-g77zd from pomerium started at 2022-12-01 00:56:56 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.727: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.727: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:24:27.727: INFO: pomerium-66c956fc58-df627 from pomerium started at 2022-12-01 01:00:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:24:27.728: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:24:27.728: INFO: registry-creds-registry-creds-controller-c6879445d-mhvtd from registry-creds started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container controller ready: true, restart count 0
Dec  2 11:24:27.728: INFO: sealed-secrets-controller-856cd994fb-xzcjz from sealed-secrets started at 2022-11-30 18:58:55 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Dec  2 11:24:27.728: INFO: sonobuoy from sonobuoy started at 2022-12-02 09:03:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  2 11:24:27.728: INFO: sonobuoy-e2e-job-4a9382b33c5d4cba from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container e2e ready: true, restart count 0
Dec  2 11:24:27.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:24:27.728: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-wtc8k from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:24:27.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:24:27.729: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:24:27.729: INFO: restic-55bfd from velero started at 2022-11-30 18:58:22 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.729: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:24:27.729: INFO: velero-69f6b65985-bqq8v from velero started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:24:27.729: INFO: 	Container velero ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
STEP: verifying the node has the label node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
STEP: verifying the node has the label node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.055: INFO: Pod argo-server-78c9599f9-phpdk requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.055: INFO: Pod workflow-controller-c8c847777-chkd8 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.055: INFO: Pod cert-manager-6bdfc96d57-j8xtt requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.055: INFO: Pod cert-manager-cainjector-786868d5bb-9s22p requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.055: INFO: Pod cert-manager-webhook-5fd474b69d-xrpgh requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.055: INFO: Pod code-server-68cdb99985-76lzq requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.055: INFO: Pod harbor-core-66c67cf548-r2496 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-exporter-6fcd6687f5-9g4ql requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-jobservice-67df6d9455-6d4w5 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.056: INFO: Pod harbor-portal-58c8bb4658-ng6v7 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-registry-566bccf488-2twt7 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.056: INFO: Pod harbor-core-66c67cf548-wklw5 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.056: INFO: Pod harbor-exporter-6fcd6687f5-tg4sq requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.056: INFO: Pod harbor-jobservice-67df6d9455-nrds9 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.056: INFO: Pod harbor-portal-58c8bb4658-rxsk5 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-registry-566bccf488-cqlc2 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-core-66c67cf548-htx9b requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-exporter-6fcd6687f5-l5zmg requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-jobservice-67df6d9455-qnhcr requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-portal-58c8bb4658-b9ccp requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-registry-566bccf488-2294p requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.057: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.057: INFO: Pod harbor-core-66c67cf548-2srb6 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.058: INFO: Pod harbor-exporter-6fcd6687f5-zttc4 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-jobservice-67df6d9455-zmxln requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.058: INFO: Pod harbor-portal-58c8bb4658-bf9pv requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.058: INFO: Pod harbor-registry-566bccf488-4r7nw requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-core-66c67cf548-6wt2b requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.058: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.058: INFO: Pod harbor-exporter-6fcd6687f5-g9cgh requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.058: INFO: Pod harbor-jobservice-67df6d9455-cr5z2 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-portal-58c8bb4658-mqpqt requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-registry-566bccf488-zzvj7 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-core-66c67cf548-bnjrn requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-exporter-6fcd6687f5-lph6q requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-jobservice-67df6d9455-4rj79 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-portal-58c8bb4658-kgnn2 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.059: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-registry-566bccf488-4scbl requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.059: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.060: INFO: Pod harbor-core-66c67cf548-f5fjz requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-exporter-6fcd6687f5-v5tz8 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-jobservice-67df6d9455-mmml7 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.060: INFO: Pod harbor-portal-58c8bb4658-zzshz requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-registry-566bccf488-dw7qz requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.060: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-core-66c67cf548-4g4zb requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.060: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.060: INFO: Pod harbor-exporter-6fcd6687f5-wrxhf requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.060: INFO: Pod harbor-jobservice-67df6d9455-gzs5l requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-portal-58c8bb4658-7gmp5 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.061: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-registry-566bccf488-dmrc5 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-core-66c67cf548-qxk8k requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.061: INFO: Pod harbor-exporter-6fcd6687f5-2vsd9 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.061: INFO: Pod harbor-jobservice-67df6d9455-4lg98 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.061: INFO: Pod harbor-portal-58c8bb4658-9tfsd requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.061: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.061: INFO: Pod harbor-registry-566bccf488-ng244 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-core-66c67cf548-wnj7c requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.062: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-exporter-6fcd6687f5-fqhxs requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-jobservice-67df6d9455-4tzqq requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.062: INFO: Pod harbor-portal-58c8bb4658-s8d6h requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-registry-566bccf488-fsj6q requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-core-66c67cf548-jv9kv requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.062: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.062: INFO: Pod harbor-exporter-6fcd6687f5-zzfwt requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.062: INFO: Pod harbor-jobservice-67df6d9455-l79z9 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.063: INFO: Pod harbor-portal-58c8bb4658-htz6g requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-registry-566bccf488-bzh9n requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.063: INFO: Pod harbor-core-66c67cf548-8bjrq requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-database-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.063: INFO: Pod harbor-exporter-6fcd6687f5-m98lw requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.063: INFO: Pod harbor-jobservice-67df6d9455-bp2t7 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.063: INFO: Pod harbor-portal-58c8bb4658-6x22v requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-redis-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.063: INFO: Pod harbor-registry-566bccf488-bcjg8 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.064: INFO: Pod harbor-trivy-0 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.064: INFO: Pod ingress-gateway-management-c6c49cf-2gtg5 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod istio-operator-67b8b6c9dd-bqlxh requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod istiod-57c4757749-hs8dc requesting resource cpu=500m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.064: INFO: Pod kiali-operator-9d8cbb844-rgrkz requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.064: INFO: Pod keycloak-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod keycloak-operator-76c854b9f4-ts46s requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod keycloak-postgresql-746d7cc6dd-rzdpf requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod docs-67854bcfd4-kfwsh requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.064: INFO: Pod coredns-6c4c59f5b4-djxn2 requesting resource cpu=100m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.064: INFO: Pod coredns-6c4c59f5b4-fgkn9 requesting resource cpu=100m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.064: INFO: Pod coredns-6c4c59f5b4-knwtr requesting resource cpu=100m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod metrics-server-757c77bdc5-2ggxp requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.065: INFO: Pod replicator-kubernetes-replicator-6774dd4f4d-5t7wv requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod logging-operator-56b5fcc8bd-xkzld requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.065: INFO: Pod loki-0 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod promtail-7pk9g requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.065: INFO: Pod promtail-854st requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod promtail-pfssc requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.065: INFO: Pod alertmanager-main-0 requesting resource cpu=204m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod alertmanager-main-1 requesting resource cpu=204m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.065: INFO: Pod alertmanager-main-2 requesting resource cpu=204m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.065: INFO: Pod blackbox-exporter-685b55655c-bdb7h requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.065: INFO: Pod grafana-69d978c595-lqn8f requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.066: INFO: Pod kube-state-metrics-85476668-r7zjx requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.066: INFO: Pod node-exporter-wh467 requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.066: INFO: Pod node-exporter-wzvwt requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.066: INFO: Pod node-exporter-zn785 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.066: INFO: Pod prometheus-k8s-0 requesting resource cpu=450m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.066: INFO: Pod prometheus-operator-6984ff874-jwfxk requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.066: INFO: Pod event-controller-7db654d86f-4l24d requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.066: INFO: Pod fluent-bit-95snl requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.066: INFO: Pod fluent-bit-lc7xf requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.066: INFO: Pod fluent-bit-w54dc requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.066: INFO: Pod metric-controller-7b78cd6c98-g275p requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.066: INFO: Pod node-exporter-77jkw requesting resource cpu=10m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod node-exporter-f8q68 requesting resource cpu=10m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.067: INFO: Pod node-exporter-ngrrc requesting resource cpu=10m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.067: INFO: Pod observability-manager-fc646c85f-mf72v requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod sink-controller-78ccbc6f4d-gjph5 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod telegraf-4hnb8 requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.067: INFO: Pod telegraf-4w56v requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod telegraf-g9948 requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.067: INFO: Pod validator-5675b6544f-pvrdh requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.067: INFO: Pod pomerium-58fb8f9f6-g77zd requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod pomerium-66c956fc58-df627 requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod registry-creds-registry-creds-controller-c6879445d-mhvtd requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.067: INFO: Pod sealed-secrets-controller-856cd994fb-xzcjz requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.068: INFO: Pod sonobuoy requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.068: INFO: Pod sonobuoy-e2e-job-4a9382b33c5d4cba requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-477351c763374666-qb7jc requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-477351c763374666-wtc8k requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25 requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.068: INFO: Pod restic-55bfd requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.068: INFO: Pod restic-pm729 requesting resource cpu=0m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.068: INFO: Pod restic-wq8qp requesting resource cpu=0m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
Dec  2 11:24:28.068: INFO: Pod velero-69f6b65985-bqq8v requesting resource cpu=0m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
STEP: Starting Pods to consume most of the cluster CPU.
Dec  2 11:24:28.068: INFO: Creating a pod which consumes cpu=2265m on Node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
Dec  2 11:24:28.087: INFO: Creating a pod which consumes cpu=2723m on Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
Dec  2 11:24:28.106: INFO: Creating a pod which consumes cpu=2087m on Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3.172cf634b65bb341], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8410/filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3 to d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3.172cf63667af83c0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3.172cf6366bd0106c], Reason = [Created], Message = [Created container filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3.172cf6367852aec9], Reason = [Started], Message = [Started container filler-pod-1891cca7-d484-4fea-83dd-b1da291252a3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead.172cf634b8a8c843], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8410/filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead to a11041ca-3d0d-4c61-a7c9-2e5b598977b5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead.172cf63a041a7142], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead.172cf63a0805662b], Reason = [Created], Message = [Created container filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead.172cf63a132fe37c], Reason = [Started], Message = [Started container filler-pod-2d7c7960-3b97-4a15-b2bc-28b3f3b95ead]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521168ad-81b6-4d28-8914-f49055a21583.172cf634b8169662], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8410/filler-pod-521168ad-81b6-4d28-8914-f49055a21583 to 5e47d0ea-e90b-466b-b6de-2748d512ebf3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521168ad-81b6-4d28-8914-f49055a21583.172cf63603f7c5df], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521168ad-81b6-4d28-8914-f49055a21583.172cf63607bcf0d2], Reason = [Created], Message = [Created container filler-pod-521168ad-81b6-4d28-8914-f49055a21583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-521168ad-81b6-4d28-8914-f49055a21583.172cf63611add1b3], Reason = [Started], Message = [Started container filler-pod-521168ad-81b6-4d28-8914-f49055a21583]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.172cf63a51ec4227], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node a11041ca-3d0d-4c61-a7c9-2e5b598977b5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:24:53.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8410" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:27.039 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":306,"skipped":5135,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:24:53.794: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 11:24:54.232110      23 warnings.go:70] No static IP address has been configured for the namespace "services-8694", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-8694
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8694 to expose endpoints map[]
Dec  2 11:24:54.909: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec  2 11:24:55.923: INFO: successfully validated that service endpoint-test2 in namespace services-8694 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8694
Dec  2 11:24:55.947: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:24:57.954: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:24:59.955: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:01.955: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8694 to expose endpoints map[pod1:[80]]
Dec  2 11:25:01.985: INFO: successfully validated that service endpoint-test2 in namespace services-8694 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Dec  2 11:25:01.985: INFO: Creating new exec pod
Dec  2 11:25:15.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  2 11:25:15.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:15.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:25:15.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.225.73 80'
Dec  2 11:25:15.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.225.73 80\nConnection to 10.100.225.73 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:15.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8694
Dec  2 11:25:15.522: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:17.529: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:19.530: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:21.530: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:23.530: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:25:25.532: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8694 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  2 11:25:25.567: INFO: successfully validated that service endpoint-test2 in namespace services-8694 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Dec  2 11:25:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  2 11:25:26.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:26.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:25:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.225.73 80'
Dec  2 11:25:26.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.225.73 80\nConnection to 10.100.225.73 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:26.959: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8694
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8694 to expose endpoints map[pod2:[80]]
Dec  2 11:25:27.020: INFO: successfully validated that service endpoint-test2 in namespace services-8694 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Dec  2 11:25:28.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  2 11:25:28.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:28.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:25:28.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-8694 exec execpodssm88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.225.73 80'
Dec  2 11:25:28.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.225.73 80\nConnection to 10.100.225.73 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:28.429: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8694
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8694 to expose endpoints map[]
Dec  2 11:25:28.491: INFO: successfully validated that service endpoint-test2 in namespace services-8694 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:25:28.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8694" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:35.670 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":307,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:25:29.464: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 11:25:29.932944      23 warnings.go:70] No static IP address has been configured for the namespace "services-7961", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7961
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7961
I1202 11:25:31.134035      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7961, replica count: 2
I1202 11:25:34.184833      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 11:25:37.185: INFO: Creating new exec pod
I1202 11:25:37.185176      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 11:25:46.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  2 11:25:46.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:46.420: INFO: stdout: "externalname-service-4nfkg"
Dec  2 11:25:46.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.215.246 80'
Dec  2 11:25:46.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.215.246 80\nConnection to 10.100.215.246 80 port [tcp/http] succeeded!\n"
Dec  2 11:25:46.622: INFO: stdout: "externalname-service-4nfkg"
Dec  2 11:25:46.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 30469'
Dec  2 11:25:46.823: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 30469\nConnection to 11.0.95.5 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:46.823: INFO: stdout: ""
Dec  2 11:25:47.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 30469'
Dec  2 11:25:48.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 30469\nConnection to 11.0.95.5 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:48.043: INFO: stdout: ""
Dec  2 11:25:48.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 30469'
Dec  2 11:25:49.054: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 30469\nConnection to 11.0.95.5 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:49.054: INFO: stdout: ""
Dec  2 11:25:49.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.5 30469'
Dec  2 11:25:50.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.5 30469\nConnection to 11.0.95.5 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:50.024: INFO: stdout: "externalname-service-lg2w2"
Dec  2 11:25:50.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 30469'
Dec  2 11:25:50.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 30469\nConnection to 11.0.95.6 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:50.215: INFO: stdout: ""
Dec  2 11:25:51.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 30469'
Dec  2 11:25:51.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 30469\nConnection to 11.0.95.6 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:51.447: INFO: stdout: ""
Dec  2 11:25:52.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 30469'
Dec  2 11:25:52.424: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 30469\nConnection to 11.0.95.6 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:52.424: INFO: stdout: ""
Dec  2 11:25:53.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-7961 exec execpodrwxf7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 11.0.95.6 30469'
Dec  2 11:25:53.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 11.0.95.6 30469\nConnection to 11.0.95.6 30469 port [tcp/*] succeeded!\n"
Dec  2 11:25:53.428: INFO: stdout: "externalname-service-lg2w2"
Dec  2 11:25:53.428: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:25:53.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7961" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:25.304 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":308,"skipped":5162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:25:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename dns
W1202 11:25:55.318181      23 warnings.go:70] No static IP address has been configured for the namespace "dns-4486", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4486.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4486.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  2 11:26:03.611: INFO: DNS probes using dns-4486/dns-test-a4738fad-d099-4826-b521-7946e00859e2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:26:03.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4486" for this suite.

• [SLOW TEST:9.349 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":309,"skipped":5199,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:26:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubelet-test
W1202 11:26:04.617127      23 warnings.go:70] No static IP address has been configured for the namespace "kubelet-test-4345", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:26:04.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4345" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:26:05.331: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 11:26:05.746240      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-1871", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1871
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec  2 11:26:06.456: INFO: Found 0 stateful pods, waiting for 3
Dec  2 11:26:16.465: INFO: Found 2 stateful pods, waiting for 3
Dec  2 11:26:26.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:26.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:26.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:26:36.465: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:36.465: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:36.465: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:26:46.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:46.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:46.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:26:56.468: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:56.468: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:26:56.468: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec  2 11:26:56.518: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  2 11:27:06.576: INFO: Updating stateful set ss2
Dec  2 11:27:06.590: INFO: Waiting for Pod statefulset-1871/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Dec  2 11:27:16.688: INFO: Found 2 stateful pods, waiting for 3
Dec  2 11:27:26.697: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:26.697: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:26.697: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:27:36.697: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:36.697: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:36.697: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:27:46.697: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:46.697: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:46.697: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  2 11:27:56.698: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:56.699: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  2 11:27:56.699: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  2 11:27:56.744: INFO: Updating stateful set ss2
Dec  2 11:27:56.766: INFO: Waiting for Pod statefulset-1871/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  2 11:28:06.807: INFO: Updating stateful set ss2
Dec  2 11:28:06.819: INFO: Waiting for StatefulSet statefulset-1871/ss2 to complete update
Dec  2 11:28:06.819: INFO: Waiting for Pod statefulset-1871/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 11:28:16.837: INFO: Deleting all statefulset in ns statefulset-1871
Dec  2 11:28:16.843: INFO: Scaling statefulset ss2 to 0
Dec  2 11:28:26.876: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 11:28:26.882: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:28:26.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1871" for this suite.

• [SLOW TEST:142.040 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":311,"skipped":5252,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:28:27.371: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-lifecycle-hook
W1202 11:28:27.835302      23 warnings.go:70] No static IP address has been configured for the namespace "container-lifecycle-hook-3863", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3863
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  2 11:28:28.049: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:28:30.056: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:28:32.057: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:28:34.059: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  2 11:28:34.088: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:28:36.095: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:28:38.095: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec  2 11:28:38.120: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  2 11:28:38.128: INFO: Pod pod-with-prestop-http-hook still exists
Dec  2 11:28:40.129: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  2 11:28:40.141: INFO: Pod pod-with-prestop-http-hook still exists
Dec  2 11:28:42.128: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  2 11:28:42.134: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:28:42.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3863" for this suite.

• [SLOW TEST:15.216 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5262,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:28:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename statefulset
W1202 11:28:43.066686      23 warnings.go:70] No static IP address has been configured for the namespace "statefulset-5660", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5660
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5660
Dec  2 11:28:43.744: INFO: Found 0 stateful pods, waiting for 1
Dec  2 11:28:53.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  2 11:28:53.820: INFO: Deleting all statefulset in ns statefulset-5660
Dec  2 11:28:53.829: INFO: Scaling statefulset ss to 0
Dec  2 11:29:03.921: INFO: Waiting for statefulset status.replicas updated to 0
Dec  2 11:29:03.928: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:29:03.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5660" for this suite.

• [SLOW TEST:21.877 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":313,"skipped":5268,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:29:04.467: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:29:04.963028      23 warnings.go:70] No static IP address has been configured for the namespace "projected-6401", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:29:05.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8" in namespace "projected-6401" to be "Succeeded or Failed"
Dec  2 11:29:05.181: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261496ms
Dec  2 11:29:07.193: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019017894s
Dec  2 11:29:09.205: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031316796s
Dec  2 11:29:11.213: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039026524s
Dec  2 11:29:13.220: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045780611s
Dec  2 11:29:15.229: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.055037636s
STEP: Saw pod success
Dec  2 11:29:15.229: INFO: Pod "downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8" satisfied condition "Succeeded or Failed"
Dec  2 11:29:15.236: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8 container client-container: <nil>
STEP: delete the pod
Dec  2 11:29:15.279: INFO: Waiting for pod downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8 to disappear
Dec  2 11:29:15.286: INFO: Pod downwardapi-volume-98eef2fa-a160-48f7-a6bf-912328fc91f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:29:15.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6401" for this suite.

• [SLOW TEST:11.361 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:29:15.830: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:29:16.302023      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9521", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:29:16.941518      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9521", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:29:17.419964      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-9521-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:29:17.901: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  2 11:29:19.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577358, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577358, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577358, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577357, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:29:23.423: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:29:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1427-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:29:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9521" for this suite.
STEP: Destroying namespace "webhook-9521-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.224 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":315,"skipped":5334,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:29:33.055: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-pred
W1202 11:29:33.484048      23 warnings.go:70] No static IP address has been configured for the namespace "sched-pred-3848", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  2 11:29:33.652: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  2 11:29:33.670: INFO: Waiting for terminating namespaces to be deleted...
Dec  2 11:29:33.693: INFO: 
Logging pods the apiserver thinks is on node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 before test
Dec  2 11:29:33.805: INFO: coredns-6c4c59f5b4-fgkn9 from kube-system started at 2022-12-02 10:36:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:33.805: INFO: promtail-pfssc from logging started at 2022-12-02 10:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.805: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:33.805: INFO: node-exporter-wh467 from monitoring started at 2022-12-01 00:27:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:33.805: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:33.805: INFO: fluent-bit-95snl from pks-system started at 2022-12-02 10:36:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:33.805: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:33.805: INFO: node-exporter-ngrrc from pks-system started at 2022-12-02 10:36:34 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:33.805: INFO: telegraf-g9948 from pks-system started at 2022-12-02 10:36:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:33.805: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25 from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:33.805: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:33.805: INFO: restic-pm729 from velero started at 2022-12-02 10:36:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.805: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:33.805: INFO: 
Logging pods the apiserver thinks is on node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 before test
Dec  2 11:29:33.935: INFO: argo-server-78c9599f9-phpdk from argo started at 2022-12-01 00:07:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container argo-server ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: cert-manager-6bdfc96d57-j8xtt from cert-manager started at 2022-12-02 09:50:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:33.935: INFO: code-server-68cdb99985-76lzq from code-server started at 2022-12-01 00:08:06 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container code-server ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-jobservice-67df6d9455-6d4w5 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-trivy-0 from harbor-restored-1 started at 2022-12-01 00:07:25 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-exporter-6fcd6687f5-tg4sq from harbor-restored-100 started at 2022-12-01 00:07:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-jobservice-67df6d9455-nrds9 from harbor-restored-100 started at 2022-12-01 00:07:44 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-registry-566bccf488-cqlc2 from harbor-restored-100 started at 2022-12-01 00:08:12 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-trivy-0 from harbor-restored-100 started at 2022-12-01 00:10:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-core-66c67cf548-htx9b from harbor-restored-2 started at 2022-12-01 00:08:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-jobservice-67df6d9455-qnhcr from harbor-restored-2 started at 2022-12-01 00:07:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-portal-58c8bb4658-b9ccp from harbor-restored-2 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-registry-566bccf488-2294p from harbor-restored-2 started at 2022-12-01 00:07:48 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-core-66c67cf548-2srb6 from harbor-restored-600 started at 2022-12-01 00:07:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-exporter-6fcd6687f5-zttc4 from harbor-restored-600 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.935: INFO: harbor-portal-58c8bb4658-bf9pv from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.935: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-registry-566bccf488-4r7nw from harbor-restored-600 started at 2022-12-01 00:07:31 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-trivy-0 from harbor-restored-600 started at 2022-12-01 00:11:19 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-exporter-6fcd6687f5-g9cgh from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-jobservice-67df6d9455-cr5z2 from harbor-restored-700 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-portal-58c8bb4658-mqpqt from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-trivy-0 from harbor-restored-700 started at 2022-12-01 00:36:30 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-exporter-6fcd6687f5-lph6q from harbor-restored-701 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-jobservice-67df6d9455-4rj79 from harbor-restored-701 started at 2022-12-01 00:08:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-portal-58c8bb4658-kgnn2 from harbor-restored-701 started at 2022-12-01 00:07:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-trivy-0 from harbor-restored-701 started at 2022-12-01 00:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.938: INFO: harbor-jobservice-67df6d9455-mmml7 from harbor-restored-702 started at 2022-12-01 00:07:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.938: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.940: INFO: harbor-registry-566bccf488-dw7qz from harbor-restored-702 started at 2022-12-01 00:08:11 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.940: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.940: INFO: harbor-core-66c67cf548-4g4zb from harbor-restored-703 started at 2022-12-01 00:08:15 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.940: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: harbor-exporter-6fcd6687f5-wrxhf from harbor-restored-703 started at 2022-12-01 00:08:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.940: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: harbor-jobservice-67df6d9455-gzs5l from harbor-restored-703 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.940: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.940: INFO: harbor-redis-0 from harbor-restored-703 started at 2022-12-01 00:07:50 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.940: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.940: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:33.941: INFO: harbor-registry-566bccf488-dmrc5 from harbor-restored-703 started at 2022-12-01 00:08:59 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.941: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.941: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.941: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.941: INFO: harbor-trivy-0 from harbor-restored-703 started at 2022-12-01 00:10:12 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.941: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.941: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.941: INFO: harbor-core-66c67cf548-qxk8k from harbor-restored-705 started at 2022-12-01 00:08:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-exporter-6fcd6687f5-2vsd9 from harbor-restored-705 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-registry-566bccf488-ng244 from harbor-restored-705 started at 2022-12-01 00:09:00 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-trivy-0 from harbor-restored-705 started at 2022-12-01 00:36:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-database-0 from harbor-restored-706 started at 2022-12-01 00:07:40 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-exporter-6fcd6687f5-fqhxs from harbor-restored-706 started at 2022-12-01 00:07:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-portal-58c8bb4658-s8d6h from harbor-restored-706 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.943: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.943: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:33.943: INFO: harbor-redis-0 from harbor-restored-706 started at 2022-12-01 00:10:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-registry-566bccf488-fsj6q from harbor-restored-706 started at 2022-12-01 00:08:19 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-trivy-0 from harbor-restored-706 started at 2022-12-01 00:10:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-core-66c67cf548-jv9kv from harbor-restored-710 started at 2022-12-01 00:07:49 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-jobservice-67df6d9455-l79z9 from harbor-restored-710 started at 2022-12-01 00:07:22 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-trivy-0 from harbor-restored-710 started at 2022-12-01 00:08:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-database-0 from harbor started at 2022-12-01 00:06:05 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-exporter-6fcd6687f5-m98lw from harbor started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-jobservice-67df6d9455-bp2t7 from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-registry-566bccf488-bcjg8 from harbor started at 2022-12-01 00:08:08 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:33.944: INFO: harbor-trivy-0 from harbor started at 2022-12-01 00:08:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: istiod-57c4757749-hs8dc from istio-system started at 2022-12-01 00:48:17 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container discovery ready: true, restart count 0
Dec  2 11:29:33.944: INFO: kiali-operator-9d8cbb844-rgrkz from istio-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container operator ready: true, restart count 0
Dec  2 11:29:33.944: INFO: coredns-6c4c59f5b4-djxn2 from kube-system started at 2022-12-01 00:32:29 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:33.944: INFO: metrics-server-757c77bdc5-2ggxp from kube-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container metrics-server ready: true, restart count 0
Dec  2 11:29:33.944: INFO: logging-operator-56b5fcc8bd-xkzld from logging started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container logging-operator ready: false, restart count 0
Dec  2 11:29:33.944: INFO: promtail-7pk9g from logging started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:33.944: INFO: alertmanager-main-1 from monitoring started at 2022-12-01 00:09:05 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: alertmanager-main-2 from monitoring started at 2022-12-02 09:50:38 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: kube-state-metrics-85476668-r7zjx from monitoring started at 2022-12-01 00:07:34 +0000 UTC (4 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container kube-rbac-proxy-main ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container kube-rbac-proxy-self ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container kube-state-metrics ready: false, restart count 0
Dec  2 11:29:33.944: INFO: node-exporter-wzvwt from monitoring started at 2022-11-30 23:36:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:33.944: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:33.944: INFO: prometheus-operator-6984ff874-jwfxk from monitoring started at 2022-12-02 09:50:26 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:29:33.944: INFO: 	Container prometheus-operator ready: false, restart count 0
Dec  2 11:29:33.944: INFO: event-controller-7db654d86f-4l24d from pks-system started at 2022-12-02 09:50:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container event-controller ready: true, restart count 0
Dec  2 11:29:33.944: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:33.944: INFO: fluent-bit-lc7xf from pks-system started at 2022-12-02 09:52:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:33.944: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:33.944: INFO: node-exporter-f8q68 from pks-system started at 2022-12-01 00:33:40 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:33.944: INFO: telegraf-4hnb8 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:33.944: INFO: validator-5675b6544f-pvrdh from pks-system started at 2022-12-02 09:50:59 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container validator ready: true, restart count 0
Dec  2 11:29:33.944: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-qb7jc from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:33.944: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:33.944: INFO: restic-wq8qp from velero started at 2022-11-30 23:37:03 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:33.944: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:33.944: INFO: 
Logging pods the apiserver thinks is on node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa before test
Dec  2 11:29:34.091: INFO: workflow-controller-c8c847777-chkd8 from argo started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.091: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.091: INFO: 	Container workflow-controller ready: false, restart count 0
Dec  2 11:29:34.091: INFO: cert-manager-cainjector-786868d5bb-9s22p from cert-manager started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.091: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:34.091: INFO: cert-manager-webhook-5fd474b69d-xrpgh from cert-manager started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.091: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:34.092: INFO: harbor-core-66c67cf548-r2496 from harbor-restored-1 started at 2022-12-01 00:08:14 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.092: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.092: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.092: INFO: harbor-database-0 from harbor-restored-1 started at 2022-12-01 00:07:47 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.092: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.092: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.092: INFO: harbor-exporter-6fcd6687f5-9g4ql from harbor-restored-1 started at 2022-12-01 00:09:23 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.092: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:34.092: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.092: INFO: harbor-portal-58c8bb4658-ng6v7 from harbor-restored-1 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.093: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.093: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.093: INFO: harbor-redis-0 from harbor-restored-1 started at 2022-12-01 00:09:57 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.093: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.093: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.093: INFO: harbor-registry-566bccf488-2twt7 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.093: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.093: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:34.093: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:34.094: INFO: harbor-core-66c67cf548-wklw5 from harbor-restored-100 started at 2022-12-01 00:09:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.094: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.094: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.094: INFO: harbor-database-0 from harbor-restored-100 started at 2022-12-01 00:08:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.094: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.094: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.094: INFO: harbor-portal-58c8bb4658-rxsk5 from harbor-restored-100 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.094: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.094: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.095: INFO: harbor-redis-0 from harbor-restored-100 started at 2022-12-01 00:08:35 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.095: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.095: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.095: INFO: harbor-database-0 from harbor-restored-2 started at 2022-12-01 00:09:41 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.095: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.095: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.095: INFO: harbor-exporter-6fcd6687f5-l5zmg from harbor-restored-2 started at 2022-12-01 00:08:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.095: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:34.095: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.096: INFO: harbor-redis-0 from harbor-restored-2 started at 2022-12-01 00:08:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.096: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.096: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.096: INFO: harbor-trivy-0 from harbor-restored-2 started at 2022-12-02 09:50:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.096: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.096: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:34.096: INFO: harbor-database-0 from harbor-restored-600 started at 2022-12-01 00:09:59 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.096: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.096: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.096: INFO: harbor-jobservice-67df6d9455-zmxln from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.097: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.097: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:34.097: INFO: harbor-redis-0 from harbor-restored-600 started at 2022-12-01 00:07:51 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.097: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.097: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.097: INFO: harbor-core-66c67cf548-6wt2b from harbor-restored-700 started at 2022-12-01 00:09:17 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.097: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.097: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.098: INFO: harbor-database-0 from harbor-restored-700 started at 2022-12-01 00:10:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.098: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.098: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.098: INFO: harbor-redis-0 from harbor-restored-700 started at 2022-12-01 00:08:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.098: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.098: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.098: INFO: harbor-registry-566bccf488-zzvj7 from harbor-restored-700 started at 2022-12-01 00:08:51 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.098: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.098: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:34.099: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:34.099: INFO: harbor-core-66c67cf548-bnjrn from harbor-restored-701 started at 2022-12-01 00:08:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.099: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.099: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.099: INFO: harbor-database-0 from harbor-restored-701 started at 2022-12-01 00:09:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.099: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.099: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.100: INFO: harbor-redis-0 from harbor-restored-701 started at 2022-12-01 00:07:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.100: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.100: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.100: INFO: harbor-registry-566bccf488-4scbl from harbor-restored-701 started at 2022-12-01 00:07:53 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.100: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.100: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:34.100: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:34.100: INFO: harbor-core-66c67cf548-f5fjz from harbor-restored-702 started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.101: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.101: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.101: INFO: harbor-database-0 from harbor-restored-702 started at 2022-12-01 00:10:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.101: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.101: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.101: INFO: harbor-exporter-6fcd6687f5-v5tz8 from harbor-restored-702 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.101: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:34.101: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.101: INFO: harbor-portal-58c8bb4658-zzshz from harbor-restored-702 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.101: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.102: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.102: INFO: harbor-redis-0 from harbor-restored-702 started at 2022-12-02 09:50:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.102: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.102: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.102: INFO: harbor-trivy-0 from harbor-restored-702 started at 2022-12-02 09:50:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.102: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.102: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:34.102: INFO: harbor-database-0 from harbor-restored-703 started at 2022-12-01 00:09:09 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.102: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.103: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.103: INFO: harbor-portal-58c8bb4658-7gmp5 from harbor-restored-703 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.103: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.103: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.103: INFO: harbor-database-0 from harbor-restored-705 started at 2022-12-01 00:10:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.103: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.103: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.103: INFO: harbor-jobservice-67df6d9455-4lg98 from harbor-restored-705 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.103: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.103: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:34.104: INFO: harbor-portal-58c8bb4658-9tfsd from harbor-restored-705 started at 2022-12-01 00:09:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.104: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.104: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.104: INFO: harbor-redis-0 from harbor-restored-705 started at 2022-11-30 19:09:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.104: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.104: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.104: INFO: harbor-core-66c67cf548-wnj7c from harbor-restored-706 started at 2022-12-01 00:08:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.104: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.104: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.104: INFO: harbor-jobservice-67df6d9455-4tzqq from harbor-restored-706 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.105: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.105: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:34.105: INFO: harbor-database-0 from harbor-restored-710 started at 2022-12-01 00:08:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.105: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:34.105: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.105: INFO: harbor-exporter-6fcd6687f5-zzfwt from harbor-restored-710 started at 2022-12-01 00:08:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.105: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:34.105: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.105: INFO: harbor-portal-58c8bb4658-htz6g from harbor-restored-710 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.106: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.106: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.106: INFO: harbor-redis-0 from harbor-restored-710 started at 2022-12-01 00:09:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.106: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.106: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.106: INFO: harbor-registry-566bccf488-bzh9n from harbor-restored-710 started at 2022-12-01 00:08:28 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.106: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.106: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:34.106: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:34.107: INFO: harbor-core-66c67cf548-8bjrq from harbor started at 2022-12-01 00:09:34 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.107: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:34.107: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.107: INFO: harbor-portal-58c8bb4658-6x22v from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.107: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.107: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:34.107: INFO: harbor-redis-0 from harbor started at 2022-12-01 00:08:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.107: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.107: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:34.107: INFO: ingress-gateway-management-c6c49cf-2gtg5 from ingress-gateway-management started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.108: INFO: 	Container istio-proxy ready: true, restart count 0
Dec  2 11:29:34.108: INFO: istio-operator-67b8b6c9dd-bqlxh from istio-operator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.108: INFO: 	Container istio-operator ready: true, restart count 0
Dec  2 11:29:34.108: INFO: keycloak-0 from keycloak started at 2022-11-30 18:59:19 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.108: INFO: 	Container keycloak ready: false, restart count 0
Dec  2 11:29:34.108: INFO: keycloak-operator-76c854b9f4-ts46s from keycloak started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.108: INFO: 	Container keycloak-operator ready: true, restart count 0
Dec  2 11:29:34.108: INFO: keycloak-postgresql-746d7cc6dd-rzdpf from keycloak started at 2022-11-30 19:10:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.109: INFO: 	Container keycloak-postgresql ready: true, restart count 0
Dec  2 11:29:34.109: INFO: docs-67854bcfd4-kfwsh from kube-plus-docs started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.109: INFO: 	Container docs ready: false, restart count 0
Dec  2 11:29:34.110: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.110: INFO: coredns-6c4c59f5b4-knwtr from kube-system started at 2022-12-01 00:36:09 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.110: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:34.110: INFO: replicator-kubernetes-replicator-6774dd4f4d-5t7wv from kubernetes-replicator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.110: INFO: 	Container kubernetes-replicator ready: true, restart count 0
Dec  2 11:29:34.110: INFO: loki-0 from logging started at 2022-12-02 09:50:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.110: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.110: INFO: 	Container loki ready: false, restart count 0
Dec  2 11:29:34.111: INFO: promtail-854st from logging started at 2022-12-01 00:09:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.111: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.111: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:34.111: INFO: alertmanager-main-0 from monitoring started at 2022-12-01 00:07:33 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.111: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:34.111: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:34.111: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.111: INFO: blackbox-exporter-685b55655c-bdb7h from monitoring started at 2022-12-02 09:50:26 +0000 UTC (4 container statuses recorded)
Dec  2 11:29:34.111: INFO: 	Container blackbox-exporter ready: false, restart count 0
Dec  2 11:29:34.112: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.112: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:29:34.112: INFO: 	Container module-configmap-reloader ready: false, restart count 0
Dec  2 11:29:34.112: INFO: grafana-69d978c595-lqn8f from monitoring started at 2022-12-01 00:09:02 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.112: INFO: 	Container grafana ready: false, restart count 0
Dec  2 11:29:34.112: INFO: 	Container grafana-sc-dashboard ready: false, restart count 0
Dec  2 11:29:34.112: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.112: INFO: node-exporter-zn785 from monitoring started at 2022-11-30 18:58:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.113: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:34.113: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:34.113: INFO: prometheus-k8s-0 from monitoring started at 2022-12-02 09:50:58 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:34.113: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:34.113: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.113: INFO: 	Container prometheus ready: false, restart count 0
Dec  2 11:29:34.113: INFO: fluent-bit-w54dc from pks-system started at 2022-12-02 09:52:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.113: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:34.113: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:34.113: INFO: metric-controller-7b78cd6c98-g275p from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.113: INFO: 	Container metric-controller ready: true, restart count 0
Dec  2 11:29:34.113: INFO: node-exporter-77jkw from pks-system started at 2022-12-01 00:33:16 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:34.114: INFO: observability-manager-fc646c85f-mf72v from pks-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container observability-manager ready: true, restart count 0
Dec  2 11:29:34.114: INFO: sink-controller-78ccbc6f4d-gjph5 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container sink-controller ready: true, restart count 0
Dec  2 11:29:34.114: INFO: telegraf-4w56v from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:34.114: INFO: pomerium-58fb8f9f6-g77zd from pomerium started at 2022-12-01 00:56:56 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.114: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:29:34.114: INFO: pomerium-66c956fc58-df627 from pomerium started at 2022-12-01 01:00:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.114: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:34.114: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:29:34.115: INFO: registry-creds-registry-creds-controller-c6879445d-mhvtd from registry-creds started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.115: INFO: 	Container controller ready: true, restart count 0
Dec  2 11:29:34.115: INFO: sealed-secrets-controller-856cd994fb-xzcjz from sealed-secrets started at 2022-11-30 18:58:55 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.115: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Dec  2 11:29:34.115: INFO: sonobuoy from sonobuoy started at 2022-12-02 09:03:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.115: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  2 11:29:34.115: INFO: sonobuoy-e2e-job-4a9382b33c5d4cba from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.115: INFO: 	Container e2e ready: true, restart count 0
Dec  2 11:29:34.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:34.115: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-wtc8k from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:34.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:34.115: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:34.116: INFO: restic-55bfd from velero started at 2022-11-30 18:58:22 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.116: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:34.116: INFO: velero-69f6b65985-bqq8v from velero started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:34.116: INFO: 	Container velero ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.172cf67c0a24b368], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:29:35.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3848" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":316,"skipped":5337,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:29:35.929: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename sched-pred
W1202 11:29:36.440020      23 warnings.go:70] No static IP address has been configured for the namespace "sched-pred-8742", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  2 11:29:36.625: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  2 11:29:36.646: INFO: Waiting for terminating namespaces to be deleted...
Dec  2 11:29:36.662: INFO: 
Logging pods the apiserver thinks is on node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 before test
Dec  2 11:29:36.739: INFO: coredns-6c4c59f5b4-fgkn9 from kube-system started at 2022-12-02 10:36:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.739: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:36.739: INFO: promtail-pfssc from logging started at 2022-12-02 10:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.739: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.739: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:36.739: INFO: node-exporter-wh467 from monitoring started at 2022-12-01 00:27:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.739: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:36.739: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:36.739: INFO: fluent-bit-95snl from pks-system started at 2022-12-02 10:36:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.739: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:36.739: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:36.739: INFO: node-exporter-ngrrc from pks-system started at 2022-12-02 10:36:34 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.739: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:36.739: INFO: telegraf-g9948 from pks-system started at 2022-12-02 10:36:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.740: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:36.740: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-zmk25 from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:36.740: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:36.740: INFO: restic-pm729 from velero started at 2022-12-02 10:36:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.740: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:36.740: INFO: 
Logging pods the apiserver thinks is on node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 before test
Dec  2 11:29:36.841: INFO: argo-server-78c9599f9-phpdk from argo started at 2022-12-01 00:07:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container argo-server ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: cert-manager-6bdfc96d57-j8xtt from cert-manager started at 2022-12-02 09:50:28 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:36.841: INFO: code-server-68cdb99985-76lzq from code-server started at 2022-12-01 00:08:06 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container code-server ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-6d4w5 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-trivy-0 from harbor-restored-1 started at 2022-12-01 00:07:25 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-exporter-6fcd6687f5-tg4sq from harbor-restored-100 started at 2022-12-01 00:07:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-nrds9 from harbor-restored-100 started at 2022-12-01 00:07:44 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-registry-566bccf488-cqlc2 from harbor-restored-100 started at 2022-12-01 00:08:12 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-trivy-0 from harbor-restored-100 started at 2022-12-01 00:10:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-core-66c67cf548-htx9b from harbor-restored-2 started at 2022-12-01 00:08:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-qnhcr from harbor-restored-2 started at 2022-12-01 00:07:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-portal-58c8bb4658-b9ccp from harbor-restored-2 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-registry-566bccf488-2294p from harbor-restored-2 started at 2022-12-01 00:07:48 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-core-66c67cf548-2srb6 from harbor-restored-600 started at 2022-12-01 00:07:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-exporter-6fcd6687f5-zttc4 from harbor-restored-600 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-portal-58c8bb4658-bf9pv from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-registry-566bccf488-4r7nw from harbor-restored-600 started at 2022-12-01 00:07:31 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-trivy-0 from harbor-restored-600 started at 2022-12-01 00:11:19 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-exporter-6fcd6687f5-g9cgh from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-cr5z2 from harbor-restored-700 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-portal-58c8bb4658-mqpqt from harbor-restored-700 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-trivy-0 from harbor-restored-700 started at 2022-12-01 00:36:30 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-exporter-6fcd6687f5-lph6q from harbor-restored-701 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-4rj79 from harbor-restored-701 started at 2022-12-01 00:08:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-portal-58c8bb4658-kgnn2 from harbor-restored-701 started at 2022-12-01 00:07:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-trivy-0 from harbor-restored-701 started at 2022-12-01 00:36:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-mmml7 from harbor-restored-702 started at 2022-12-01 00:07:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-registry-566bccf488-dw7qz from harbor-restored-702 started at 2022-12-01 00:08:11 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-core-66c67cf548-4g4zb from harbor-restored-703 started at 2022-12-01 00:08:15 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-exporter-6fcd6687f5-wrxhf from harbor-restored-703 started at 2022-12-01 00:08:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.841: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.841: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.841: INFO: harbor-jobservice-67df6d9455-gzs5l from harbor-restored-703 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-redis-0 from harbor-restored-703 started at 2022-12-01 00:07:50 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-registry-566bccf488-dmrc5 from harbor-restored-703 started at 2022-12-01 00:08:59 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-trivy-0 from harbor-restored-703 started at 2022-12-01 00:10:12 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-core-66c67cf548-qxk8k from harbor-restored-705 started at 2022-12-01 00:08:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-exporter-6fcd6687f5-2vsd9 from harbor-restored-705 started at 2022-12-01 00:07:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-registry-566bccf488-ng244 from harbor-restored-705 started at 2022-12-01 00:09:00 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-trivy-0 from harbor-restored-705 started at 2022-12-01 00:36:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-database-0 from harbor-restored-706 started at 2022-12-01 00:07:40 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-exporter-6fcd6687f5-fqhxs from harbor-restored-706 started at 2022-12-01 00:07:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-portal-58c8bb4658-s8d6h from harbor-restored-706 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-redis-0 from harbor-restored-706 started at 2022-12-01 00:10:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-registry-566bccf488-fsj6q from harbor-restored-706 started at 2022-12-01 00:08:19 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-trivy-0 from harbor-restored-706 started at 2022-12-01 00:10:32 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-core-66c67cf548-jv9kv from harbor-restored-710 started at 2022-12-01 00:07:49 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-jobservice-67df6d9455-l79z9 from harbor-restored-710 started at 2022-12-01 00:07:22 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-trivy-0 from harbor-restored-710 started at 2022-12-01 00:08:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-database-0 from harbor started at 2022-12-01 00:06:05 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-exporter-6fcd6687f5-m98lw from harbor started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-jobservice-67df6d9455-bp2t7 from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-registry-566bccf488-bcjg8 from harbor started at 2022-12-01 00:08:08 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.842: INFO: harbor-trivy-0 from harbor started at 2022-12-01 00:08:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: istiod-57c4757749-hs8dc from istio-system started at 2022-12-01 00:48:17 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container discovery ready: true, restart count 0
Dec  2 11:29:36.842: INFO: kiali-operator-9d8cbb844-rgrkz from istio-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container operator ready: true, restart count 0
Dec  2 11:29:36.842: INFO: coredns-6c4c59f5b4-djxn2 from kube-system started at 2022-12-01 00:32:29 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:36.842: INFO: metrics-server-757c77bdc5-2ggxp from kube-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container metrics-server ready: true, restart count 0
Dec  2 11:29:36.842: INFO: logging-operator-56b5fcc8bd-xkzld from logging started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container logging-operator ready: false, restart count 0
Dec  2 11:29:36.842: INFO: promtail-7pk9g from logging started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:36.842: INFO: alertmanager-main-1 from monitoring started at 2022-12-01 00:09:05 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: alertmanager-main-2 from monitoring started at 2022-12-02 09:50:38 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: kube-state-metrics-85476668-r7zjx from monitoring started at 2022-12-01 00:07:34 +0000 UTC (4 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container kube-rbac-proxy-main ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container kube-rbac-proxy-self ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container kube-state-metrics ready: false, restart count 0
Dec  2 11:29:36.842: INFO: node-exporter-wzvwt from monitoring started at 2022-11-30 23:36:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:36.842: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:36.842: INFO: prometheus-operator-6984ff874-jwfxk from monitoring started at 2022-12-02 09:50:26 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:29:36.842: INFO: 	Container prometheus-operator ready: false, restart count 0
Dec  2 11:29:36.842: INFO: event-controller-7db654d86f-4l24d from pks-system started at 2022-12-02 09:50:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container event-controller ready: true, restart count 0
Dec  2 11:29:36.842: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:36.842: INFO: fluent-bit-lc7xf from pks-system started at 2022-12-02 09:52:18 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:36.842: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:36.842: INFO: node-exporter-f8q68 from pks-system started at 2022-12-01 00:33:40 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:36.842: INFO: telegraf-4hnb8 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:36.842: INFO: validator-5675b6544f-pvrdh from pks-system started at 2022-12-02 09:50:59 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container validator ready: true, restart count 0
Dec  2 11:29:36.842: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-qb7jc from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:36.842: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:36.842: INFO: restic-wq8qp from velero started at 2022-11-30 23:37:03 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.842: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:36.842: INFO: 
Logging pods the apiserver thinks is on node d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa before test
Dec  2 11:29:36.948: INFO: workflow-controller-c8c847777-chkd8 from argo started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container workflow-controller ready: false, restart count 0
Dec  2 11:29:36.948: INFO: cert-manager-cainjector-786868d5bb-9s22p from cert-manager started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:36.948: INFO: cert-manager-webhook-5fd474b69d-xrpgh from cert-manager started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container cert-manager ready: true, restart count 0
Dec  2 11:29:36.948: INFO: harbor-core-66c67cf548-r2496 from harbor-restored-1 started at 2022-12-01 00:08:14 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-database-0 from harbor-restored-1 started at 2022-12-01 00:07:47 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-exporter-6fcd6687f5-9g4ql from harbor-restored-1 started at 2022-12-01 00:09:23 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-portal-58c8bb4658-ng6v7 from harbor-restored-1 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-redis-0 from harbor-restored-1 started at 2022-12-01 00:09:57 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-registry-566bccf488-2twt7 from harbor-restored-1 started at 2022-12-01 00:07:45 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.948: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.948: INFO: harbor-core-66c67cf548-wklw5 from harbor-restored-100 started at 2022-12-01 00:09:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.948: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-100 started at 2022-12-01 00:08:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-rxsk5 from harbor-restored-100 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-100 started at 2022-12-01 00:08:35 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-2 started at 2022-12-01 00:09:41 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-exporter-6fcd6687f5-l5zmg from harbor-restored-2 started at 2022-12-01 00:08:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-2 started at 2022-12-01 00:08:31 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-trivy-0 from harbor-restored-2 started at 2022-12-02 09:50:42 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-600 started at 2022-12-01 00:09:59 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-jobservice-67df6d9455-zmxln from harbor-restored-600 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-600 started at 2022-12-01 00:07:51 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-core-66c67cf548-6wt2b from harbor-restored-700 started at 2022-12-01 00:09:17 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-700 started at 2022-12-01 00:10:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-700 started at 2022-12-01 00:08:58 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-registry-566bccf488-zzvj7 from harbor-restored-700 started at 2022-12-01 00:08:51 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-core-66c67cf548-bnjrn from harbor-restored-701 started at 2022-12-01 00:08:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-701 started at 2022-12-01 00:09:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-701 started at 2022-12-01 00:07:43 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-registry-566bccf488-4scbl from harbor-restored-701 started at 2022-12-01 00:07:53 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-core-66c67cf548-f5fjz from harbor-restored-702 started at 2022-12-01 00:08:33 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-702 started at 2022-12-01 00:10:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-exporter-6fcd6687f5-v5tz8 from harbor-restored-702 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-zzshz from harbor-restored-702 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-702 started at 2022-12-02 09:50:55 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-trivy-0 from harbor-restored-702 started at 2022-12-02 09:50:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container trivy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-703 started at 2022-12-01 00:09:09 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-7gmp5 from harbor-restored-703 started at 2022-12-02 09:50:28 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-705 started at 2022-12-01 00:10:11 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-jobservice-67df6d9455-4lg98 from harbor-restored-705 started at 2022-12-02 09:50:29 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-9tfsd from harbor-restored-705 started at 2022-12-01 00:09:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-705 started at 2022-11-30 19:09:16 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-core-66c67cf548-wnj7c from harbor-restored-706 started at 2022-12-01 00:08:53 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-jobservice-67df6d9455-4tzqq from harbor-restored-706 started at 2022-12-02 09:50:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container jobservice ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-database-0 from harbor-restored-710 started at 2022-12-01 00:08:02 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container database ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-exporter-6fcd6687f5-zzfwt from harbor-restored-710 started at 2022-12-01 00:08:37 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container exporter ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-htz6g from harbor-restored-710 started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor-restored-710 started at 2022-12-01 00:09:54 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-registry-566bccf488-bzh9n from harbor-restored-710 started at 2022-12-01 00:08:28 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registry ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container registryctl ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-core-66c67cf548-8bjrq from harbor started at 2022-12-01 00:09:34 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container core ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-portal-58c8bb4658-6x22v from harbor started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container portal ready: false, restart count 0
Dec  2 11:29:36.949: INFO: harbor-redis-0 from harbor started at 2022-12-01 00:08:27 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container redis ready: false, restart count 0
Dec  2 11:29:36.949: INFO: ingress-gateway-management-c6c49cf-2gtg5 from ingress-gateway-management started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: true, restart count 0
Dec  2 11:29:36.949: INFO: istio-operator-67b8b6c9dd-bqlxh from istio-operator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-operator ready: true, restart count 0
Dec  2 11:29:36.949: INFO: keycloak-0 from keycloak started at 2022-11-30 18:59:19 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container keycloak ready: false, restart count 0
Dec  2 11:29:36.949: INFO: keycloak-operator-76c854b9f4-ts46s from keycloak started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container keycloak-operator ready: true, restart count 0
Dec  2 11:29:36.949: INFO: keycloak-postgresql-746d7cc6dd-rzdpf from keycloak started at 2022-11-30 19:10:31 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container keycloak-postgresql ready: true, restart count 0
Dec  2 11:29:36.949: INFO: docs-67854bcfd4-kfwsh from kube-plus-docs started at 2022-12-02 09:50:26 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container docs ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: coredns-6c4c59f5b4-knwtr from kube-system started at 2022-12-01 00:36:09 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container coredns ready: true, restart count 0
Dec  2 11:29:36.949: INFO: replicator-kubernetes-replicator-6774dd4f4d-5t7wv from kubernetes-replicator started at 2022-11-30 18:58:53 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container kubernetes-replicator ready: true, restart count 0
Dec  2 11:29:36.949: INFO: loki-0 from logging started at 2022-12-02 09:50:45 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container loki ready: false, restart count 0
Dec  2 11:29:36.949: INFO: promtail-854st from logging started at 2022-12-01 00:09:38 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container promtail ready: false, restart count 0
Dec  2 11:29:36.949: INFO: alertmanager-main-0 from monitoring started at 2022-12-01 00:07:33 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container alertmanager ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: blackbox-exporter-685b55655c-bdb7h from monitoring started at 2022-12-02 09:50:26 +0000 UTC (4 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container blackbox-exporter ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container kube-rbac-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container module-configmap-reloader ready: false, restart count 0
Dec  2 11:29:36.949: INFO: grafana-69d978c595-lqn8f from monitoring started at 2022-12-01 00:09:02 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container grafana ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container grafana-sc-dashboard ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: node-exporter-zn785 from monitoring started at 2022-11-30 18:58:13 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  2 11:29:36.949: INFO: 	Container node-exporter ready: true, restart count 0
Dec  2 11:29:36.949: INFO: prometheus-k8s-0 from monitoring started at 2022-12-02 09:50:58 +0000 UTC (3 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container config-reloader ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container prometheus ready: false, restart count 0
Dec  2 11:29:36.949: INFO: fluent-bit-w54dc from pks-system started at 2022-12-02 09:52:24 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container fluent-bit ready: true, restart count 0
Dec  2 11:29:36.949: INFO: 	Container ghostunnel ready: true, restart count 0
Dec  2 11:29:36.949: INFO: metric-controller-7b78cd6c98-g275p from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container metric-controller ready: true, restart count 0
Dec  2 11:29:36.949: INFO: node-exporter-77jkw from pks-system started at 2022-12-01 00:33:16 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  2 11:29:36.949: INFO: observability-manager-fc646c85f-mf72v from pks-system started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container observability-manager ready: true, restart count 0
Dec  2 11:29:36.949: INFO: sink-controller-78ccbc6f4d-gjph5 from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container sink-controller ready: true, restart count 0
Dec  2 11:29:36.949: INFO: telegraf-4w56v from pks-system started at 2022-12-02 09:50:58 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container telegraf ready: true, restart count 0
Dec  2 11:29:36.949: INFO: pomerium-58fb8f9f6-g77zd from pomerium started at 2022-12-01 00:56:56 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:29:36.949: INFO: pomerium-66c956fc58-df627 from pomerium started at 2022-12-01 01:00:04 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container istio-proxy ready: false, restart count 0
Dec  2 11:29:36.949: INFO: 	Container pomerium ready: false, restart count 0
Dec  2 11:29:36.949: INFO: registry-creds-registry-creds-controller-c6879445d-mhvtd from registry-creds started at 2022-11-30 18:58:54 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container controller ready: true, restart count 0
Dec  2 11:29:36.949: INFO: sealed-secrets-controller-856cd994fb-xzcjz from sealed-secrets started at 2022-11-30 18:58:55 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container sealed-secrets-controller ready: true, restart count 0
Dec  2 11:29:36.949: INFO: sonobuoy from sonobuoy started at 2022-12-02 09:03:30 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  2 11:29:36.949: INFO: sonobuoy-e2e-job-4a9382b33c5d4cba from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container e2e ready: true, restart count 0
Dec  2 11:29:36.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:36.949: INFO: sonobuoy-systemd-logs-daemon-set-477351c763374666-wtc8k from sonobuoy started at 2022-12-02 09:03:36 +0000 UTC (2 container statuses recorded)
Dec  2 11:29:36.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  2 11:29:36.949: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  2 11:29:36.949: INFO: restic-55bfd from velero started at 2022-11-30 18:58:22 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.950: INFO: 	Container restic ready: true, restart count 0
Dec  2 11:29:36.950: INFO: velero-69f6b65985-bqq8v from velero started at 2022-12-02 09:50:26 +0000 UTC (1 container statuses recorded)
Dec  2 11:29:36.950: INFO: 	Container velero ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d834ca42-bf14-4eff-8725-07a123c91cf0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 11.0.95.7 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-d834ca42-bf14-4eff-8725-07a123c91cf0 off the node 5e47d0ea-e90b-466b-b6de-2748d512ebf3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d834ca42-bf14-4eff-8725-07a123c91cf0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:34:47.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8742" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:311.718 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":317,"skipped":5349,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:34:47.648: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename init-container
W1202 11:34:48.089443      23 warnings.go:70] No static IP address has been configured for the namespace "init-container-1413", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  2 11:34:48.304: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:34:57.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1413" for this suite.

• [SLOW TEST:10.165 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":318,"skipped":5360,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:34:57.813: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-webhook
W1202 11:34:58.262882      23 warnings.go:70] No static IP address has been configured for the namespace "crd-webhook-6603", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:34:59.438: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec  2 11:35:01.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577699, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577699, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577699, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577699, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85f9c96f6f\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:35:04.914: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:35:04.922: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:35:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6603" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:16.528 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":319,"skipped":5361,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:35:14.342: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 11:35:14.811693      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-2887", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  2 11:35:15.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-2887 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  2 11:35:16.256: INFO: stderr: ""
Dec  2 11:35:16.257: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec  2 11:35:16.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-2887 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Dec  2 11:35:18.790: INFO: stderr: ""
Dec  2 11:35:18.790: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  2 11:35:18.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-2887 delete pods e2e-test-httpd-pod'
Dec  2 11:35:22.781: INFO: stderr: ""
Dec  2 11:35:22.781: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:35:22.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2887" for this suite.

• [SLOW TEST:8.939 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:913
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":320,"skipped":5373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:35:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:35:23.746513      23 warnings.go:70] No static IP address has been configured for the namespace "projected-7076", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7076
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-b3726597-2999-45b7-9a4c-72e0ca9d47bd
STEP: Creating configMap with name cm-test-opt-upd-9af73f00-31c3-4053-8ca5-c018a2fdc43f
STEP: Creating the pod
Dec  2 11:35:24.013: INFO: The status of Pod pod-projected-configmaps-7cc7cf99-341e-4021-b87a-345bfee3e375 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:35:26.021: INFO: The status of Pod pod-projected-configmaps-7cc7cf99-341e-4021-b87a-345bfee3e375 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:35:28.021: INFO: The status of Pod pod-projected-configmaps-7cc7cf99-341e-4021-b87a-345bfee3e375 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:35:30.021: INFO: The status of Pod pod-projected-configmaps-7cc7cf99-341e-4021-b87a-345bfee3e375 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-b3726597-2999-45b7-9a4c-72e0ca9d47bd
STEP: Updating configmap cm-test-opt-upd-9af73f00-31c3-4053-8ca5-c018a2fdc43f
STEP: Creating configMap with name cm-test-opt-create-ee4006cc-19cb-4615-a47f-3b5d0be5e65c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:36:34.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7076" for this suite.

• [SLOW TEST:72.000 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5413,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:36:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 11:36:35.732682      23 warnings.go:70] No static IP address has been configured for the namespace "services-6479", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6479
STEP: creating service affinity-clusterip in namespace services-6479
STEP: creating replication controller affinity-clusterip in namespace services-6479
I1202 11:36:36.392482      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6479, replica count: 3
I1202 11:36:39.443589      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:42.444964      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:45.445651      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:48.446050      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:51.448327      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:54.449654      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:36:57.450083      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:37:00.452482      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:37:03.453624      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:37:06.454412      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1202 11:37:09.454719      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  2 11:37:09.469: INFO: Creating new exec pod
Dec  2 11:37:14.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6479 exec execpod-affinitywpp86 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec  2 11:37:14.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec  2 11:37:14.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:37:14.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6479 exec execpod-affinitywpp86 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.240.127 80'
Dec  2 11:37:14.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.240.127 80\nConnection to 10.100.240.127 80 port [tcp/http] succeeded!\n"
Dec  2 11:37:14.943: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:37:14.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-6479 exec execpod-affinitywpp86 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.240.127:80/ ; done'
Dec  2 11:37:15.300: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.240.127:80/\n"
Dec  2 11:37:15.300: INFO: stdout: "\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz\naffinity-clusterip-dvdqz"
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Received response from host: affinity-clusterip-dvdqz
Dec  2 11:37:15.300: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6479, will wait for the garbage collector to delete the pods
Dec  2 11:37:15.410: INFO: Deleting ReplicationController affinity-clusterip took: 12.691104ms
Dec  2 11:37:15.511: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.725361ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:37:21.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6479" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:46.663 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":322,"skipped":5424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:37:21.946: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename var-expansion
W1202 11:37:22.416210      23 warnings.go:70] No static IP address has been configured for the namespace "var-expansion-7544", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Dec  2 11:37:22.612: INFO: Waiting up to 5m0s for pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315" in namespace "var-expansion-7544" to be "Succeeded or Failed"
Dec  2 11:37:22.619: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315": Phase="Pending", Reason="", readiness=false. Elapsed: 7.60746ms
Dec  2 11:37:24.627: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015195358s
Dec  2 11:37:26.634: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021998538s
Dec  2 11:37:28.641: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02963651s
Dec  2 11:37:30.650: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038607872s
STEP: Saw pod success
Dec  2 11:37:30.650: INFO: Pod "var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315" satisfied condition "Succeeded or Failed"
Dec  2 11:37:30.658: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315 container dapi-container: <nil>
STEP: delete the pod
Dec  2 11:37:30.712: INFO: Waiting for pod var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315 to disappear
Dec  2 11:37:30.719: INFO: Pod var-expansion-f1f6feb0-1325-4f53-a6fc-6f60352b0315 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:37:30.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7544" for this suite.

• [SLOW TEST:9.239 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":5462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:37:31.187: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename crd-publish-openapi
W1202 11:37:31.805413      23 warnings.go:70] No static IP address has been configured for the namespace "crd-publish-openapi-8490", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8490
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec  2 11:37:31.981: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:38:27.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8490" for this suite.

• [SLOW TEST:57.285 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":324,"skipped":5493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:38:28.474: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 11:38:28.970511      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-5141", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-d7221b57-62c4-4b72-9e18-4edb2a6b4888
STEP: Creating a pod to test consume secrets
Dec  2 11:38:29.204: INFO: Waiting up to 5m0s for pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf" in namespace "secrets-5141" to be "Succeeded or Failed"
Dec  2 11:38:29.212: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202683ms
Dec  2 11:38:31.227: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023614897s
Dec  2 11:38:33.240: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036187345s
Dec  2 11:38:35.252: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047957044s
Dec  2 11:38:37.265: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.061572551s
STEP: Saw pod success
Dec  2 11:38:37.265: INFO: Pod "pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf" satisfied condition "Succeeded or Failed"
Dec  2 11:38:37.272: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 11:38:37.329: INFO: Waiting for pod pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf to disappear
Dec  2 11:38:37.336: INFO: Pod pod-secrets-670eda24-9f21-4852-8669-65bf7c6e0fdf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:38:37.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5141" for this suite.

• [SLOW TEST:9.322 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":325,"skipped":5524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:38:37.797: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 11:38:38.311528      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-7159", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  2 11:38:38.509: INFO: Waiting up to 5m0s for pod "pod-715ccc65-516f-433f-8e92-240432294f77" in namespace "emptydir-7159" to be "Succeeded or Failed"
Dec  2 11:38:38.516: INFO: Pod "pod-715ccc65-516f-433f-8e92-240432294f77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.803329ms
Dec  2 11:38:40.531: INFO: Pod "pod-715ccc65-516f-433f-8e92-240432294f77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02200473s
Dec  2 11:38:42.544: INFO: Pod "pod-715ccc65-516f-433f-8e92-240432294f77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034366813s
Dec  2 11:38:44.563: INFO: Pod "pod-715ccc65-516f-433f-8e92-240432294f77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054147165s
STEP: Saw pod success
Dec  2 11:38:44.564: INFO: Pod "pod-715ccc65-516f-433f-8e92-240432294f77" satisfied condition "Succeeded or Failed"
Dec  2 11:38:44.570: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-715ccc65-516f-433f-8e92-240432294f77 container test-container: <nil>
STEP: delete the pod
Dec  2 11:38:44.620: INFO: Waiting for pod pod-715ccc65-516f-433f-8e92-240432294f77 to disappear
Dec  2 11:38:44.625: INFO: Pod pod-715ccc65-516f-433f-8e92-240432294f77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:38:44.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7159" for this suite.

• [SLOW TEST:7.321 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5548,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:38:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename secrets
W1202 11:38:45.534715      23 warnings.go:70] No static IP address has been configured for the namespace "secrets-2086", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3be4324f-d251-47b3-9d53-59a0037d1d63
STEP: Creating a pod to test consume secrets
Dec  2 11:38:45.766: INFO: Waiting up to 5m0s for pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd" in namespace "secrets-2086" to be "Succeeded or Failed"
Dec  2 11:38:45.775: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54116ms
Dec  2 11:38:47.787: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020647899s
Dec  2 11:38:49.799: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03281517s
Dec  2 11:38:51.810: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043509265s
Dec  2 11:38:53.822: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055623265s
STEP: Saw pod success
Dec  2 11:38:53.822: INFO: Pod "pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd" satisfied condition "Succeeded or Failed"
Dec  2 11:38:53.829: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd container secret-volume-test: <nil>
STEP: delete the pod
Dec  2 11:38:53.870: INFO: Waiting for pod pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd to disappear
Dec  2 11:38:53.876: INFO: Pod pod-secrets-9b1d5363-2848-463e-a53e-03a2efd9cfbd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:38:53.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2086" for this suite.

• [SLOW TEST:9.204 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5549,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:38:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename pods
W1202 11:38:54.811390      23 warnings.go:70] No static IP address has been configured for the namespace "pods-3144", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Dec  2 11:38:55.036: INFO: The status of Pod pod-hostip-b998b4be-5f6a-4409-b256-8060aa8f7092 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:38:57.047: INFO: The status of Pod pod-hostip-b998b4be-5f6a-4409-b256-8060aa8f7092 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:38:59.049: INFO: The status of Pod pod-hostip-b998b4be-5f6a-4409-b256-8060aa8f7092 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:39:01.047: INFO: The status of Pod pod-hostip-b998b4be-5f6a-4409-b256-8060aa8f7092 is Running (Ready = true)
Dec  2 11:39:01.062: INFO: Pod pod-hostip-b998b4be-5f6a-4409-b256-8060aa8f7092 has hostIP: 11.0.95.7
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:01.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3144" for this suite.

• [SLOW TEST:7.218 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5563,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:01.542: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename webhook
W1202 11:39:02.257101      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7270", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7270
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
W1202 11:39:02.942971      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7270", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
W1202 11:39:03.426262      23 warnings.go:70] No static IP address has been configured for the namespace "webhook-7270-markers", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  2 11:39:03.814: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  2 11:39:05.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  2 11:39:07.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63805577943, loc:(*time.Location)(0xa04d060)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d5bc8ffd5\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  2 11:39:11.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:23.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7270" for this suite.
STEP: Destroying namespace "webhook-7270-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:23.453 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":329,"skipped":5563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:25.007: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename security-context
W1202 11:39:25.467673      23 warnings.go:70] No static IP address has been configured for the namespace "security-context-2546", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-2546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec  2 11:39:25.660: INFO: Waiting up to 5m0s for pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9" in namespace "security-context-2546" to be "Succeeded or Failed"
Dec  2 11:39:25.667: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617423ms
Dec  2 11:39:27.680: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019292435s
Dec  2 11:39:29.695: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034448798s
Dec  2 11:39:31.707: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046241786s
Dec  2 11:39:33.717: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.056973153s
STEP: Saw pod success
Dec  2 11:39:33.717: INFO: Pod "security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9" satisfied condition "Succeeded or Failed"
Dec  2 11:39:33.724: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9 container test-container: <nil>
STEP: delete the pod
Dec  2 11:39:33.768: INFO: Waiting for pod security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9 to disappear
Dec  2 11:39:33.778: INFO: Pod security-context-e1cbb777-97b5-458b-ae4e-c6c1b81049e9 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:33.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2546" for this suite.

• [SLOW TEST:9.280 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":330,"skipped":5621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:34.287: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 11:39:34.734458      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-1719", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Dec  2 11:39:34.939: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-1719 proxy --unix-socket=/tmp/kubectl-proxy-unix604741185/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:34.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1719" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":331,"skipped":5664,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubectl
W1202 11:39:35.886189      23 warnings.go:70] No static IP address has been configured for the namespace "kubectl-8335", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Dec  2 11:39:36.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=kubectl-8335 cluster-info'
Dec  2 11:39:36.150: INFO: stderr: ""
Dec  2 11:39:36.150: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.100.192.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:36.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8335" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":332,"skipped":5682,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:36.579: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename events
W1202 11:39:37.120903      23 warnings.go:70] No static IP address has been configured for the namespace "events-6484", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:39:37.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6484" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":333,"skipped":5702,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:39:37.950: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename container-probe
W1202 11:39:38.400354      23 warnings.go:70] No static IP address has been configured for the namespace "container-probe-9072", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9072
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:57
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-adc697b0-9d19-4a2e-9b7b-4e016d6bb554 in namespace container-probe-9072
Dec  2 11:39:44.671: INFO: Started pod liveness-adc697b0-9d19-4a2e-9b7b-4e016d6bb554 in namespace container-probe-9072
STEP: checking the pod's current state and verifying that restartCount is present
Dec  2 11:39:44.682: INFO: Initial restart count of pod liveness-adc697b0-9d19-4a2e-9b7b-4e016d6bb554 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:43:46.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9072" for this suite.

• [SLOW TEST:248.880 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:43:46.832: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename projected
W1202 11:43:47.297650      23 warnings.go:70] No static IP address has been configured for the namespace "projected-6228", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:43:47.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d" in namespace "projected-6228" to be "Succeeded or Failed"
Dec  2 11:43:47.497: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181239ms
Dec  2 11:43:49.503: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012967329s
Dec  2 11:43:51.517: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026505012s
Dec  2 11:43:53.531: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0402587s
Dec  2 11:43:55.540: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.049789792s
STEP: Saw pod success
Dec  2 11:43:55.540: INFO: Pod "downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d" satisfied condition "Succeeded or Failed"
Dec  2 11:43:55.547: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d container client-container: <nil>
STEP: delete the pod
Dec  2 11:43:55.619: INFO: Waiting for pod downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d to disappear
Dec  2 11:43:55.625: INFO: Pod downwardapi-volume-6b456f3d-59d4-4d41-b8c5-9e4f0787085d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:43:55.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6228" for this suite.

• [SLOW TEST:9.363 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:43:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename kubelet-test
W1202 11:43:56.675958      23 warnings.go:70] No static IP address has been configured for the namespace "kubelet-test-9584", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:43:56.877: INFO: The status of Pod busybox-readonly-fsaaa06b5e-a7f4-448b-9fcf-1ce8bf5f2d8b is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:43:58.890: INFO: The status of Pod busybox-readonly-fsaaa06b5e-a7f4-448b-9fcf-1ce8bf5f2d8b is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:44:00.889: INFO: The status of Pod busybox-readonly-fsaaa06b5e-a7f4-448b-9fcf-1ce8bf5f2d8b is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:44:02.890: INFO: The status of Pod busybox-readonly-fsaaa06b5e-a7f4-448b-9fcf-1ce8bf5f2d8b is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:44:02.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9584" for this suite.

• [SLOW TEST:7.142 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5747,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:44:03.339: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename job
W1202 11:44:03.825279      23 warnings.go:70] No static IP address has been configured for the namespace "job-8944", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8944, will wait for the garbage collector to delete the pods
Dec  2 11:44:10.103: INFO: Deleting Job.batch foo took: 13.643467ms
Dec  2 11:44:10.203: INFO: Terminating Job.batch foo pods took: 100.732172ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:44:43.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8944" for this suite.

• [SLOW TEST:40.146 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":337,"skipped":5758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:44:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename downward-api
W1202 11:44:43.943160      23 warnings.go:70] No static IP address has been configured for the namespace "downward-api-7053", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  2 11:44:44.170: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4" in namespace "downward-api-7053" to be "Succeeded or Failed"
Dec  2 11:44:44.178: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093207ms
Dec  2 11:44:46.192: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022148255s
Dec  2 11:44:48.203: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032680012s
Dec  2 11:44:50.216: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04624542s
Dec  2 11:44:52.230: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059869733s
STEP: Saw pod success
Dec  2 11:44:52.230: INFO: Pod "downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4" satisfied condition "Succeeded or Failed"
Dec  2 11:44:52.238: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4 container client-container: <nil>
STEP: delete the pod
Dec  2 11:44:52.285: INFO: Waiting for pod downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4 to disappear
Dec  2 11:44:52.293: INFO: Pod downwardapi-volume-b4ab619d-ada4-4e9a-a084-4ac9edd59ef4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:44:52.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7053" for this suite.

• [SLOW TEST:9.280 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":5821,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:44:52.767: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 11:44:53.236409      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-3632", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3632
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  2 11:44:53.492: INFO: Waiting up to 5m0s for pod "pod-5d456556-4547-45fa-a520-630f7f55ce40" in namespace "emptydir-3632" to be "Succeeded or Failed"
Dec  2 11:44:53.502: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.755896ms
Dec  2 11:44:55.513: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021917994s
Dec  2 11:44:57.524: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032301457s
Dec  2 11:44:59.538: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046507774s
Dec  2 11:45:01.554: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.062286386s
STEP: Saw pod success
Dec  2 11:45:01.554: INFO: Pod "pod-5d456556-4547-45fa-a520-630f7f55ce40" satisfied condition "Succeeded or Failed"
Dec  2 11:45:01.562: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-5d456556-4547-45fa-a520-630f7f55ce40 container test-container: <nil>
STEP: delete the pod
Dec  2 11:45:01.610: INFO: Waiting for pod pod-5d456556-4547-45fa-a520-630f7f55ce40 to disappear
Dec  2 11:45:01.618: INFO: Pod pod-5d456556-4547-45fa-a520-630f7f55ce40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:45:01.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3632" for this suite.

• [SLOW TEST:9.308 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":5842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:45:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename custom-resource-definition
W1202 11:45:02.571022      23 warnings.go:70] No static IP address has been configured for the namespace "custom-resource-definition-324", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-324
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:45:02.767: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:45:08.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-324" for this suite.

• [SLOW TEST:7.241 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":340,"skipped":5881,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:45:09.319: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename custom-resource-definition
W1202 11:45:09.780847      23 warnings.go:70] No static IP address has been configured for the namespace "custom-resource-definition-9768", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9768
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:45:09.949: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:45:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9768" for this suite.

• [SLOW TEST:9.441 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":341,"skipped":5892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:45:18.762: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename deployment
W1202 11:45:19.240946      23 warnings.go:70] No static IP address has been configured for the namespace "deployment-5077", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  2 11:45:19.426: INFO: Creating deployment "webserver-deployment"
Dec  2 11:45:19.435: INFO: Waiting for observed generation 1
Dec  2 11:45:21.456: INFO: Waiting for all required pods to come up
Dec  2 11:45:21.468: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  2 11:45:47.487: INFO: Waiting for deployment "webserver-deployment" to complete
Dec  2 11:45:47.500: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec  2 11:45:47.518: INFO: Updating deployment webserver-deployment
Dec  2 11:45:47.518: INFO: Waiting for observed generation 2
Dec  2 11:45:49.539: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  2 11:45:49.545: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  2 11:45:49.552: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  2 11:45:49.570: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  2 11:45:49.570: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  2 11:45:49.577: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  2 11:45:49.588: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec  2 11:45:49.588: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec  2 11:45:49.605: INFO: Updating deployment webserver-deployment
Dec  2 11:45:49.605: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec  2 11:45:49.619: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  2 11:45:49.625: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  2 11:45:49.666: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5077  8c7d1665-25ce-4597-9525-0c6d908ea917 52228451 3 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005180b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-12-02 11:45:47 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-02 11:45:49 +0000 UTC,LastTransitionTime:2022-12-02 11:45:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec  2 11:45:49.674: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5077  32cbb5d3-e330-421c-8d49-af926d73feb8 52228450 3 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8c7d1665-25ce-4597-9525-0c6d908ea917 0xc005180f87 0xc005180f88}] []  [{kube-controller-manager Update apps/v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c7d1665-25ce-4597-9525-0c6d908ea917\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005181028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  2 11:45:49.674: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec  2 11:45:49.674: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-5077  2c85c684-1b60-4ffd-957f-a6bc1a7c548c 52228448 3 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8c7d1665-25ce-4597-9525-0c6d908ea917 0xc005181087 0xc005181088}] []  [{kube-controller-manager Update apps/v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c7d1665-25ce-4597-9525-0c6d908ea917\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-02 11:45:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005181118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec  2 11:45:49.721: INFO: Pod "webserver-deployment-795d758f88-797ql" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-797ql webserver-deployment-795d758f88- deployment-5077  b915c421-d7c9-40bd-89e8-d18e6a612e21 52228437 0 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc0067a7787 0xc0067a7788}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jq8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jq8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:a11041ca-3d0d-4c61-a7c9-2e5b598977b5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.6,PodIP:,StartTime:2022-12-02 11:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.721: INFO: Pod "webserver-deployment-795d758f88-bc767" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bc767 webserver-deployment-795d758f88- deployment-5077  113eeba2-6caa-4a3d-9481-737255787c2f 52228432 0 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc0067a7967 0xc0067a7968}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ph2rl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ph2rl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:,StartTime:2022-12-02 11:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.722: INFO: Pod "webserver-deployment-795d758f88-c2vpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c2vpr webserver-deployment-795d758f88- deployment-5077  e27b6d9b-441f-4e08-8739-f28e5b7b12e9 52228406 0 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc0067a7b47 0xc0067a7b48}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fl4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fl4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:,StartTime:2022-12-02 11:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.722: INFO: Pod "webserver-deployment-795d758f88-q6kxl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-q6kxl webserver-deployment-795d758f88- deployment-5077  fd15a08b-76f6-4247-bd88-8d42f13a8d58 52228415 0 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc0067a7d27 0xc0067a7d28}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-td9rf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-td9rf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:,StartTime:2022-12-02 11:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.722: INFO: Pod "webserver-deployment-795d758f88-q8w7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-q8w7w webserver-deployment-795d758f88- deployment-5077  27347c10-eea1-4d11-b480-0d48a924940c 52228442 0 2022-12-02 11:45:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc0067a7f17 0xc0067a7f18}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcz8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcz8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.5,PodIP:,StartTime:2022-12-02 11:45:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.723: INFO: Pod "webserver-deployment-795d758f88-r4q8h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r4q8h webserver-deployment-795d758f88- deployment-5077  ce10e4bf-d049-4941-afc8-143f3e9fff80 52228457 0 2022-12-02 11:45:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc003ca82f7 0xc003ca82f8}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7vbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7vbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:a11041ca-3d0d-4c61-a7c9-2e5b598977b5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.723: INFO: Pod "webserver-deployment-795d758f88-t5vd9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t5vd9 webserver-deployment-795d758f88- deployment-5077  31d3f6ce-5fd2-4172-ab59-09b6c84e1b19 52228460 0 2022-12-02 11:45:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 32cbb5d3-e330-421c-8d49-af926d73feb8 0xc003ca8467 0xc003ca8468}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32cbb5d3-e330-421c-8d49-af926d73feb8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9spv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9spv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.723: INFO: Pod "webserver-deployment-847dcfb7fb-2ks8f" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2ks8f webserver-deployment-847dcfb7fb- deployment-5077  a36dea71-7568-4b61-bb88-3f1a65161de8 52228310 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca85d0 0xc003ca85d1}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjm72,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjm72,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.5,PodIP:11.34.25.6,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://84e891703c2b6209af0a4fffeaf4af8fa03749dfa408e9b37fcfaa0f84c35df1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.724: INFO: Pod "webserver-deployment-847dcfb7fb-7j2m5" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7j2m5 webserver-deployment-847dcfb7fb- deployment-5077  1691825e-1c6e-48c1-8860-a0ffed12ebff 52228294 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca87a7 0xc003ca87a8}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts5sn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts5sn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:a11041ca-3d0d-4c61-a7c9-2e5b598977b5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.6,PodIP:11.34.25.2,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://418a705179f5c73dfa8121e1adaf4930c3b1a0c3911b9d2cfb240164d52deefe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.724: INFO: Pod "webserver-deployment-847dcfb7fb-86v24" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-86v24 webserver-deployment-847dcfb7fb- deployment-5077  017cb0bc-a2cc-4a74-a3ee-2493582a9247 52228244 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca8987 0xc003ca8988}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47x5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47x5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.8,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://5a904808d103c87e5774875ba75be3c9c8ee7479b80580606d5947e3d41f3c0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.724: INFO: Pod "webserver-deployment-847dcfb7fb-9zccz" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9zccz webserver-deployment-847dcfb7fb- deployment-5077  98cc9e3a-0fa7-4657-b821-bd27592c9c6d 52228206 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca8b67 0xc003ca8b68}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c97r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c97r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.3,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://c03e902ec0c4c7fa4aeef1b11504681ede9f20a7d26f1dc16119d460e8174886,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.725: INFO: Pod "webserver-deployment-847dcfb7fb-gw248" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gw248 webserver-deployment-847dcfb7fb- deployment-5077  257272fd-6ae3-4034-994e-87e3bfd5ca9a 52228455 0 2022-12-02 11:45:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca8d47 0xc003ca8d48}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h9cf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h9cf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.725: INFO: Pod "webserver-deployment-847dcfb7fb-k8nnc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-k8nnc webserver-deployment-847dcfb7fb- deployment-5077  587ee7b1-bfe7-4ebb-a230-d1845055714d 52228235 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca8ea7 0xc003ca8ea8}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4skvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4skvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:5e47d0ea-e90b-466b-b6de-2748d512ebf3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.7,PodIP:11.34.25.4,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://0db016e5425816ddc3fbd679e2a987f4f0face8accef88da651f9d130795c06a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.725: INFO: Pod "webserver-deployment-847dcfb7fb-kzv5w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kzv5w webserver-deployment-847dcfb7fb- deployment-5077  1aa864c7-abda-4597-bbc1-fe165af3d2e8 52228461 0 2022-12-02 11:45:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca9097 0xc003ca9098}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpz9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpz9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.726: INFO: Pod "webserver-deployment-847dcfb7fb-m6rsd" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-m6rsd webserver-deployment-847dcfb7fb- deployment-5077  bff69a03-1ecf-49a5-939a-4810c0d61ede 52228458 0 2022-12-02 11:45:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca91e0 0xc003ca91e1}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp9j9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp9j9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.726: INFO: Pod "webserver-deployment-847dcfb7fb-txlfb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-txlfb webserver-deployment-847dcfb7fb- deployment-5077  12a9a9af-5650-435a-932d-fbbd916ad2fd 52228371 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca9320 0xc003ca9321}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67fnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67fnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:a11041ca-3d0d-4c61-a7c9-2e5b598977b5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.6,PodIP:11.34.25.7,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://2d8584be1f17febe2a56478fb84c3deb46e3324a30332d556420a41d76bb3b9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.726: INFO: Pod "webserver-deployment-847dcfb7fb-wh9gg" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wh9gg webserver-deployment-847dcfb7fb- deployment-5077  1d2359db-9947-4831-a523-904a5c82e128 52228367 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca9517 0xc003ca9518}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blxbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blxbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:d6268ea2-e1ff-4fcd-86b8-62b83a8a25fa,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.5,PodIP:11.34.25.9,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://6ea4d8448fb7c6dc3ec78cbb4948e076dd2afcdcfc77140264d5a709e9c8f475,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  2 11:45:49.727: INFO: Pod "webserver-deployment-847dcfb7fb-x6tjh" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-x6tjh webserver-deployment-847dcfb7fb- deployment-5077  000598c9-a08d-499e-a221-98dbed14e8ec 52228379 0 2022-12-02 11:45:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c85c684-1b60-4ffd-957f-a6bc1a7c548c 0xc003ca9707 0xc003ca9708}] []  [{kube-controller-manager Update v1 2022-12-02 11:45:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c85c684-1b60-4ffd-957f-a6bc1a7c548c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-02 11:45:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"11.34.25.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2dbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2dbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:a11041ca-3d0d-4c61-a7c9-2e5b598977b5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:kube-plus-pull-secret,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-02 11:45:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:11.0.95.6,PodIP:11.34.25.11,StartTime:2022-12-02 11:45:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-02 11:45:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker://sha256:65bcd44d194018c72e1dc40f9cf5e9297f452870e36de525e991b676d1cd7754,ContainerID:docker://71eed8f1a119c4f6407ecbd0922acad8423ce1bb9ee60b4174b191d5fcce528b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:11.34.25.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:45:49.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5077" for this suite.

• [SLOW TEST:31.601 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":342,"skipped":5986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:45:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename events
W1202 11:45:50.895805      23 warnings.go:70] No static IP address has been configured for the namespace "events-3466", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:45:51.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3466" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":343,"skipped":6026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:45:51.683: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename daemonsets
W1202 11:45:52.146245      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-3254", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
W1202 11:45:52.752357      23 warnings.go:70] No static IP address has been configured for the namespace "daemonsets-3254", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  2 11:45:52.815: INFO: Number of nodes with available pods: 0
Dec  2 11:45:52.815: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:53.834: INFO: Number of nodes with available pods: 0
Dec  2 11:45:53.834: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:54.836: INFO: Number of nodes with available pods: 0
Dec  2 11:45:54.836: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:55.838: INFO: Number of nodes with available pods: 0
Dec  2 11:45:55.838: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:56.838: INFO: Number of nodes with available pods: 0
Dec  2 11:45:56.838: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:57.836: INFO: Number of nodes with available pods: 0
Dec  2 11:45:57.836: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:58.833: INFO: Number of nodes with available pods: 0
Dec  2 11:45:58.833: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:45:59.834: INFO: Number of nodes with available pods: 0
Dec  2 11:45:59.834: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:00.839: INFO: Number of nodes with available pods: 1
Dec  2 11:46:00.839: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:01.836: INFO: Number of nodes with available pods: 1
Dec  2 11:46:01.836: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:02.838: INFO: Number of nodes with available pods: 1
Dec  2 11:46:02.838: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:03.839: INFO: Number of nodes with available pods: 1
Dec  2 11:46:03.839: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:04.835: INFO: Number of nodes with available pods: 1
Dec  2 11:46:04.835: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:05.842: INFO: Number of nodes with available pods: 1
Dec  2 11:46:05.842: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:06.836: INFO: Number of nodes with available pods: 1
Dec  2 11:46:06.836: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:07.837: INFO: Number of nodes with available pods: 1
Dec  2 11:46:07.837: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:08.835: INFO: Number of nodes with available pods: 1
Dec  2 11:46:08.835: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:09.834: INFO: Number of nodes with available pods: 1
Dec  2 11:46:09.835: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:10.836: INFO: Number of nodes with available pods: 2
Dec  2 11:46:10.836: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:11.839: INFO: Number of nodes with available pods: 2
Dec  2 11:46:11.839: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:12.835: INFO: Number of nodes with available pods: 2
Dec  2 11:46:12.835: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:13.834: INFO: Number of nodes with available pods: 2
Dec  2 11:46:13.834: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:14.836: INFO: Number of nodes with available pods: 2
Dec  2 11:46:14.836: INFO: Node a11041ca-3d0d-4c61-a7c9-2e5b598977b5 is running more than one daemon pod
Dec  2 11:46:15.840: INFO: Number of nodes with available pods: 3
Dec  2 11:46:15.840: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  2 11:46:15.896: INFO: Number of nodes with available pods: 2
Dec  2 11:46:15.896: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:16.926: INFO: Number of nodes with available pods: 2
Dec  2 11:46:16.926: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:17.921: INFO: Number of nodes with available pods: 2
Dec  2 11:46:17.921: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:18.918: INFO: Number of nodes with available pods: 2
Dec  2 11:46:18.918: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:19.918: INFO: Number of nodes with available pods: 2
Dec  2 11:46:19.918: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:20.921: INFO: Number of nodes with available pods: 2
Dec  2 11:46:20.921: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:21.917: INFO: Number of nodes with available pods: 2
Dec  2 11:46:21.917: INFO: Node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 is running more than one daemon pod
Dec  2 11:46:22.918: INFO: Number of nodes with available pods: 3
Dec  2 11:46:22.918: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3254, will wait for the garbage collector to delete the pods
Dec  2 11:46:22.994: INFO: Deleting DaemonSet.extensions daemon-set took: 14.025361ms
Dec  2 11:46:23.094: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.694276ms
Dec  2 11:46:42.805: INFO: Number of nodes with available pods: 0
Dec  2 11:46:42.805: INFO: Number of running nodes: 0, number of available pods: 0
Dec  2 11:46:42.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52229228"},"items":null}

Dec  2 11:46:42.818: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52229228"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:46:42.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3254" for this suite.

• [SLOW TEST:51.793 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":344,"skipped":6078,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:46:43.478: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename services
W1202 11:46:43.995749      23 warnings.go:70] No static IP address has been configured for the namespace "services-1974", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-1974
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1974 to expose endpoints map[]
Dec  2 11:46:44.703: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec  2 11:46:45.720: INFO: successfully validated that service multi-endpoint-test in namespace services-1974 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1974
Dec  2 11:46:45.747: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:46:47.759: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:46:49.760: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1974 to expose endpoints map[pod1:[100]]
Dec  2 11:46:49.785: INFO: successfully validated that service multi-endpoint-test in namespace services-1974 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1974
Dec  2 11:46:49.803: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:46:51.814: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  2 11:46:53.815: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1974 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  2 11:46:53.848: INFO: successfully validated that service multi-endpoint-test in namespace services-1974 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Dec  2 11:46:53.848: INFO: Creating new exec pod
Dec  2 11:46:58.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-1974 exec execpodknkm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Dec  2 11:47:00.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec  2 11:47:00.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:47:00.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-1974 exec execpodknkm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.204.143 80'
Dec  2 11:47:00.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.204.143 80\nConnection to 10.100.204.143 80 port [tcp/http] succeeded!\n"
Dec  2 11:47:00.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:47:00.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-1974 exec execpodknkm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Dec  2 11:47:00.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec  2 11:47:00.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  2 11:47:00.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-143877446 --namespace=services-1974 exec execpodknkm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.204.143 81'
Dec  2 11:47:00.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.204.143 81\nConnection to 10.100.204.143 81 port [tcp/*] succeeded!\n"
Dec  2 11:47:00.947: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1974
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1974 to expose endpoints map[pod2:[101]]
Dec  2 11:47:01.015: INFO: successfully validated that service multi-endpoint-test in namespace services-1974 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1974
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1974 to expose endpoints map[]
Dec  2 11:47:01.065: INFO: successfully validated that service multi-endpoint-test in namespace services-1974 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:47:01.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1974" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:18.556 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":345,"skipped":6083,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  2 11:47:02.034: INFO: >>> kubeConfig: /tmp/kubeconfig-143877446
STEP: Building a namespace api object, basename emptydir
W1202 11:47:02.479919      23 warnings.go:70] No static IP address has been configured for the namespace "emptydir-3247", therefore it is not synchronized with Tufin (iKubeMgmtApi/TufinExtension)
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  2 11:47:02.686: INFO: Waiting up to 5m0s for pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc" in namespace "emptydir-3247" to be "Succeeded or Failed"
Dec  2 11:47:02.694: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.550038ms
Dec  2 11:47:04.702: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016083889s
Dec  2 11:47:06.715: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028651936s
Dec  2 11:47:08.735: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048497136s
Dec  2 11:47:10.746: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059457019s
Dec  2 11:47:12.755: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.068327244s
STEP: Saw pod success
Dec  2 11:47:12.755: INFO: Pod "pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc" satisfied condition "Succeeded or Failed"
Dec  2 11:47:12.762: INFO: Trying to get logs from node 5e47d0ea-e90b-466b-b6de-2748d512ebf3 pod pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc container test-container: <nil>
STEP: delete the pod
Dec  2 11:47:12.825: INFO: Waiting for pod pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc to disappear
Dec  2 11:47:12.831: INFO: Pod pod-7c7e92b6-b40f-49d9-acaa-5218f4c1dcbc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  2 11:47:12.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3247" for this suite.

• [SLOW TEST:11.344 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6087,"failed":0}
SSSSSSSSSSSDec  2 11:47:13.381: INFO: Running AfterSuite actions on all nodes
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec  2 11:47:13.381: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Dec  2 11:47:13.381: INFO: Running AfterSuite actions on node 1
Dec  2 11:47:13.381: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6098,"failed":0}

Ran 346 of 6444 Specs in 9808.610 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6098 Skipped
PASS

Ginkgo ran 1 suite in 2h43m31.86038425s
Test Suite Passed
